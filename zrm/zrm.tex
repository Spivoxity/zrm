% The Z Notation: A Reference Manual
% J. M. Spivey
% Programming Research Group
% University of Oxford
%
% This edition first published 1992 by
% Prentice Hall International (UK) Ltd
%
% Published 1998 by
% J. M. Spivey
% Oriel College, Oxford, OX1 4EW, England
% 
% Copyright J. M. Spivey, 1989, 1992
%
% All rights reserved. No part of this publication may be reproduced,
% stored in a retrieval system, or transmitted, in any form. or by any
% means, electronic, mechanical, photocopying, recording or otherwise,
% without prior permission, in writing, from the publisher.
% For permission in all countries contact the author.

\documentstyle[fuzz,zrm]{phi}
	\makeindex \makeglossary \makesymdex
	\hyphenation{para-meters opera-tion opera-tions
		speci-fi-ca-tion predi-cate speci-fi-ca-tions
		predi-cates}
	\finalhyphendemerits=100000000
\begin{document}

\pagenumbering{roman}
\setcounter{page}{3}	% We omit the half-title.

% Title page
\thispagestyle{empty}
{\parindent=0pt \parskip=0pt
\hrule height0pt
\vskip 21pt
{\Hugebf The Z Notation:\par}
\vskip 14pt
{\LARGE\bf A Reference Manual\par}
\vskip 14pt
{\large Second Edition\par}
\vskip 28pt
{\Large J. M. Spivey\par}
\vskip 7pt
{\it Programming Research Group\\
  University of Oxford\par}
\vskip 28pt
{\large Based on the work of\par}
\vskip 7pt
{\hsize=0.66\hsize \large \raggedright \noindent
	J.~R.~Abrial, I.~J.~Hayes, C.~A.~R.~Hoare,
	He~Jifeng, C.~C.~Morgan, J.~W.~Sanders,
	I.~H.~S\o rensen, J.~M.~Spivey, B.~A.~Sufrin \par}}
\clearpage

% T.P. verso
\thispagestyle{empty}
{\raggedright
\noindent
This edition first published 1992 by\\
Prentice Hall International (UK) Ltd
\vskip 14pt
\noindent
Published 1998 by\\
J. M. Spivey\\
Oriel College, Oxford, OX1 4EW, England
\vskip 14pt
\noindent
\copyright\ J. M. Spivey, 1989, 1992
\vskip 14pt
\noindent
All rights reserved. No part of this publication may be reproduced,
stored in a retrieval system, or transmitted, in any form. or by any
means, electronic, mechanical, photocopying, recording or otherwise,
without prior permission, in writing, from the publisher.\\
For permission in all countries contact the author.
\clearpage}

\tableofcontents		% Table of contents

% Preface begins on a recto - revise if necessary for even working.
\clearpage\markboth{}{}\cleardoublepage
\chapter*{Preface}

\begin{thema}\begin{playlet}
\item[Jack] You're quite perfect, Miss Fairfax.
\item[Gwendolen] Oh! I hope I am not that. It would leave no room
	for developments, and I intend to develop in many~directions.
	\source{Oscar Wilde, {\em The Importance of Being Earnest}}
\end{playlet}\end{thema}

\noindent
The Z notation for specifying and designing software has evolved
over the best part of a decade, and it is now possible to identify
a standard set of notations which, although simple, capture the
essential features of the method. This is the aim of the reference
manual in front of you, and it is written with the everyday needs
of readers and writers of Z specifications in mind. It is not
a tutorial, for a concise statement of general rules
is often given rather than a presentation of illustrative examples;
nor is it a formal definition of the notation, for an informal
but rigorous style of presentation will be more accessible to
Z users, who may not be familiar with the special techniques of
formal language definition.

It is perhaps worth recording here the causes which led to even this
modest step towards standardization of Z. The first of these is the
growing trend towards computer assistance in the writing and
manipulation of Z specifications. While the specifier's tools
amounted to little more than word-processing facilities, they had
enough inherent flexibility to make small differences in notation
unimportant. But tools are now being built which depend on syntactic
analysis, and to some extent on semantic analysis, of specifications.
For these tools -- syntax checkers, structure editors, type checkers,
and so on -- to be useful and reliable, there must be agreement on
the grammatical rules of the language they support.

Communication between people is also helped by an agreed common
notation, and here I expect the part of this manual devoted to
the standard `mathematical tool-kit' to be especially useful. In
this part, I have given a formal definition of each mathematical
symbol, together with an informal description and a collection
of useful algebraic laws relating the symbol to others.

A third reason for standardization is the need to define a syllabus
for training courses\index{training courses} in the use of Z.  Whilst
there is an important difference between learning the Z language and
learning to be effective in reading and writing Z specifications, just
as learning to program is much more than learning a programming
language, I hope that this description of the language will provide a
useful check-list of topics to be covered in courses.

Finally, as the use of Z increases, there will be a need for a
reference point for contracts and research proposals which call
for a specification to be written in Z, and this manual is intended
to fill that need also.

In selecting the language features and the mathematical symbols to be
included, I have tried to maintain a balance between
comprehensiveness and simplicity. On one hand, there is a need to promote
common notations for as many important concepts as possible; but on the
other hand, there is little point in including notations which are used
so rarely that they will be forgotten before they are needed. This
observation principally affects the choice of symbols to be included in
the `mathematical tool-kit'.

Because one of the aims is increased stability of Z, I have felt
obliged to omit from the account certain aspects of Z which still
appear to be tentative. I found it difficult to reconcile the idea of
{\em overloading\/}\index{overloading: not allowed} -- that is, the
possibility that two distinct variables in the same scope might have
identical names -- with the idea that common components are
identified when schemas are joined, so overloading is forbidden in
the language described.  The relative weakness of the Z type system
would, in any case, make overloading less useful than it is in other
languages.

More importantly, I have also felt unable to include a system of
formal inference rules for deriving theorems
about specifications.  The principles on which such a system might be
based are clear enough, at least for the parts of Z which mirror
ordinary mathematical notation; but the practical usefulness of
inference rules seems to depend crucially on making them interact
smoothly, and we have not yet gained enough experience to do this.

\section*{How to use this book}

Here is a brief summary of the contents of each chapter:

Chapter~\ref{c:tutorial} is an overview of the Z notation and its use
in specifying and developing programs. The chapter begins with a
simple example of a Z specification; this is followed by examples of
the use of the schema calculus to modularize a specification and the
use of data refinement to relate specifications and designs.

Chapter~\ref{c:background} explains the concepts behind the Z
language, such as {\em schemas\/} and {\em types}. It contains
definitions of the terms which are used later to explain the
constructs of Z. Although the presentation is informal, it assumes a
basic knowledge of naive set theory and predicate calculus.

Chapter~\ref{c:language} contains a description of the Z language
itself. It is organized according to the syntactic categories of the
language, with separate sections on declarations, predicates,
expressions, and so on.  Some more advanced features of the language,
{\em generics\/} and {\em free types}, are given their own sections at
the end of the chapter.

Chapter~\ref{c:library} describes a standard collection of
mathematical symbols which are useful in specifying information
systems.  It is divided into six sections, each dealing with a small
mathematical theory such as sets, relations or sequences.  The chapter
starts with a classified list of the symbols it defines, on
pages~\pageref{p:symlist+} to~\pageref{p:symlist-}.

Chapter~\ref{c:seqprog} explains the conventions used in describing
sequential programs with Z specifications, including the processes of
operation and data refinement, by which abstract specifications can be
developed into more concrete designs.

Chapter~\ref{c:syntax} contains a summary of the syntax of Z. It is
here that the fine details of Z syntax are presented, such as the
relative binding powers of operators, connectives and quantifiers.

Large parts of Chapters~\ref{c:language} and~\ref{c:library} are
organized into `manual pages' with a fixed layout.  Each manual page
deals with a single construct or symbol, or a small group of related
ones.  In Chapter~\ref{c:language}, the pages may contain the
following items:
\begin{description}
\item[Name] 
	The constructs defined on the page are listed, and a short
	descriptive title is given for each of them.
\item[Syntax] 
	The syntax rules for each construct are given in Backus--Naur
	Form (BNF).
\item[Scope rules]
	If variables are introduced by a construct, this item
	identifies the region of text in the specification where they
	are visible.  If the meaning of a construct depends implicitly
	on the values of certain variables, these variables are listed.
\item[Type rules]
	The type of each kind of expression is described in terms of
	the types of its sub-expressions.  Restrictions on the types
	of sub-expressions are stated.
\item[Description]
	The meaning of each construct is explained informally.
\item[Laws]
	Some mathematical properties of the constructs and
	relationships with other constructs are listed.
\end{description}
In Chapter~\ref{c:library}, the format is a little different: each
mathematical symbol is defined formally in an item headed `{\bf
Definition}', using the Z notation itself. Particular emphasis is laid
on the collection of mathematical laws\index{algebraic laws} obeyed by
the symbols. For brevity, the variables used in these laws are not
declared explicitly if their types are clear from the context. An item
headed `{\bf Notation}' sometimes explains special-purpose notations
designed to make the symbols easier to use.

Several special pages in Chapter~\ref{c:library} consist entirely of
laws of a certain kind: for example, the laws which express the
monotonicity with respect to $\subseteq$ of various operations on sets
and relations are collected on page~\pageref{p:mono} under the title
`Monotonic operations'.

As well as the usual entries under descriptive terms, the general
index at the back of the book contains entries for each syntactic
class of the language such as \(Expression\) or \(Paragraph\). These
entries appear in sans-serif type, and refer to the syntax rules for
the class.  Each symbol defined as part of the mathematical tool-kit
has an entry, either under the symbol itself, if it is a word such as
$head$, or under a descriptive name if it is a special symbol such as
$\oplus$.  These special symbols also appear in the one-page `Index of
symbols'.

The glossary at the back of the book contains concise definitions of
the technical terms used in describing Z.  Each term defined in the
glossary is set in {\sl italic\/} type the first time it appears in
the text.

\section*{Acknowledgements}

It gives me great pleasure to end this preface by thanking my present and
former colleagues for allowing me to contribute to the work of theirs
reported in this book; many of the ideas are theirs, and I am happy
that their names appear with mine on the title page.  I owe a special
debt of thanks to Bernard Sufrin, whose {\em Z Handbook\/} was the
starting point for this manual, and whose constant advice and
encouragement have helped me greatly.  
I should like to thank all
those who have pointed out errors and suggested possible improvements,
and especially the following, who have helped me with detailed
comments on the manuscript: 
Tim Clement (University of Manchester),
Anthony Hall (Praxis Systems), 
Nigel Haigh (Seer Management), 
Ian Hayes (University of Queensland),
Steve King (University of Oxford),
Ruaridh MacDonald (Royal Signals and Radar Establishment), 
Sebastian Masso (University of Oxford),
Dan Simpson (Brighton Polytechnic), 
Sam Valentine (Logica).  

I am grateful to Katharine Whitehorn for permission to quote from her
book {\em Cooking in a Bedsitter}.  Chapter 1 is adapted from a paper
which first appeared in {\em Software Engineering Journal} under the
title `An introduction to Z and formal specifications', and is
reproduced with the permission of the Institute of Electrical
Engineers.

My final thanks go to my wife Petronella, who contributed large
helpings of the two most vital ingredients, patience and food, even
when I seemed to spend more time with the {\em \TeX book} than I did
with her.
\par\vskip\baselineskip\noindent
{\em Oriel College, Oxford}\hfill J. M. S.\\
{\em September, 1988}

\newpage
\section*{Preface to the second edition}

This second edition remedies a number of defects.  There are several
language constructs that I had omitted from the first edition as
being of marginal use, but turn out to be far more widely used than
I had imagined.  The most significant of these is notation for the
renaming of schema components, but there are many other smaller
changes.  I have also made some additions to the library of
mathematical notation following suggestions from many people.
Obviously, this process of extension could go on for ever, and I
have only adopted new notations when they seem to be widely needed
and to have a close relationship with the notation that was already
there.  The purpose of the library is not to be an exhaustive list of
concepts that are used in specifications, but to provide a basic
vocabulary that readers and writers of Z specifications can have in
common.  All the substantive changes to the language and library are
listed in an appendix.

The new edition has also provided an opportunity to improve the
exposition in many small ways, and I am grateful to the many people
who have written with suggestions, or with questions that they could
not answer from the account of Z contained in the first edition.
The biggest change is the introduction of an explicit notation for
bindings, the objects that inhabit schema types, and its use in
explaining the language constructs that involve schemas.  I am
grateful to Paul Gardiner for persuading me that an
explanation of non-generic schemas could be given in this way.

Both the \LaTeX\ style option that was used to print the Z
specifications in the book and a type-checking program that
enforces the syntax, scope, and type rules may be obtained from the
author.  For details, write to Mrs.\ A.\ Spivey, 34, Westlands
Grove, Stockton Lane, York, {\sc yo3 0ef}.
\par\vskip\baselineskip\noindent
{\em Wolfson College, Oxford}\hfill J. M. S.\\
{\em \today}
\par\nobreak\vfill\nobreak
\begin{quotation}
	You probably cannot afford elaborate equipment, and you
	certainly have no room for it: but the {\em right\/} simple
	tools will stop you longing for the other, complicated ones.
	\source{Katharine Whitehorn, {\em Cooking in a Bedsitter}}
\end{quotation}
\unskip
\clearpage\markboth{}{}\cleardoublepage

\pagenumbering{arabic}
\chapter{Tutorial Introduction}\label{c:tutorial}

This chapter is an introduction to some of the features of the Z
notation, and to its use in specifying information systems and
developing rigorously checked designs.
The first part introduces the idea of a formal specification using
a simple example: that of a `birthday book', in which people's
birthdays can be recorded, and which is able to issue reminders
on the appropriate day.
The behaviour of this system for correct input is
specified first, then the schema calculus is used to
strengthen the specification into one requiring error
reports for incorrect input.

The second part of the chapter introduces the idea of data refinement as
a means of constructing designs which achieve a formal
specification. Refinement is presented through the medium of two
examples: the first is a direct implementation of the birthday
book from part one, and the second is a simple checkpoint facility,
which allows the current state of a database to be saved and later
restored. A Pascal-like programming language is used to show
the code for some of the operations in the examples.

\section{What is a formal specification?}

Formal specifications use mathematical notation to describe in a
precise way the properties which an information system must have,
without unduly constraining the way in which these properties are
achieved. They describe {\em what\/} the system must do without saying
{\em how\/} it is to be done.  This {\em abstraction\/} makes formal
specifications useful in the process of developing a computer system,
because they allow questions about what the system does to be answered
confidently, without the need to disentangle the information from a
mass of detailed program code, or to speculate about the meaning of
phrases in an imprecisely-worded prose description.

A formal specification can serve as a single, reliable reference point
for those who investigate the customer's needs, those who implement programs
to satisfy those needs, those who test the results, and those who write
instruction manuals for the system.
Because it is independent of the program code, a formal specification of
a system can be completed early in its development. Although it
might need to be changed as the design team gains in understanding and
the perceived needs of the customer evolve, it can be a valuable means
of promoting a common understanding among all those concerned with the
system.

One way in which mathematical notation can help to achieve these
goals is through the use of {\em mathematical data types\/}
to model the data in a system. These data types
are not oriented towards computer representation, but they
obey a rich collection of mathematical laws which make it possible
to reason effectively about the way a specified system will
behave. We use the notation of {\em predicate logic\/} to
describe abstractly the effect of each operation of our
system, again in a way that enables us to reason about its
behaviour.

The other main ingredient in Z is a way of decomposing
a specification into small pieces called {\em schemas}. By
splitting the specification into schemas, we can present
it piece by piece. Each piece can be linked with a
commentary which explains informally the significance of the
formal mathematics.
In Z, schemas are used to describe both
static and dynamic aspects of a system. The static aspects
include:
\begin{itemize}
\item	the states it can occupy;
\item	the invariant relationships that are maintained
	as the system moves from state to state.
\end{itemize}
The dynamic aspects include:
\begin{itemize}
\item	the operations that are possible;
\item	the relationship between their inputs and outputs;
\item	the changes of state that happen.
\end{itemize}
Later, we shall see how the schema language allows different
facets of a system to be described separately, then related
and combined. For example, the operation of a system when it
receives valid input may be described first, then the
description may be extended to show how errors in the input
are handled. Or the evolution of a single process in a
complete system may be described in isolation, then related
to the evolution of the system as a whole.

We shall also see how schemas can be used to describe a
transformation from one view of a system to another, and so
explain why an abstract specification is correctly
implemented by another containing more details of a concrete
design. By constructing a sequence of specifications, each
containing more details than the last, we can eventually
arrive at a program with confidence that it satisfies the
specification.

\section{The birthday book}\label{bb}

The best way to see how these ideas work out is to look at a
small example. For a first example, it is important to
choose something simple, and I have chosen a system so
simple that it is usually implemented with a notebook and
pencil rather than a computer. It is a system which records
people's birthdays, and is able to issue a reminder when the
day comes round.

In our account of the system, we shall need to deal with people's
names and with dates. For present purposes, it will not matter what
form these names and dates take, so we introduce the set of all names
and the set of all dates as {\em basic types\/} of the specification:
\[ [NAME, DATE]. \]
This allows us to name the sets without saying what kind of objects
they contain.
The first aspect of the system to describe is its {\em state space\/},
and we do this with a schema:
\begin{schema}{BirthdayBook}
    known: \power NAME \\
    birthday: NAME \pfun DATE
\where
    known=\dom birthday
\end{schema}
Like most schemas, this consists of a part above the central
dividing line, in which some variables are declared, and a
part below the line which gives a relationship between the
values of the variables. In this case we are describing the
state space of a system, and the two variables represent
important {\em observations\/} which we can make of the
state:
\begin{itemize}
\item $known$ is the set of names with birthdays recorded;
\item $birthday$ is a function which, when applied to certain
names, gives the birthdays associated with them.
\end{itemize}
The part of the schema below the line gives a relationship
which is true in every state of the system and is maintained
by every operation on it: in this case, it says that the set
$known$ is the same as the domain of the function $birthday$
-- the set of names to which it can be validly applied.
This relationship is an {\em invariant\/} of the system.

In this example, the invariant allows the value of the variable
$known$ to be derived from the value of $birthday$: $known$ is a {\em
derived\/}\index{derived component}%
	\glossary{[derived component] A component of a schema
	describing the state space of an abstract data type whose
	value can be deduced from the values of the other components.}
component of the state, and it would be possible to
specify the system without mentioning $known$ at all.  However,
giving names to important concepts helps to make specifications more
readable; because we are describing an abstract view of the state space
of the birthday book, we can do this without making a
commitment to represent $known$ explicitly in an implementation.

One possible state of the system has three people in the set $known$,
with their birthdays recorded by the function $birthday$:
\[
	known = \{\,{\rm John, Mike, Susan}\,\} \\
\also
	birthday = \{\,\vtop{\halign{\strut#\hfil&${}\mapsto{}$#\hfil\cr
			John&  25--Mar,\cr
			Mike&  20--Dec,\cr
			Susan& 20--Dec\,\}.\cr}}
\]
The invariant is satisfied, because $birthday$ records a date for
exactly the three names in $known$.

Notice that in this description of the state space of the
system, we have not been forced to place a limit on the
number of birthdays recorded in the birthday book, 
nor  to say that the
entries will be stored in a particular order. We have also
avoided making a premature decision about the format of
names and dates. On the other hand, we have concisely
captured the information that each person can have only one
birthday, because the variable $birthday$ is a function, and
that two people can share the same birthday as in our
example.

So much for the state space; we can now start on some {\em
operations\/} on the system. The first of these is to add a
new birthday, and we describe it with a schema:
\begin{schema}{AddBirthday}
     \Delta BirthdayBook \\
     name?: NAME \\
     date?: DATE
\where
     name? \notin known
\also
     birthday' = birthday \cup \{name? \mapsto date?\}
\end{schema}
The declaration $\Delta BirthdayBook$\index{Delta convention} alerts
us to the fact that the schema is describing a {\em state change}: it
introduces four variables $known$, $birthday$, $known'$ and
$birthday'$. The first two are observations of the state before the
change, and the last two are observations of the state after the
change. Each pair of variables is implicitly constrained to satisfy
the invariant, so it must hold both before and after the operation.
Next come the declarations of the two inputs to the operation.
By convention, the names of inputs end in a question mark.

The part of the schema below the line first of all gives a {\em
pre-condition\/}\index{pre-condition} for the success of the
operation: the name to be added must not already be one of those known
to the system. This is reasonable, since each person can only have one
birthday. This specification does not say what happens if the
pre-condition is not satisfied: we shall see later how to extend the
specification to say that an error message is to be produced. If the
pre-condition is satisfied, however, the second line says that the
birthday function is extended to map the new name to the given date.

We expect that the set of names known to the system will be
augmented with the new name:
\[ known' = known \cup \{name?\}. \]
In fact we can {\em prove\/} this from the specification of
$AddBirthday$, using the invariants on the state before and
after the operation:      
\begin{argue}
	known' = \dom birthday' &		invariant after \\
\t1    	= \dom (birthday \cup \{name? \mapsto date?\}) &
						spec.\ of $AddBirthday$ \\
\t1	= \dom birthday \cup \dom\,\{name? \mapsto date?\} &
						fact about `$\dom$' \\
\t1	= \dom birthday \cup \{name?\} &	fact about `$\dom$' \\
\t1	= known \cup \{name?\}. & 		invariant before
\end{argue}
Stating and proving properties like this one is a good way
of making sure the specification is accurate; reasoning from
the specification allows us to explore the behaviour
of the system without going to the trouble and expense of
implementing it.
The two facts about `$\dom$' used in this proof are examples
of the laws obeyed by mathematical data types:
\[
	\dom (f \cup g) = (\dom f) \cup (\dom g) \\
\also
	\dom \{a \mapsto b\} = \{a\}.
\]
Chapter~\ref{c:library} contains many laws like these.

Another operation might be to find the
birthday of a person known to the system. Again we describe
the operation with a schema:
\begin{schema}{FindBirthday}
	\Xi BirthdayBook \\
	name?: NAME \\
	date!: DATE 
\where
	name? \in known
\also
	date! = birthday(name?)
\end{schema}
This schema illustrates two new notations.  The
declaration $\Xi BirthdayBook$\index{Xi convention} indicates that
this is an operation in which the state does not change: the values $known'$
and $birthday'$ of the observations after the operation are equal to
their values $known$ and $birthday$ beforehand. Including $\Xi
BirthdayBook$ above the line has the same effect as including $\Delta
BirthdayBook$ above the line and the two equations
\[
	known' = known 
\also
	birthday' = birthday
\]
below it. The other notation is the use of a name ending in
an exclamation mark for an output: the
$FindBirthday$ operation takes
a name as input and yields
the corresponding birthday as output.
\pagebreak[1]
The pre-condition for success 
of the operation is that
$name?$ is one of the names known to the system; if this is
so, the output $date!$ is the value of the birthday function
at argument $name?$.

The most useful operation on the system is the one to find
which people have birthdays on a given date.  The operation has one
input $today?$, and one output, $cards!$, which is a {\em set\/} of names:
there may be zero, one, or more people with birthdays on a
particular day, to whom birthday cards should be sent.
\begin{schema}{Remind}
	\Xi BirthdayBook \\
	today?: DATE \\
	cards!: \power NAME
\where
	cards! = \{\,n: known | birthday(n) = today?\,\}
\end{schema}
Again the $\Xi$ convention is used to indicate that the
state does not change. This time there is no pre-condition.
The output $cards!$ is specified to be equal to the set of
all values $n$ drawn from the set $known$ such that the
value of the birthday function at $n$ is $today?$. In general,
$y$ is a member of the set $\{\,x:S | \ldots x \ldots\,\}$
exactly if $y$ is a member of $S$ and the condition $\ldots
y \ldots$, obtained by replacing $x$ with $y$, is satisfied:
\[ y \in \{\,x: S | \ldots x \ldots \,\}
	\iff y \in S \land (\ldots y \ldots). \]
So, in our case,
\[ m \in \{\,n: known | birthday(n) = today?\,\} \\
\t1 		\iff m \in known \land birthday(m) = today?~. \]
A name $m$ is in the output set $cards!$ exactly if it is
known to the system and the birthday recorded for it is
$today?$.

To finish the specification, we must say what state the system is in
when it is first started. This is the {\em initial state\/} of the
system, and it also is specified by a schema:
\begin{schema}{InitBirthdayBook}
	BirthdayBook
\where
	known = \empty
\end{schema}
This schema describes a birthday book in which the set $known$ is
empty: in consequence, the function $birthday$ is empty too.

What have we achieved in this specification? We have described in the
same mathematical framework both the state space of our birthday-book
system and the operations which can be performed on it.  The data
objects which appear in the system were described in terms of
mathematical data types such as sets and functions. The description of
the state space included an invariant relationship between the parts of
the state -- information which would not be part of a program
implementing the system, but which is vital to understanding it.

The effects of the operations are described in terms of the
relationship which must hold between the input and the
output, rather than by giving a recipe to be followed. This
is particularly striking in the case of the $Remind$
operation, where we simply documented the conditions under
which a name should appear in the output. An implementation
would probably have to examine the known names one at a
time, printing the ones with today's date as it found them,
but this complexity has been avoided in the specification. The
implementor is free to use this technique, or any other one,
as he or she chooses.

% Mathematical specifications have the three virtues of being concise,
% precise and unambiguous.  They are {\em concise\/} because mathematical
% notation is capable of expressing complex facts about information
% systems in a short space.  Practical experience shows that a
% mathematical specification of a system is often much shorter than an
% equivalent informal specification.
% Mathematical specifications are {\em precise\/} because they allow
% requirements to be documented accurately. The desired function of a
% system is described in a way that does not unduly constrain either the
% data structures used to represent the information in the system, or the
% algorithms used to compute with it.  Finally, mathematical
% specifications are {\em unambiguous}: differences of interpretation can
% be avoided when specifications are expressed in a standardized language
% with a well-understood meaning.

\section{Strengthening the specification}

A correct implementation of our specification will faithfully record
birthdays and display them, so long as there are no mistakes in the
input. But the specification has a serious flaw: as soon as the user
tries to add a birthday for someone already known to the system, or
tries to find the birthday of someone not known, it says nothing about
what happens next. The action of the system may be perfectly
reasonable: it may simply ignore the incorrect input. On the other
hand, the system may break down: it may start to display rubbish, or
perhaps worst of all, it may appear to operate normally for several
months, until one day it simply forgets the birthday of a rich and
elderly relation.

Does this mean that we should scrap the specification and begin a new
one? That would be a shame, because the specification we have
describes clearly and concisely the behaviour for correct input, and
modifying it to describe the handling of incorrect input could only
make it obscure.  Luckily there is a better solution: we can describe,
separately from the first specification, the errors which might be
detected and the desired responses to them, then use the operations of
the Z {\em schema calculus\/} to combine the two descriptions into a
stronger specification.

We shall add an extra output $result!$ to each operation on the
system.  When an operation is successful, this output will take the
value $ok$, but it may take the other values $"already\_known"$ and
$"not\_known"$ when an error is detected. The following {\em free type
definition\/}\index{free type: definition} defines $REPORT$ to be a set
containing exactly these three values:
\[ REPORT ::= ok | "already\_known" | "not\_known". \]
We can define a schema $Success$ which just specifies that the
result should be $ok$, without saying how the state changes:
\begin{schema}{Success}
	result!: REPORT
\where
	result! = ok
\end{schema}
The conjunction operator $\land$ of the schema calculus allows us to
combine this description with our previous description of $AddBirthday$:
\[ AddBirthday \land Success. \]
This describes an operation which, for correct input, both acts as
described by $AddBirthday$ and produces the result $ok$.

For each error that might be detected in the input, we define a schema
which describes the conditions under which the error occurs and
specifies that the appropriate report is produced. Here is a schema
which specifies that the report $"already\_known"$ should be produced
when the input $name?$ is already a member of $known$:
\begin{schema}{AlreadyKnown}
	\Xi BirthdayBook \\
	name?: NAME \\
	result!: REPORT
\where
	name? \in known \\
	result! = "already\_known"
\end{schema}
The declaration $\Xi BirthdayBook$ specifies that if the error occurs,
the state of the system should not change.

We can combine this description with the previous one to give a
specification for a robust version of $AddBirthday$:
\[ RAddBirthday \defs (AddBirthday \land Success) \lor AlreadyKnown. \]
This definition introduces a new schema called $RAddBirthday$,
obtained by combining the three schemas on the right-hand side.
The operation $RAddBirthday$ must terminate whatever its input.  If
the input $name?$ is already known, the state of the system does not
change, and the result $"already\_known"$ is returned; otherwise,
the new birthday is added to the database as described by
$AddBirthday$, and the result $ok$ is returned.

We have specified the various requirements for this operation separately,
and then combined them into a single specification of the whole
behaviour of the operation. This does not mean that each requirement
must be implemented separately, and the implementations combined
somehow.
In fact, an implementation might search for a place to store the new
birthday, and at the same time check that the name is not already known;
the code for normal operation and error handling might be thoroughly
mingled.
This is an example of the abstraction which is possible when we use a
specification language free from the constraints necessary
in a programming language. The operators $\land$ and $\lor$
cannot (in general) be implemented efficiently as ways of combining
programs, but this should not stop us from using them to combine
specifications if that is a convenient thing to do.

The operation $RAddBirthday$ could be specified directly by writing
a single schema which combines the predicate parts of the three
schemas $AddBirthday$, $Success$ and $AlreadyKnown$.
The effect of the schema $\lor$ operator is to make a schema
in which the predicate part is the result of joining the predicate parts of
its two arguments with the logical connective $\lor$. Similarly, the effect
of the schema $\land$ operator is to take the conjunction of the two
predicate parts.
Any common variables of the two schemas are merged: in this example, the
input $name?$, the output $result!$, and the four observations of the
state before and after the operation are shared by the two arguments of
$\lor$.
\begin{schema}{RAddBirthday}
	\Delta BirthdayBook \\
	name?: NAME \\
	date?: DATE \\
	result!: REPORT
\where
	(name? \notin known \land \\
\t1		birthday' = birthday \cup \{name? \mapsto date?\} \land \\
\t1		result! = ok) \lor \\
	(name? \in known \land \\
\t1		birthday' = birthday \land \\
\t1		result! = "already\_known")
\end{schema}
In order to write $RAddBirthday$ as a single schema, it has been
necessary to write out explicitly that the state doesn't change when
an error is detected, a fact that was implicitly
part of the declaration $\Xi BirthdayBook$ before.

A robust version of the $FindBirthday$ operation must be able to report
if the input name is not known:
\begin{schema}{NotKnown}
	\Xi BirthdayBook \\
	name?: NAME \\
	result!: REPORT
\where
	name? \notin known \\
	result! = "not\_known"
\end{schema}
The robust operation either behaves as described by $FindBirthday$ and
reports success, or reports that the name was not known:
\[ RFindBirthday \defs (FindBirthday \land Success) \lor NotKnown. \]
The $Remind$ operation can be called at any time: it never results in
an error, so the robust version need only add the reporting of success:
\[ RRemind \defs Remind \land Success. \]

\looseness=1
The separation of normal operation from error-handling which we
have seen here is the simplest but also the most common kind of
modularization possible with the schema calculus.
More complex modularizations include {\em promotion} or
{\em framing}, where operations
on a single entity -- for example, a file -- are made into
operations on a named entity in a larger system -- for example, a
named file in a directory.
The operations of reading and writing a file might be described by
schemas. Separately, another schema might describe the way a file can
be accessed in a directory under its name. Putting these two parts
together would then result in a specification of operations for
reading and writing named~files.

Other modularizations are possible: for example, the
specification of a system with access restrictions might separate the
description of who may call an operation from the description of what
the operation actually does.
There are also facilities for generic definitions in Z which allow, for
example, the notion of resource management to be specified in general,
then applied to various aspects of a complex system.

\section{From specifications to designs}

We have seen how the Z notation can be used to specify software modules,
and how the schema calculus allows us to put together the specification
of a module from pieces which describe various facets of its function.
Now we turn our attention to the techniques used in Z to document the design
of a program which implements the specification.

The central idea is to describe the concrete data structures which the
program will use to represent the abstract data in the specification,
and to derive descriptions of the operations in terms of the concrete
data structures. We call this process {\em data refinement}, and it is
fully explained in Chapter~\ref{c:seqprog}.  Often, a data refinement
will allow some of the control structure of the program to be made
explicit, and this is achieved by one or more steps of {\em operation
refinement\/} or {\em algorithm development}.

For simple systems, it is possible to go from the abstract specification
to the final program in one step, a method sometimes called {\em direct
refinement}. In more complex systems, however, there are too many design
decisions for them all to be recorded clearly in a single refinement step,
and the technique of {\em deferred refinement\/} is appropriate.
Instead of a finished program, the first refinement step results in a new
specification, and this is then subjected to further steps of refinement until
a program is at last reached. The result is a sequence of design documents,
each describing a small collection of related design decisions. As the
details of the data structures are filled in step by step, so more of the
control structure can be filled in, leaving certain sub-tasks to be
implemented in subsequent refinement steps. These sub-tasks can be made
into subroutines in the final program, so the step-wise structure of the
development leads to a modular structure in the~program.

Program developments are often documented by giving an idealized
account of the path from specification to program. In these accounts,
the ideas all appear miraculously at the right time, one after
another. There are no mistakes, no false starts, no decisions taken
which are later revised. Of course, real program developments do not
happen like that, and the earlier stages of a development are often
revised many times as later stages cast new light on the system. In
any case, specifications are seldom written without at least a rough
idea of how they might be implemented, and it is very rare to find
that something similar has not been implemented before.
This does not mean that the idealized accounts are worthless, however.
They are often the best way of presenting the decisions which have been made
and the relationships between them, and such an account can be a valuable
piece of documentation.

The rest of this chapter concentrates on data refinement
in Z, although the results of the operation refinement which might
follow it are shown.  Two examples of data refinement are presented.
The first shows direct refinement; the birthday book we specified in
Section~\ref{bb} is implemented using a pair of arrays. In the second
example, deferred refinement is used to show the implementation of a
simple checkpoint--restart mechanism.  The implementation uses two
sub-modules for which specifications in Z are derived as part of the
refinement step. This demonstrates the way in which mathematics can
help us to explore design decisions at a high level of abstraction.

\section{Implementing the birthday book}

The specification of the birthday book worked with abstract data structures
chosen for their expressive clarity rather than their ability to be directly
represented in a computer. In the implementation, the data structures must
be chosen with an opposite set of criteria, but they can still be modelled
with mathematical data types and documented with schemas.

In our implementation, we choose to represent the birthday book with two
arrays, which might be declared by
\[
	names:\;\ARRAY [1\upto{}] \OF NAME; \\
	dates:\;\ARRAY [1\upto{}] \OF DATE;
\]
I have made these arrays `infinite' for the sake of simplicity. In
a real system development, we would use the schema calculus to specify
a limit on the number of entries, with appropriate error reports if
the limit is exceeded. Finite arrays could then be used in a more
realistic implementation; but for now, this would just be a distraction,
so let us pretend that potentially infinite arrays are part of our
programming language. We shall, in any case, only use a finite part of
them at any time.
These arrays can be modelled mathematically by functions from the set
$\nat_1$ of strictly positive integers to $NAME$ or $DATE$:
\[
	names: \nat_1 \fun NAME \\
	dates: \nat_1 \fun DATE.
\]
The element $names[i]$ of the array is simply the value $names(i)$ of
the function, and the assignment
$names[i] := v$
is exactly described by the specification
\[ names' = names \oplus \{i \mapsto v\}. \]
The right-hand side of this equation is a function which takes the
same value as $names$ everywhere except at the argument~$i$, where it
takes the value~$v$.

We describe the state space of the program as a schema. There is
another variable $hwm$ (for `high water mark'); it shows how much of
the arrays is in use.
\begin{schema}{BirthdayBook1}
	names: \nat_1 \fun NAME \\
	dates: \nat_1 \fun DATE \\
	hwm: \nat
\where
	\forall i, j: 1 \upto hwm @ \\
\t1		i \neq j \implies names(i) \neq names(j)
\end{schema}
The predicate part of this schema says that there are no repetitions
among the elements $names(1)$, \dots, $names(hwm)$.

The idea of this representation is that each name is linked with the
date in the corresponding element of the array $dates$. We can
document this with a schema $Abs$ that defines the {\em
abstraction relation\/} between the abstract state space
$BirthdayBook$ and the concrete state space $BirthdayBook1$:
\begin{schema}{Abs}
	BirthdayBook \\
	BirthdayBook1
\where
	known = \{\,i: 1 \upto hwm @ names(i)\,\} \\
\also
	\forall i: 1 \upto hwm @ \\
\t1		birthday(names(i)) = dates(i)
\end{schema}
This schema relates two points of view on the state of the system.
The observations involved are both those of the abstract state --
$known$ and $birthday$ -- and those of the concrete state -- $names$,
$dates$ and $hwm$.  The first predicate says that the set $known$
consists of just those names which occur somewhere among $names(1)$,
\dots,~$names(hwm)$.
The set $\{\,y: S @ \ldots y \ldots\,\}$ contains those values
taken by the expression $\ldots y \ldots$ as $y$ takes values in the
set $S$, so $known$ contains a name $n$ exactly if $n = names(i)$ for
some value of $i$ such that $1 \leq i \leq hwm$. We can write this in
symbols with an existential quantifier:
\[ n \in known \iff (\exists i: 1 \upto hwm @ n = names(i)). \]
The second predicate says that the birthday for $names(i)$ is the
corresponding element $dates(i)$ of the array~$dates$.

\new Several concrete states may represent the same abstract state:
in the example, the order of the names and dates in the arrays does
not matter, so long as names and dates correspond properly. The
order is not used in determining which abstract state is represented
by a concrete state, so two states which have the same names and
dates in different orders will represent the same abstract state.
This is quite usual in data refinement, because efficient
representations of data often cannot avoid including superfluous
information.
 
\new On the other hand, each concrete state represents only one
abstract state.  This is usual, because we don't expect to find
superfluous information in the abstract state that does not need to
be represented in the concrete state. It does sometimes
happen that one concrete state represents several abstract states,
% but this is more often than not a sign of a badly-written [Rev 1]
but this is often a sign of a badly-written
specification that has a bias towards a particular implementation.

Having explained what the concrete state space is, and how concrete
states are related to abstract states, we can begin to implement the
operations of the specification.  To add a new name, we increase
$hwm$ by one, and fill in the name and date in the arrays:
\begin{schema}{AddBirthday1}
	\Delta BirthdayBook1 \\
	name?: NAME \\
	date?: DATE
\where
	\forall i: 1 \upto hwm @ name? \neq names(i)
\also
	hwm' = hwm + 1 \\
	names' = names \oplus \{ hwm' \mapsto name? \} \\
	dates' = dates \oplus \{ hwm' \mapsto date? \}
\end{schema}
This schema describes an operation which has the same inputs and
outputs as $AddBirthday$, but operates on the concrete instead of the abstract
state. It is a correct implementation of $AddBirthday$, because of the
following two facts:
\begin{enumerate}
\item	Whenever $AddBirthday$ is legal in some abstract state,
	the implementation $AddBirthday1$ is legal in any corresponding
	concrete state.
\item	The final state which results from $AddBirthday1$ represents
	an abstract state which $AddBirthday$ could produce.
\end{enumerate}
Why are these two statements true?  
The operation $AddBirthday$ is legal exactly if its pre-condition
$name? \notin known$ is satisfied.  If this is so, the predicate
\[ known = \{\,i: 1 \upto hwm @ names(i)\,\} \]
from $Abs$ tells us that $name?$ is not one of the elements $names(i)$:
\[ \forall i: 1 \upto hwm @ name? \neq names(i). \]
This is the pre-condition of $AddBirthday1$.

To prove the second fact, we need to think about the concrete states before
and after an execution of $AddBirthday1$, and the abstract states they
represent according to $Abs$. 
The two concrete states are related by $AddBirthday1$,
and we must show that the two abstract states are related as
prescribed by $AddBirthday$:
\[ birthday' = birthday \cup \{name? \mapsto date?\}. \]
The domains of these two functions are the same, because
\begin{argue}
	\dom birthday' = known' &	invariant after \\
\t1	= \{\,i: 1 \upto hwm' @ names'(i)\,\} & from $Abs'$ \\
\t1	= \{\,i: 1 \upto hwm @ names'(i)\,\} \cup \{names'(hwm')\} &
	    				$hwm' = hwm + 1$ \\
\t1	= \{\,i: 1 \upto hwm @ names(i)\,\} \cup \{name?\} \\
	    \because{$names' = names \oplus \{hwm' \mapsto name?\}$}
\t1	= known \cup \{name?\} &	from $Abs$ \\
\t1	= \dom birthday \cup \{name?\}. & invariant before
\end{argue}
There is no change in the part of the arrays which was
in use before the operation, so for all $i$ in the range $1 \upto hwm$,
\[ names'(i) = names(i) \land dates'(i) = dates(i). \]
For any $i$ in this range,
\begin{argue}
	birthday'(names'(i)) \\
\t1	= dates'(i) & 			from $Abs'$ \\
\t1	= dates(i) & 			$dates$ unchanged \\
\t1	= birthday(names(i)). &		from $Abs$
\end{argue}
For the new name, stored at index $hwm' = hwm+1$,
\begin{argue}
	birthday'(name?) \\
\t1	= birthday'(names'(hwm')) &	$names'(hwm') = name?$ \\
\t1	= dates'(hwm') &		from $Abs'$ \\
\t1	= date?~. &			spec.\ of $AddBirthday1$
\end{argue}
So the two functions $birthday'$ and $birthday \cup \{name? \mapsto date?\}$
are equal, and the abstract states before and after the operation are
guaranteed to be related as described by $AddBirthday$.

The description of the concrete operation uses only notation which has a
direct counterpart in our programming language, so we can translate it
directly into a subroutine to perform the operation:
\[
	\PROC AddBirthday(name: NAME; date: DATE); \\
	\BEGIN \\
\t1		hwm := hwm + 1; \\
\t1		names[hwm] := name; \\
\t1		dates[hwm] := date \\
	\END;
\]

The second operation, $FindBirthday$, is implemented by the following
operation, again described in terms of the concrete state:
\begin{schema}{FindBirthday1}
	\Xi BirthdayBook1 \\
	name?: NAME \\
	date!: DATE
\where
	\exists i: 1 \upto hwm @ \\
\t1		name? = names(i) \land date! = dates(i)
\end{schema}
The predicate says that there is an index $i$ at which the $names$
array contains the input $name?$, and the output $date!$ is the
corresponding element of the array $dates$.  For this to be possible,
$name?$ must in fact appear somewhere in the array $names$: this is
the pre-condition of the operation. 

Since neither the abstract nor the concrete operation changes the
state, there is no need to check that the final concrete state is
acceptable, but we need to check that the pre-condition of
$FindBirthday1$ is sufficiently liberal, and that the output $date!$
is correct.  The pre-conditions of the abstract and concrete operations
are in fact the same: that the input $name?$ is known. The output is
correct because for some~$i$, $name? = names(i)$ and $date! =
dates(i)$, so
\begin{argue}
	date! = dates(i) &		spec.\ of $FindBirthday1$ \\
\t1	= birthday(names(i)) &		from $Abs$ \\
\t1	= birthday(name?). &		spec.\ of $FindBirthday1$
\end{argue}
The existential quantifier in the description of $FindBirthday1$ leads to
a loop in the program code, searching for a suitable value of~$i$:
\[
	\PROC FindBirthday(name: NAME; \VAR date: DATE); \\
\t1		\VAR i: INTEGER; \\
	\BEGIN \\
\t1		i := 1; \\
\t1		\WHILE names[i] \neq name \DO i := i+1; \\
\t1		date := dates[i] \\
	\END;
\]

\looseness=1
The operation $Remind$ poses a new problem, because its output
$cards$ is a {\em set\/} of names, and cannot be directly
represented in the programming language. We can deal with it by
introducing a new abstraction relation, showing how it can be
represented by an array and an integer.  Since this decision about
representation affects the interface between the birthday book
module we are developing and a program that uses it, this
abstraction relation will form part of the documentation of that
interface.  Here is a schema $AbsCards$ that defines the abstraction
relation:
\begin{schema}{AbsCards}
	cards: \power NAME \\
	cardlist: \nat_1 \fun NAME \\
	ncards: \nat
\where
	cards = \{\,i: 1 \upto ncards @ cardlist(i)\,\}
\end{schema}
The concrete operation can now be described: it produces as outputs
$cardlist$ and $ncards$:
\begin{schema}{Remind1}
	\Xi BirthdayBook1 \\
	today?: DATE \\
	cardlist!: \nat_1 \fun NAME \\
	ncards!: \nat
\where
	\{\,i: 1 \upto ncards! @ cardlist!(i)\,\} \\
\t1		\hbox spread-2pt{$= \{\,j: 1 \upto hwm |
				dates(j) = today? @ names(j)\,\}$}
\end{schema}
The set on the right-hand side of the equation contains all the names
in the $names$ array for which the corresponding entry in the $dates$
array is $today?$.
The program code for $Remind$ uses a loop to examine the entries one by one:
\[
	\PROC Remind(today: DATE; \\
\t5			\VAR cardlist:\;\ARRAY [1\upto {}]\OF NAME; \\
\t5			\VAR ncards: INTEGER); \\
\t1		\VAR j: INTEGER; \\
	\BEGIN \\
\t1		ncards := 0; j := 0; \\
\t1		\WHILE j \lt hwm \DO\;\BEGIN \\
\t2			j := j + 1; \\
\t2			\IF dates[j] = today \THEN\;\BEGIN \\
\t3				ncards := ncards + 1; \\
\t3				cardlist[ncards] := names[j] \\
\t2			\END \\
\t1		\END \\
	\END;
\]
The initial state of the program has $hwm = 0$:
\begin{schema}{InitBirthdayBook1}
	BirthdayBook1
\where
	hwm = 0
\end{schema}
Nothing is said about the initial values of the arrays $names$ and
$dates$, because they do not matter. If the initial concrete state
satisfies this description, and it is related to the initial abstract
state by the abstraction schema $Abs$, then
\begin{argue}
	known = \{\,i: 1 \upto hwm @ names(i)\,\} & from $Abs$ \\
\t1	= \{\,i: 1 \upto 0 @ names(i)\,\}	& from $InitBirthdayBook1$ \\
\t1	= \empty,				& $1 \upto 0 = \empty$
\end{argue}
so the initial abstract state is as described by $InitBirthdayBook$.
This description of the initial concrete state can be used to write a
subroutine to initialize our program module:
\[
	\PROC InitBirthdayBook; \\
	\BEGIN \\
\t1		hwm := 0 \\
	\END;
\]

In this direct refinement, we have taken the birthday book specification
and in a single step produced a program module which implements it.
The relationship between the state of the book as described in the
specification and the values of the program variables which represent
that state was documented with an abstraction schema, and this allowed
descriptions of the operations in terms of the program variables to
be derived. These operations were simple enough to implement immediately,
but in a more complex example, rules of operation refinement could be
used to check the code against the concrete operation descriptions.

\section{A simple checkpointing scheme}

This example shows how refinement techniques can be used at
a high level in the design of systems, as well as in
detailed programming. 
What we shall call a {\em database\/} is simply a function from
addresses to pages of data. We first introduce $ADDR$ and $PAGE$ as
basic types:
\[ [ADDR, PAGE]. \]
We define $DATABASE$ as an abbreviation for the set of all
functions from $ADDR$ to $PAGE$:
\[ DATABASE == ADDR \fun PAGE. \]
We shall be looking at a system which -- from the user's point of view
-- contains two versions of a database. Here is a schema describing
the state space:
\begin{schema}{CheckSys}
	working: DATABASE \\
	backup: DATABASE
\end{schema}
This schema has no predicate part: it specifies that the two observations
$working$ and $backup$ may be any databases at all, and need not be
related in any way.

Most operations affect only the working database. For example,
it is possible to access the page at a specified address:
\begin{schema}{Access}
	\Xi CheckSys \\
	a?: ADDR \\
	p!: PAGE
\where
	p!= working(a?)
\end{schema}
This operation takes an address $a?$ as input, and produces as its output
$p!$ the page stored in the working database at that address.  Neither
version of the database changes in the operation.

It is also possible to update the working database with a new page:
\begin{schema}{Update}
	\Delta CheckSys \\
	a?: ADDR \\
	p?: PAGE \\
\where
	working' = working \oplus \{a? \mapsto p? \} \\
	backup' = backup
\end{schema}
In this operation, both an address $a?$ and a page $p?$ are supplied as
input, and the working database is updated so that the page $p?$ is now
stored at address $a?$.  The page previously stored at address $a?$ is
lost.

There are two operations involving the back-up
database. We can take a copy of the working database: this
is the $CheckPoint$ operation: 
\begin{schema}{CheckPoint}
	\Delta CheckSys
\where
	working' = working \\
	backup' = working
\end{schema}
We can also restore the working database to the state it had
at the last checkpoint:
\begin{schema}{Restart}
	\Delta CheckSys
\where 
	working'=backup \\
	backup' = backup
\end{schema}
This completes the specification of our system, and we can begin to
think of how we might implement it.
A first idea might be really to keep two copies of the database, so
implementing the specification directly.
But experience tells us that copying the entire database is an expensive
operation, and that if checkpoints are taken frequently, then the computer
will spend much more time copying than it does accessing and updating
the working database.

A better idea for an implementation might be to keep only one complete
copy of the database, together with a record of
the changes made since creation of this master copy.
The master copy consists of a single database:
\begin{schema}{Master}
	master: DATABASE
\end{schema}
The record of changes made since the last checkpoint is a
{\em partial function} from addresses to pages: it is partial because
we expect that not every page will have been updated since the last
checkpoint.
\begin{schema}{Changes}
	changes: ADDR \pfun PAGE
\end{schema}
The concrete state space is described by putting these two parts together:
\begin{schema}{CheckSys1}
	Master \\
	Changes
\end{schema}
How does this concrete state space mirror our original abstract view?
The master database is what we described as the back-up, and
the working database is $master \oplus changes$, the result
of updating the master copy with the recorded changes. We can record
this relationship with an abstraction schema:
\begin{schema}{Abs}
	CheckSys \\
	CheckSys1
\where  
	backup = master \\
	working = master \oplus changes
\end{schema}
The notation $master \oplus changes$ denotes a function which agrees
with $master$ everywhere except in the domain of $changes$, where it
agrees with $changes$.

How can we implement the four operations? Accessing a page
at address $a?$ should return a page from the working copy of the
database, and according to the abstraction relation,
\[ working(a?) = (master \oplus changes)(a?), \]
so a valid specification of $Access1$ is as follows:
\begin{schema}{Access1}
	\Xi CheckSys1 \\
	a?: ADDR \\
	p!: PAGE
\where
	p! = (master \oplus changes)(a?)
\end{schema}
But we can do a little better than this:
if $a? \in \dom changes$, then
\[ (master \oplus changes)(a?) \]
is equal to $changes(a?)$ and if $a? \notin \dom changes$, then it
is equal to $master(a?)$.
So we can use operation refinement to develop the operation further; it is
implemented by
\[
	\PROC Access(a: ADDR; \VAR p: PAGE); \\
\t1		\VAR r: REPORT; \\
	\BEGIN \\
\t1		GetChange(a, p, r); \\
\t1		\IF r \ne ok \THEN \\
\t2			ReadMaster(a, p) \\
	\END;
\]
What are the operations $GetChange$ and $ReadMaster$? We need give only
their specifications here, and can leave their implementation to a
later stage in the development.
$GetChange$ operates only on the $changes$ part of the state;
it checks whether a given page is present,
returning a report and, if possible, the page itself:
\begin{schema}{GetChange}
	\Xi Changes \\
	a?: ADDR \\
	p!: PAGE \\
	r!: REPORT
\where
	(a? \in \dom changes \land \\
\t1		p! = changes(a?) \land \\
\t1		r! = ok) \lor \\
	(a? \notin \dom changes \land \\
\t1		r! = "not\_present")
\end{schema}
As you will see, this is a specification which could be structured nicely
with the schema $\lor$ operator.
The $ReadMaster$ operation simply returns a page from the $master$ database:
\begin{schema}{ReadMaster}
	\Xi Master \\
	a?: ADDR \\
	p!: PAGE
\where
	p! = master(a?)
\end{schema}
For the $Update$ operation, we want $backup' = backup$, so
\[ master' = backup' = backup = master. \]
Also
$working' = working \oplus \{a? \mapsto p? \}$,
so we want
\[ master' \oplus changes'
	= (master \oplus changes) \oplus \{a? \mapsto p?\}. \]
Luckily, the overriding operator $\oplus$ is associative: it
satisfies the law
\[ (f \oplus g) \oplus h = f \oplus (g \oplus h). \]
If we let $changes' = changes \oplus \{a? \mapsto p?\}$, then
\begin{argue}
	working' = working \oplus \{a? \mapsto p?\} &
					spec.\ of $Update$ \\
\t1	= (master \oplus changes) \oplus \{a? \mapsto p?\} & 
					from $Abs$ \\
\t1	= master \oplus (changes \oplus \{a? \mapsto p? \}) & 
					associativity of $\oplus$ \\
\t1	= master' \oplus changes', & 	spec.\ of $Update1$
\end{argue}
and the abstraction relation is maintained.
So the specification for $Update1$ is
\begin{schema}{Update1}
	\Delta CheckSys1 \\
	a?: ADDR	 \\
	p?: PAGE \\  
\where
	master' = master \\
	changes' = changes \oplus \{a? \mapsto p?\}
\end{schema}
This is implemented by an operation $MakeChange$ which has the same effect
as described here, but operates only on the $Changes$ part of the state.

For the $CheckPoint$ operation, we want $backup' = working$,
so we immediately see that
\[ master' = backup' = working = master \oplus changes. \]
We also want $working' = working$, so 
\[ master' \oplus changes' = master \oplus changes = master'. \]
This equation is solved by setting $changes' = \empty$, since
the empty function $\empty$ is a right identity for $\oplus$,
as expressed by the law
\[ f \oplus \empty = f. \]
So a specification for $CheckPoint1$ is
\begin{schema}{CheckPoint1}
	\Delta CheckSys1
\where
	master' = master \oplus changes \\
	changes' = \empty
\end{schema}
This can be refined to the code
\[ MultiWrite(changes); ResetChanges \]
where $MultiWrite$ updates the $master$ database,
and $ResetChanges$ sets $changes$ to $\empty$.

Finally, for the operation $Restart1$, we have $backup' = backup$, so we need
$master' = master$, as for $Update$.  Again, we want
\[ master' \oplus changes' = master', \]
this time because $working' = backup$,
so we choose $changes' = \empty$ as before:
\begin{schema}{Restart1}
	\Delta CheckSys1
\where
	master' = master \\
	changes' = \empty
\end{schema}
This can be refined to a simple call to $ResetChanges$.

Now we have found implementations for all the operations
of our original specification.
In these implementations, we have used two new sets of operations,
which we have specified with schemas but not yet implemented.
One set, $ReadMaster$ and $MultiWrite$, operates on the $master$ part of
the concrete state, and the other, containing $MakeChange$, $GetChange$,
and $ResetChanges$, operates only on the $changes$ part of the state.
The result is two new specifications for what are in effect modules
of the system, and in later stages they can be developed independently.
Perhaps the $master$ function would be represented by an array of
pages stored on a disk, and $changes$ by a hash table held in
main store.

In mathematics, we can describe data structures with equal ease,
whether they are held in primary or secondary storage. Operations
are described in
terms of their function, and it makes no difference 
whether their execution takes microseconds or hours to finish. Of
course, the designer must be very closely concerned with the
capabilities of the equipment to be used, and it is vital to
distinguish primary storage, which though fast has limited capacity,
from the slower but larger secondary storage. But we regard it as a
strength and not a weakness of the mathematical method that it does
not reflect this distinction.  By modelling only the functional
characteristics of a software module, a mathematical specification
technique encourages a healthy {\em separation of concerns}: it helps
the designer to focus his or her attention on functional aspects, and
to compare different designs, even if they differ widely in
performance.

\vskip\baselineskip 

The rest of this book is a reference manual for the notation and ideas
used in the examples we have looked at here.  In
Chapter~\ref{c:background}, an outline is given of the mathematical
world of sets, relations and functions in which Z operates, and the
way Z specifications describe objects in this world.  These concepts
are applied in Chapter~\ref{c:language}, where an account of the Z
language is given. The language is made usable by the library of
definitions which is implicitly a part of every Z specification,
described in Chapter~\ref{c:library} on `the mathematical tool-kit'.
This chapter contains many laws of the kind we have used in reasoning
about the examples.  Chapter~\ref{c:seqprog} covers the conventions by
which Z specifications are used to describe sequential programs, and
the rules for developing concrete representations of data types from
their mathematical specifications.  The final chapter contains a
summary of the syntax of the Z language described in the manual.

\chapter{Background}\label{c:background}

\new The language of Z specifications is grounded in mathematics,
and this chapter contains a description of the world of mathematical
objects in which specifications have their meaning. It describes
what objects exist, and how relationships between them may be
made into specifications. These two themes are developed more fully
in later chapters:  Chapter~\ref{c:language} deals in detail with
the Z language and how it can be used to express specifications, and
Chapter~\ref{c:library} extends the vocabulary of mathematical
objects into a collection of powerful data types, using the Z
language for the definitions.

\section{Objects and types}\label{s:types}

\new A {\em type\/}\index{type+}% 
	\glossary{[type] A type is an expression of a restricted
	kind that denotes a set. The type of an expression
	determines a set which always contains the value of the
	expression.  There are four kinds of types:  {\em basic
	types}, {\em set types}, {\em Cartesian product types}, and
	{\em schema types}.}
is an expression of a restricted kind: it is either a given set
name, or a compound type built up from simpler types using one of a
small collection of type constructors.  The value of a type is a set
called the {\em carrier\/}%
	\glossary{[carrier] The carrier of a type is the set of all
	the values that can be taken by expressions with that type.}
of the type.  By abuse of language, we often say that an object is a
member of a type when it is a member of the carrier of the type.

\new Every expression that appears in a proper Z specification is
associated with a unique type, and if the expression is defined,
then the value of the expression is a member of (the carrier of) its
type.  Each variable has a type that can be deduced from its
declaration, and there are rules for deriving the type of each kind
of compound expression from the types of its sub-expressions.

\new Types are important because it is possible to calculate
automatically the types of all the expressions in a specification and
check\index{type: checking of} that they make sense.  For example, in the
equation
\[ (0, 1) = \{ 1, 2, 3 \}, \]
the left-hand side is an ordered pair, but the right-hand side is
a set, so (according to the type system of Z) the equation is nonsense.
This is the kind of mistake which can be detected by a type checker.
There is, of course, no guarantee that a specification free from
type errors can be implemented, and still less that it really says
what the customer wants.  The possibility of automatic type-checking
is a strong pragmatic reason for having types in Z, and there are also
theoretical reasons connected with ensuring that every expression in
a specification exists as a set, and avoiding the set-theoretic
paradoxes of Russell and others.

Every Z specification begins with certain objects that play a
part in the specification but have no internal structure of interest.
These atomic objects\index{atomic object} are the members of the
{\em basic types\/}\index{basic type}%
	\glossary{[basic type] A named type denoting a set of objects
	regarded as atomic in a specification.}
or {\em given sets\/}\index{given set} of the specification. Many
specifications have the integers\index{integer: as atomic object} as
atomic objects, and these are members of the basic type $\num$, but
there may be other basic types; for example, a specification of a
filing system might have file-names as atomic objects belonging to
the basic type $FNAME$, and a specification of a language might have
expressions as atomic objects belonging to the basic type $EXP$.

\new Starting with atomic objects, more complex objects can be put
together in various ways.  These composite objects are the members
of composite types, put together with the type
constructors\index{type: constructor} of Z.  There are three kinds of
composite types: set types, Cartesian product types, and schema
types.  The type constructors can be applied repeatedly to obtain
more and more complex types, whose members have a more and more
complex internal structure.

\subsection{Sets and set types}

\new Any set of objects that are members of the same type $t$ is itself
an object in the {\em set type\/}% %FIXED
\index{set: type ($\power$)}\symdex{$\power$}%
	\glossary{[set type] A type $\power t$ containing the sets
	of objects drawn from another type $t$.}
$\power t$. Sets may be written in Z by listing their elements. For example:
\[ \{ 1, 2, 4, 8, 16 \} \]
has type $\power \num$ and is a set of integers, the first five
powers of 2.  They may also be written by giving a property which is
characteristic of the elements of the set.  For example:
\[ \{~p: PERSON | age(p) \geq 16~\} \]
has type $\power PERSON$; it is the set whose members are exactly
those members of the basic type $PERSON$ for which the function
$age$ has value at least $16$. Two sets of the same type
$\power t$ are equal exactly if they have the same members.

\subsection{Tuples and Cartesian product types}

\new If $x$ and $y$ are two objects that are members of the types
$t$ and $u$ respectively, then the ordered pair\index{ordered pair}
$(x, y)$ is an object in the {\em Cartesian product type\/}%
\index{Cartesian product: as type}\symdex{$\cross$}%
	\glossary{[Cartesian product type]
	A type $t_1 \cross t_2 \cross \cdots \cross t_n$ containing 
	ordered $n$-tuples $(x_1, x_2, \ldots, x_n)$
	of objects drawn from $n$ other types.}
$t \cross u$. Similarly,
if $x$, $y$ and $z$ are three objects of types $t$, $u$ and $v$
respectively, then the ordered triple $(x, y, z)$ is an object
with type $t \cross u \cross v$.

More generally, if $x_1$, \dots,~$x_n$ are $n$ objects of types
$t_1$, \dots,~$t_n$ respectively, then the ordered
$n$-tuple\index{tuple} $(x_1, \ldots, x_n)$ is an object of type
$t_1 \cross \cdots \cross t_n$.  If $(y_1, \ldots, y_n)$ is another
$n$-tuple of the same type, then the two are equal exactly if $x_i =
y_i$ for each $i$ with $1 \leq i \leq n$.

\new Note that there is no connection between Cartesian products with
different numbers of terms: for example, the ternary
product $t \cross u \cross v$ is different from the iterated binary
products $t \cross (u \cross v)$ and $(t \cross u) \cross v$:
it is best to think of $t \cross u \cross v$ as an application
of the type constructor $\_ \cross \_ \cross \_$ of three arguments.
Consequently, the triple $(a, b, c)$ is different from both
$(a, (b, c))$ and $((a, b), c)$: in fact, they have different
types. This distinction allows the application of functions of
several arguments to be type-checked\index{type: checking of} more closely.
Although in theory it is possible to have tuples with no components or only
one component, there is no way to write them in Z specifications.

\subsection{Bindings and schema types}

\new If $p$ and $q$ are distinct identifiers, and
$x$ and $y$ are objects of types $t$ and $u$ respectively, then
there is a {\em binding\/}\index{binding}%
	\glossary{[binding] An object with one or more components named by
	identifiers.  Bindings are the elements of {\em schema types}.}
$z = <p \bind x, q \bind y>$ with components\index{component:
of binding} $z.p$ equal to $x$ and $z.q$ equal to $y$. This binding
is an object with the {\em schema type\/}\index{schema: type
($\lblot\ldots\rblot$)}%
	\symdex{$\lblot\ldots\rblot$}%
	\glossary{[schema type] A type
	$\lblot~p_1: t_1; p_2: t_2; \ldots; p_n: t_n~\rblot$ containing
	{\em bindings\/} with components named $p_1$, $p_2$, \dots,~$p_n$
	drawn from other types.}
$\lblot~p: t; q: u~\rblot$. More generally, if $p_1$, \dots,~$p_n$
are distinct identifiers and $x_1$, \dots,~$x_n$ are objects of
types $t_1$, \dots,~$t_n$ respectively, then there is a binding
\[ z = <p_1 \bind x_1, \ldots, p_n \bind x_n> \]
with components $z.p_i = x_i$ for each $i$, $1 \leq i \leq n$. This
binding is an object with the schema type
\[ \lblot~p_1: t_1; \ldots; p_n: t_n~\rblot. \]
The binding $z$ is equal to another binding $w$ of the same type
exactly if $z.p_i = w.p_i$ for each $i$ with $1 \leq i \leq n$.  Two
schema types are regarded as identical if they differ only in the
order in which the components are listed; likewise, two bindings are
equal if they have the same components, regardless of the order in
which they are written down.  The notation $\lblot~x, y: T~\rblot$ is
sometimes used as an abbreviation for $\lblot~x: T; y: T~\rblot$.

\new Bindings are used in the operations of Z which allow instances
of a schema to be regarded as mathematical objects in their own
right: the components of the binding correspond to the components of
the schema.  They are also used in this manual to describe the
meaning of the predicate parts of schemas.  Although the notation
for bindings and schema types is not part of the Z language in the
way $\power$ and $\cross$ are, the concept is implicit in the
operations on schemas provided by the language. The expression
$\theta S$, where $S$ is a schema, has a binding as its value, and
variables with schema types are introduced by declarations like $x:
S$.

\new There are many schema types like $\lblot~x: \num~\rblot$ with
only one component, and their elements are one-component bindings
like $<x \bind 3>$.  There is also a unique schema type
$\lblot~\rblot$ that has no components; its only element is the 
empty binding (which might be written $<>$ if that were not the
notation for the empty sequence).  Schema types with only one
component are associated with schemas with one component, and the
empty schema type is associated with the result of hiding all the
variables of a schema, a possible but not very useful operation.%
\index{type-}

\subsection{Relations and functions}

The three kinds of object introduced so far -- sets, tuples and
bindings -- are the only ones which are fundamental to Z.  Other
mathematical objects can be modelled by combining these three basic
constructions, and Chapter~\ref{c:library} contains definitions which
accomplish this for several important classes of object.

Among the most important mathematical objects are binary relations and
functions, and both are modelled in Z by their {\em graphs}.%
\index{function: modelled by graph}\index{relation: modelled by graph}%
\index{graph: of a function or relation}
	\glossary{[graph] The set of ordered pairs of objects for which a
	binary relation holds. In Z, relations are modelled by their graphs.}
The graph of a binary relation is the set of ordered pairs for which
it holds: for example, the graph of the relation $\_ \lt \_$ on
integers contains the pairs $(0,1)$, $(0,2)$, $(1,2)$, $(\minus 37,
42)$, and so on, but not $(3,3)$ or $(45,34)$.  The identification
between a binary relation and its graph is so strong in Z that we
speak of them as being the same object. The notation $X \rel Y$,
meaning the set of binary relations between the sets $X$ and$Y$, is
defined in Chapter~\ref{c:library} as a synonym for the set
$\power(X \cross Y)$ of subsets of the set $X \cross Y$ of ordered
pairs.

Mathematical functions are a special kind of relation: those which
relate each object on the left to at most one object on the right.
Chapter~\ref{c:library} defines the notation $X \pfun Y$ as a
synonym for the set of relations with this property. They are called
{\em partial\/}\index{partial function}%
	\glossary{[partial function] A partial function from a set
	$X$ to a set $Y$ relates some elements of $X$, but not
	necessarily all of them, each to a unique element of $Y$.
	Compare {\em total\/} functions.}
functions, because they need not give a result for
every possible argument.  The set $X \fun Y$ contains all the {\em
total\/}\index{total function}%
	\glossary{[total function] A total function from a set $X$
	to a set $Y$ relates each element of $X$ to a unique element
	of $Y$.  Compare {\em partial\/} functions.}
functions from $X$ to $Y$: they relate each member of $X$
to exactly one member of $Y$.  The notation $f(x)$ can be used if
$f$ is a function: the value of this expression is that unique
element of $Y$ to which $x$ is related by $f$.  Functions with several
arguments are modelled by letting the set on the left of the arrow
be a Cartesian product: in a sense, they do not have many arguments,
but only one, which happens to be a tuple.

In common with ordinary mathematical practice, Z regards functions
as static relations between arguments and results; this contrasts
with the view encouraged by some programming languages, where
`functions' are methods for computing the result from the argument.
In particular, we can talk quite freely in Z about two functions
being equal -- it simply means that they contain the same ordered
pairs -- even though it is difficult to tell whether two different
algorithms compute the same result from the same argument, and in
general the question is undecidable.  Mathematical functions are a
valuable tool for describing data abstractly, even though they
cannot be represented directly in the memory of a computer.  In
implementing a specification which talks about functions, design
decisions will have to be taken about how the data modelled by
functions is to be represented, but the specification abstracts from
this detail.

The birthday-book specification in Chapter~\ref{c:tutorial} used a
mathematical function $birthday$ to model the relationship between
names and birthdays; later, the implementation used a pair of arrays
to represent the same information.  This use of functions in
specifications can be compared to the use of real numbers to specify
numerical calculations. Even though only some real numbers can be
represented by floating point values, arithmetic on real numbers
provides a convenient language for describing and reasoning about
the calculations that the computer performs.

To make the system of types simple enough for types to be calculated
automatically, it is necessary to disregard some of the information
given in the declaration of a function when calculating its type.
In fact, the type system makes no distinction between functions and
simple binary relations; the two variables $f$ and $g$ declared by
\[ f: A \rel B \\
   g: A \fun B \]
have the same type $\power (A \cross B)$.  This is because functions
are just relations with a certain property, so a relation declared
like $f$ could in fact be a function, perhaps by virtue of its
definition. So the equation $f = g$ makes perfect sense, and if $f$
is indeed a function, the expression $f(a)$ also makes sense.
Deciding whether the definition of $f$ makes it a function is, in
general, as difficult as arbitrary theorem proving, so we cannot
expect an automatic type checker to do it for us.

\section{Properties and schemas}\label{s:propschema}

\new A {\em signature\/}\index{signature}%
	\glossary{[signature] A collection of variables, each with a type.}
is a collection of variables, each with a type. Signatures are created by
declarations, and they provide a vocabulary for making mathematical
statements, which are expressed by {\em predicates}\index{predicate}.%
	\glossary{[predicate] A formula describing a relationship between the
	values of the variables in a {\em signature}.}
For example, the declaration $x, y: \num$ creates a signature with two
variables $x$ and $y$, both of type $\num$. In this signature, the
predicate $x \lt y$ expresses the property that the value of $x$ is
less than the value of $y$.  This will be so when $x$ and $y$ take
certain values -- if, say, $x$ is 3 and $y$ is 5 -- and not when they
take certain other values -- if, say, $x$ is 6 and $y$ is 4. Two
different predicates may express the same property: in the example, the
predicate $y \gt x$ expresses the same property as $x \lt y$.

\new Each signature is naturally associated with a schema type; for
example, the signature created by declaring $x, y: \num$ is
associated with the schema type $\lblot~x, y: \num~\rblot$.  The
values in this type are bindings in which the variables take
different values drawn from their types.  (These bindings are called
`assignments', `interpretations' or `structures' in mathematical
logic; I have avoided these terms because of their different
connotations in computing science.)  A {\em property\/}\index{property+}%
	\glossary{[property] The mathematical relationship expressed
	by a predicate. A property is characterized by the set of
	{\em bindings\/} under which it is true.}
over the signature is characterized by the set of bindings under which
it is true.  For example, the property expressed by $x \lt y$ is
true under the binding $<x \bind 3, y \bind 5>$ and false under the
binding $<x \bind 6, y \bind 4>$.

A predicate expresses a property, and by extension we say a
predicate is true under a binding if the property it expresses is true
under that binding. We say that the binding {\em satisfies\/}%
\index{satisfaction}%
	\glossary{[satisfaction] A binding satisfies a property or
	predicate if the property or predicate is true under the
	binding.}
the property, or the predicate
which expresses it, if the property is true under the binding.  As we
have just seen, there may be more than one way of expressing a
property as a predicate: we say two predicates over a signature are
{\em logically equivalent\/}\index{logical equivalence}%
	\glossary{[logically equivalent] Two predicates are logically
	equivalent if they express the same property; that is, if they
	are true under exactly the same bindings.}
if they express the same property; that is, if they are true under
exactly the same bindings as each other.

A {\em schema\/}\index{schema+}%
	\glossary{[schema] A {\em signature\/} together with a
	{\em property\/} relating the variables of the signature.}
is a signature together with a property over the signature.  The
schema $Aleph$ with the signature and property in our example might
be written
\begin{schema}{Aleph}
        x, y: \num
\where
        x \lt y
\end{schema}
We call $x$ and $y$ the {\em components\/}%
\index{component: of schema}%
	\glossary{[component] The components of a schema are the
	variables that are declared in its signature.}
of $Aleph$. For the moment, we may think of the components of a schema
as being simply the variables in its signature. Later (in Section
\ref{ss:glovar}) we shall revise this definition to bring global
variables into the account.

Roughly speaking, the signature and property parts of a schema
correspond to the declaration and predicate written in the text of
the schema. Sometimes, however, the declaration\index{declaration:
contributes to property} contributes something to the property; for
example, in the schema
\begin{schema}{Beth}
        f: \num \fun \num
\where
        f(3) = 4
\end{schema}
the type of $f$ is $\power(\num \cross \num)$, and the fact that
$f$ is a function is part of the property, as well as the fact
that its value at $3$ is 4. We call the property expressed in a
declaration the {\em constraint\/}\index{constraint: of declaration}%
	\glossary{[constraint] A declaration may require that the
	values of the variables it introduces should satisfy a
	certain property.  This property is the constraint of the
	declaration.}
of the declaration.

\subsection{Combining properties}

The simplest predicates are $true$\index{$true$}, which expresses a
property true under all bindings, and $false$\index{$false$}, which
expresses a property true under no binding.  An
equation\index{equality ($=$)}
\[ E_1 = E_2 \]
expresses the property that the values of the expressions $E_1$
and $E_2$ are equal, and the predicate\index{membership}
\[ E_1 \in E_2 \]
expresses the property that the value of $E_1$ is a member of whatever set
is the value of $E_2$.

These basic predicates can be combined in various ways to express
more complicated properties. For example, the predicate
\[ P_1 \land P_2 \]
expresses the conjunction\index{conjunction: of predicates}
of the properties expressed by the predicates $P_1$ and $P_2$. It is
true exactly when both $P_1$ and $P_2$ are true individually. The
other connectives of the propositional calculus, $\lor$, $\implies$,
$\neg$ and $\iff$, may also be used to combine predicates (see
Section~\ref{s:pred}).

If $x$ is a natural number, the universally quantified%
\index{universal quantifier: for predicates}\index{quantifier+}
predicate
\[ \forall z: \nat @ x \leq z \]
expresses the property that the value of $x$ is less than or equal
to every natural number, i.e.\ that $x$ is zero. The existential
quantifier\index{existential quantifier: for predicates} $\exists$ 
and the unique quantifier\index{unique quantifier: for predicates} 
$\exists_1$ may be used as well as $\forall$.
The most general form of a universally quantified predicate is
\[ \forall D | P @ Q \]
where $D$ is a declaration and $P$ and $Q$ are predicates. $D$
and $P$ together form a schema $S$, and the whole predicate expresses
the following property: that whatever values are taken by the
components of $S$, if the property of $S$ is satisfied, then the
predicate $Q$ will also be satisfied. The components of $S$ are
{\em local variables\/}\index{local variable}
of the whole predicate, in a sense explained in Section~\ref{s:scope}.
\index{property-}\index{schema-}\index{quantifier-}

\subsection{Decorations and renaming}\label{ss:decor}

\new A fundamental operation on schemas is systematic
decoration\index{decoration+}.  If $S$ is a schema, then $S'$ is
the same as $S$, except that all the component names have been
suffixed with the decoration ${}'$. The signature of $S'$ contains a
component $x'$ for each component $x$ of $S$, and the type of $x'$
in $S'$ is the same as the type of $x$ in $S$.

\new From a binding $z$ for this new signature, a binding $z_0$ for the
signature of $S$ can be derived.
In $z_0$, each component $x$ of $S$ is given the value that $x'$
takes in $z$, so that $z_0.x = z.x'$.
The property of $S'$ is true under $z$ exactly if the property of
$S$ is true under the derived binding~$z_0$.

\new For example, if $Aleph$ is the schema we defined before, then
bindings for $Aleph$ have the type $\lblot~x, y: \num~\rblot$, and
bindings for $Aleph'$ have the type $\lblot~x', y': \num~\rblot$.
From the binding $z = <x' \bind 3, y' \bind 5>$ for $Aleph'$ is
derived the binding $z_0 = {<x \bind 3, y \bind 5>}$ for $Aleph$; since
the property of $Aleph$ is true under $z_0$, the property of $Aleph'$
is true under $z$.

There are three standard decorations\index{decoration: standard
(${}'$, $?$, $!$)} used in describing operations on abstract data
types (see
Chapter~\ref{c:seqprog}): ${}'$ for labelling the final state of an
operation, $?$ for labelling its inputs, and $!$ for labelling its
outputs.  Subscript digits may also be used as decorations.  An
identifier or schema may have a sequence of decorations, so the
identifiers $x''$, $x'''$, etc.\ are allowed, as well as the less
useful $x'?$, $x?!$, and so on.  Note that the identifiers $x_1!$ and
$x!_1$ are different.\index{decoration-}

\new Another operation on schemas is renaming\index{renaming+}.
If $S$ is a schema, then
\[ S[y_1/x_1, \ldots, y_n/x_n] \]
is a schema obtained by replacing each component $x_i$ by the
corresponding name $y_i$.  For this to make sense, the identifiers
$x_i$ must be distinct, and they must all be components of the
schema $S$, but the $y_i$'s need not be distinct from each other or
from the components of $S$.  If any two components of the original
schema end up with the same name after the renaming has been done,
they must have the same type.  Such merging can happen because two
components are both renamed with the same identifier, or because a
component is renamed with an identifier that is already in use by
another component that is not renamed.

\new The signature of the schema $S[y_1/x_1, \ldots, y_n/x_n]$ is
obtained from the signature of $S$ by replacing each component $x_i$
by the corresponding $y_i$, with
merging of components that have the same name after renaming.  From
a binding $z$ for this new signature, a binding $z_0$ for the
signature of $S$ can be derived.
In $z_0$, each component that matches one of the $x_i$'s is given
the value that $z$ gives to the corresponding $y_i$. 
The other components take the same value in both bindings.
So $z_0.x_i = z.y_i$ for each $i$, and $z_0.w = z.w$ if $w$ is
distinct from all the $x_i$'s.
The property of the renamed schema is true under the binding $z$
exactly if the property of $S$ is true under the derived binding~$z_0$.

\new For example, the schema $Aleph[y/x]$ has a single component
$y$, because the $x$ component of $Aleph$ is renamed as $y$ and
merges with the original $y$ component.  The binding $z = <y \bind 3>$
has the correct type $\lblot~y: \num~\rblot$, but the binding $z_0 =
<x \bind 3, y \bind 3>$ for $Aleph$ that is derived from it does not
satisfy the property of $Aleph$, so $z$ does not satisfy the
property of $Aleph[y/x]$.\index{renaming-}

\subsection{Combining schemas}\label{ss:combschema}

Two signatures are said to be 
{\em type compatible\/}\index{type: compatibility of}%
	\glossary{[type compatible] Two signatures are type compatible
	if each variable common to both signatures has the same type
	in both of them.  Many of the operations on schemas demand
	that their arguments have type compatible signatures.}
if each variable common to the two has the same type in
both of them. If two signatures have this property, we can
{\em join\/}\index{joining signatures}%
	\glossary{[join] Two {\em type compatible\/} signatures can be
	joined to form a signature that has all the variables of
	each of the original ones, with the same types.}
them to make a larger signature which contains all the variables
from each of them. For example, the two signatures
\[ a: \power X; b: X \cross Y \]
and
\[ b: X \cross Y; c: Z \] 
are type compatible because their only common variable $b$ has
the same type $X \cross Y$ in both of them. They can be joined to
make the signature
\[ a: \power X; b: X \cross Y; c: Z. \]
The new signature contains all the variables of each of the original
ones, with the same types; for this reason, we say that the original
signatures are {\em sub-signatures\/}\index{sub-signature}%
	\glossary{[sub-signature] One signature is a sub-signature of
	another one if the second contains all the variables of the
	first, with the same types.}
of the new one. If one signature is a sub-signature of another one,
a binding $z_1$ for the first can be derived from any %FIXED...
binding $z$ for
the second by simply ignoring the extra components: we call this
binding $z_1$ the {\em restriction\/}\index{restriction}%
	\glossary{[restriction] The restriction $z_1$ of a {\em
	binding\/} $z$ for one signature to another signature is
	defined if the second signature is a {\em sub-signature\/}
	of the first.  Each variable is given the same value in $z_1$
	as it has in $z$, and variables not in the smaller signature
	are ignored.}
of the original binding $z$ to the smaller signature. Conversely, we say
that $z$ is an {\em extension\/}\index{extension: of binding}%
	\glossary{[extension] One {\em binding\/} $z$ is an extension of
	another binding $z_1$ if and only if $z_1$ is a {\em restriction\/}
	of $z$ to a smaller signature.}
of $z_1$ to the larger signature.

To be type compatible, two signatures must give the same type to their
common variables, but this does not mean that the variables must be
declared in the same way, for as we have seen, a declaration can
provide more information than just the type of a variable. As a simple
example, both binary relations between two sets $X$ and $Y$ and functions
from $X$ to $Y$ have the same type $\power(X \cross Y)$, so two
signatures would be type compatible even if one resulted from the
declaration
\[ f: X \rel Y \]
and the other from the declaration
\[ f: X \fun Y. \]

\new Two schemas $S$ and $T$ with type compatible signatures may be
combined with the schema conjunction\index{conjunction: of schemas}
operator to give a new schema $S \land T$.  The signature of this
new schema is the result of joining the signatures of $S$ and $T$,
and its property is in effect the conjunction of the properties of
$S$ and $T$: it is true under any binding $z$ exactly if {\em
both\/} the restriction of $z$ to the signature of $S$ satisfies the
property of $S$ {\em and\/} the restriction of $z$ to the signature
of $T$ satisfies the property of~$T$.

Provided that no component of $S$ has the same name as a global
variable mentioned in the body of $T$, and vice versa (see Section
\ref{ss:glovar}), the schema $S \land T$ can be expanded textually:
the declaration part of the expansion has all the declarations from
both $S$ and $T$ (with duplicates eliminated), and the
predicate part is the conjunction of the predicate parts of $S$ and
$T$.  For example, if schema $Aleph$ is as before, and $Gimel$ is
defined by
\begin{schema}{Gimel}
	y: \num \\
	z: 1 \upto 10
\where
	y = z * z
\end{schema}
then $Aleph \land Gimel$ is schema like this:
\begin{schema*}
	x, y: \num \\
	z: 1 \upto 10
\where
	x \lt y \land y = z * z
\end{schema*}
(Unnamed schemas like this are not really part of the Z syntax.)

Other logical connectives such as $\lor$, $\implies$, and $\iff$ may
also be used to combine two type compatible schemas. They join the
signatures as for $\land$, and combine the properties in a way which
depends on the connective: for example, the
property\index{disjunction: of schemas} of $S \lor T$ is true under a
binding $z$ exactly if either or both of the restrictions of $z$
satisfy the properties of $S$ or $T$ respectively.  The
negation\index{negation: of schemas} $\lnot S$ of a schema $S$ has
the same signature as $S$ but the negation of its property.

Compound schemas resulting from these operations can also be
expanded textually,
but care is necessary if the declaration part contributes to the
property of the schema\index{declaration: contributes to property}.
For example, the negation of the schema $Gimel$ defined above is
\begin{schema*}
	y, z: \num
\where
	z \lt 1 \lor z \gt 10 \lor \\
	y \neq z * z
\end{schema*}
\new This expansion of $\lnot Gimel$ is reached by first making
explicit the contribution made by the declaration part of $Gimel$ to
its property:
\begin{schema}{Gimel}
	y, z: \num
\where
	1 \leq z \leq 10 \land \\
	y = z * z
\end{schema}
Only when this information is made explicit can the predicate part be
negated directly.

The hiding operators\index{hiding operator} $\hide$ and $\project$
provide ways of removing components from schemas. If $S$ is a
schema, and $x_1$, \dots,~$x_n$ are components of $S$ then
\[ S \hide (x_1, \ldots, x_n) \]
is a schema. Its components are the components of $S$, except
for $x_1$, \ldots,~$x_n$, and they have the same types as in
$S$. The property of this schema is true under exactly those
bindings that are restrictions of bindings that satisfy the property
of $S$. Provided there is no clash of variables, the schema can be written
using an existential quantifier: if $Gimel$ is the schema defined above,
then $Gimel \hide (z)$ is the schema
\begin{schema*}
	y: \num
\where
	\exists z: 1 \upto 10 @ y = z * z
\end{schema*}
It is possible (but not very useful) to hide all the components of a
schema: the result is a schema with an empty signature and a property
which is either true or false under the (unique) empty binding,
depending on whether any bindings satisfied the property of the
original schema.

If $S$ and $T$ are schemas with type compatible signatures, then $S
\project T$ is also a schema: it has the signature of $T$, and its
property is satisfied by exactly those bindings which are a
restriction of a binding satisfying the property of $S \land T$. It
is the same as $(S \land T) \hide (x_1, \ldots, x_n)$, where $x_1$,
\dots,~$x_n$ are all the components of $S$ not shared by $T$.

Quantifiers provide another way of hiding components of schemas. If
$D$ is a declaration, $P$ is a predicate, and $S$ is a schema,
then\index{universal quantifier: for schemas}\index{quantifier+}
\[ \forall D | P @ S \] 
is a schema. The schema $S$ must have as components all the
variables introduced by $D$, and they must have the same types. The
signature of the result contains all the components of $S$ except
those introduced by $D$, and they have the same types as in $S$.
The property of the result is derived as follows: for any binding $z$
for the signature of the result, consider all the extensions $z_1$ of
$z$ to the signature of $S$.  If every such
extension $z_1$ which satisfies both the constraint of $D$ and the
predicate $P$ also satisfies the property of $S$, then the original
binding satisfies the property of $\forall D | P @ S$.

The schema $\exists D | P @ S$\index{existential quantifier:
for schemas} has the same signature as $\forall D | P @ S$, but
its property is true under a binding $z$ if at least one of the
extensions of $z$ simultaneously satisfies the constraint of $D$, the
predicate $P$, and the property of $S$. Similarly, the schema
$\exists_1 D | P @ S$\index{unique quantifier: for schemas} has
the same signature, but its property is true under any binding which
can be extended in exactly one way so that these three are
simultaneously satisfied.

Quantified schema expressions can be expanded textually by introducing
a quantifier into the body of the schema. As an example, the expression
\[ \forall z: \num | z \gt 5 @ Gimel \]
can be written as
\begin{schema*}
	y: \num
\where
	\forall z: \num | z \gt 5 @ 
		z \in 1 \upto 10 \land y = z * z
\end{schema*}
Again, it has been necessary to make explicit the information about
$z$ given by its declaration before making the expansion.\index{quantifier-}

\section{Variables and scope}\label{s:scope}

Specifications can contain global variables, components
of schemas, and local variables\index{local variable+}
introduced, for example, by the universal quantifier $\forall$.
The {\em scope rules\/}\index{scope rules+}%
	\glossary{[scope rules] A set of rules which determine what
	identifiers may be used at each point in a specification
	and what declaration each of them refers to.}
of Z define the collection of names which may
be used at each point in the specification, and identify the
declaration to which a name refers at each point.

\subsection{Nested scopes}

\new Like many programming languages (e.g.\ Algol~60, Pascal) and
many formal systems (e.g.\ $\lambda$-calculus, first-order logic), Z
has a system of nested scopes\index{scope: nested}. For each
variable introduced by a declaration, there is a region of the
specification, called the {\em scope\/}%
	\glossary{[scope] The region of a specification in which a
	variable refers to a particular declaration of it.
	Throughout this region, we say that the variable is
	{\em in scope}.}
of the declaration, where the name of the variable refers to this
declaration. We say that the variable is {\em local}\index{local
variable}%
	\glossary{[local variable] A variable is local to a certain
	textual region of a specification if that region contains
	the whole {\em scope\/} of some declaration of the variable.}
to this region of the specification, and that it is {\em in scope\/}
throughout the region.

In many cases, the names of variables which are local to a region in a
specification can be changed without affecting the meaning: for
example, in the predicate
\[ \exists y: \nat @ x \gt y, \]
the name of the variable $y$ can be changed without changing the
property being expressed; this predicate is logically equivalent to
the predicate
\[ \exists u: \nat @ x \gt u. \]
The renaming of the local variables of a universally
quantified predicate is possible because the names themselves are
not part of the meaning of the predicate: we only care about which
bindings make it true.

Sometimes the scope of a declaration has `holes' in it, caused by a
nested declaration of another variable with the same name. For
example, in the predicate
\[ \exists x: \nat @ ((\exists x: \nat @ x \lt 10) \land x \gt 3), \]
the occurrence of $x$ in $x \lt 10$ refers to the inner declaration of
$x$: the whole of the inner quantification is a hole in the scope of
the declaration of $x$ introduced by the outer quantifier.  Where renaming
of local variables is possible, this kind of confusion can be avoided,
and it is usually good practice to do so: our example might be
rewritten as
\[ \exists x: \nat @ 
	((\exists y: \nat @ y \lt 10) \land x \gt 3) \]
by renaming the local variable of the inner quantifier, or even --
since the inner quantification is now independent of $x$ -- as
\[ (\exists x: \nat @ x \gt 3)
	\land (\exists y: \nat @ y \lt 10). \]

There are two special features added to this system of scopes by the
schema notation. The first is that some declarations, those which
call for the inclusion of a schema, do not mention explicitly the
variables being declared. If $Aleph$ is the schema defined by
\begin{schema}{Aleph}
        x, y: \num
\where
        x \lt y
\end{schema}
then $Aleph$ used as a declaration introduces the two variables $x$
and $y$ without naming them explicitly.  The second special feature is
that the components of a schema, although they are in some respects
local to the schema's definition, cannot be renamed without affecting
the meaning. For example, the schema $NewAleph$ defined by
\begin{schema}{NewAleph}
        u, v: \num
\where
        u \lt v
\end{schema}
is different from $Aleph$, because it has different component names.

Nevertheless, the scope of the component names consists only of the
predicate part of the schema, unless the schema is included in a
declaration elsewhere as explained above. Component names are also
used in the notation $a.x$ for selecting
a component $x$ from a binding $a$, but, properly speaking, this is
not a use of the {\em variable\/} $x$, but just of $x$ as an {\em
identifier}. Its meaning does not depend on $x$'s being in scope,
because the information about which selectors are allowed is carried
in the type of $a$.

Other kinds of name can appear in Z specifications besides variables.
{\em Basic types\/}\index{basic type: scope rules} and {\em generic
constants\/}\index{generic constant: scope rules} respect the
nesting of scopes. Basic type names may be global, or may be local
to a generic definition, as described in Section~\ref{s:generics}.
Generic constants are always global. Objects of each of these three
kinds can be hidden by inner declarations of other objects with the
same name, but it is not possible to have two different objects with
the same name at the same level of nesting.\label{p:multidec}

{\em Schema names\/}\index{schema: scope rules} do not have any
nesting of scope. Any name which is defined as a schema may only be
used as such throughout the specification document. The first place in
the specification where the name occurs must be its definition. A
schema can have only one definition, and all uses of the name refer to
this definition.

\subsection{Schemas with global variables}\label{ss:glovar}

\new So\index{global variable+} far, we have been considering
schemas in isolation: the only variables which have appeared in the
predicate part have been the components\index{component: of schema}
of the schema.  This is not the whole story, however, because Z
specifications can also contain global variables that are declared
outside any schema, and these variables can be used in defining
schemas.  The mathematical library of Z declares many such variables,
and in fact we have been taking for granted symbols like $+$ and
$\lt$ that are really global variables from the library.

In addition to using global variables declared as part of the
mathematical library, a specification will often introduce global
variables of its own.  An example might be the specification of a
counter whose value is bounded by some limit. We
might first introduce, by an axiomatic description, a global
variable $limit$ to stand for the maximum value to be taken by the
counter:
\begin{axdef}
	limit: \nat
\where
	limit \leq 65535
\end{axdef}
Incidentally, the limit is itself restricted to be at most 65535. Now
we can define a schema to represent the state space of the counter:
\begin{schema}{Counter}
	value: \nat
\where
	value \leq limit
\end{schema}
The predicate part of this schema mentions both the component $value$
and the global variable $limit$, constraining one to be no greater
than the other.

\new Together with their types, the global variables of a Z
specification form a {\em global signature}.%
\index{global signature}
	\glossary{[global signature] A {\em signature\/} that
	contains all the global variables of a specification with
	their types.}
The axioms that relate the values of the
global variables contribute to a global property of the
specification.  Just like a schema, the global part of a
specification consists of a signature and a property over the
signature.  In the example, $limit$ is a variable in this global
signature, and the axiom $limit \leq 65535$ forms part of the global
property.  

In a schema definition, the predicate part may mention both the
components of the schema itself and global variables.
In effect, it is written with respect to a signature formed by
adding the components of the schema to the global signature, with --
strictly speaking -- special provision to avoid a clash of variables.
In the schema $Counter$, the signature for the predicate part
contains both the global variable $limit$ and the component $value$,
and the predicate $value \leq limit$ mentions both of them.
The global property of the
specification is incorporated in the property of every schema: in
the example, the fact that $limit \leq 65535$ is part of the
property of $Counter$, so we can conclude that $value \leq 65535$.

\new Although the signature of each schema in a specification is
effectively an extension of the global signature, decoration affects
only the schema's own 
components, not its inherited global variables; when schemas are
combined, their global parts merge, so that in $S \land S'$ there is
just one `copy' of the global variables. Also, the expressions
$\theta S$ and $S$ or $\{~S~\}$ (where $S$ is a schema) form bindings
that contain only the components of $S$ and not the global
variables.

\new One way to understand a specification with global variables is
to imagine fixing on one binding for them, so that they take fixed
values that satisfy the global property. The property of each schema
then restricts the components of the schema to take values that bear
a certain relationship to the values of the global variables.
Different choices of values for the global variables will make the
properties of the schemas pick out different ranges of possible
values for the components, but whatever choice is made, it is
applied consistently throughout the specification.

Understood in this way, a specification describes a family with one
member for each binding that satisfies the global property. If this
family has more than one member, we say the specification is {\em
loose}.\index{loose specification} The $Counter$ example is a loose
specification, because the predicate $limit \leq 65535$ does not fix
a single value for the global variable $limit$. The use of loose
specifications for describing families of abstract data types is
described in Section~\ref{s:loose}.

Sometimes it happens that a component of a schema has the same name as
a global variable: in this case, the component hides the global
variable on the predicate part of the schema, which forms a `hole' in
the scope of the global variable. Occurrences of the name in the
predicate part of the schema refer to the component, rather than the
global variable.%
\index{global variable-}\index{local variable-}\index{scope rules-}

\section{Generic constructions}\label{s:generics}

Many mathematical constructions are independent of the elements from
which the construction starts: for example, we recognize sequences of
numbers and sequences of characters as being the same kind of object,
even though the elements they are built from -- numbers and characters
-- are different, and we recognize concatenation of sequences as being
the same operation whatever set the elements are drawn from.
Equally, we can often describe parts of computer systems
independently of the particular data they operate on: a resource
management module, for example, does the same kind of thing whether it
is managing printers or tape drives.

The generic constructs\index{generic constructs} of Z allow such
families of concepts to be captured in a single definition. Z allows
both generic constants,\index{generic constant} like the set of
sequences over a particular set and the operation of concatenation,
and generic schemas,\index{generic schema} like the state space of a
resource manager. In the definition of these generic objects, the
collection of basic types is locally extended with one or more
formal generic parameters,\index{generic parameter} which stand for 
the as-yet-unknown sets of elements on which the definition is based.
Later, when the generic object is used, actual generic
parameters are supplied; these determine the sets which the formal
parameters take as their values.

The following generic schema $Pool$ describes the state space of a
generic resource manager\index{resource management}.  It has the set
$RESOURCE$ of resource units as a formal generic parameter, but
assumes a set $USER$ of user names from its context:
\begin{schema}{Pool[RESOURCE]}
        owner: RESOURCE \pfun USER \\
        free: \power RESOURCE
\where
        (\dom owner) \cup free = RESOURCE \\
        (\dom owner) \cap free = \empty
\end{schema}
The two components of this schema have types which are built from both the
basic types of the specification (e.g.\ $USER$) and the formal
generic parameters (e.g.\ $RESOURCE$):
\[ owner: \power (RESOURCE \cross USER); free: \power RESOURCE. \]

The state space of a particular resource manager can be described as
an instance of this general pattern; an example might be a pool of
disks identified by numbers from 0 to 7:
\[ DiskPool \defs Pool[0 \upto 7]. \]
The actual generic parameter $0 \upto 7$ has been supplied here; its
type is $\power \num$, so the signature
of $DiskPool$ is obtained by substituting\index{substitution of types}
$\num$ for $RESOURCE$ in the signature of $Pool$:
\[ owner: \power (\num \cross USER); free: \power \num. \]
More generally, if the type of the actual parameter is $\power t$,
the signature of the instance of the generic schema is obtained by
substituting $t$ for the formal parameter. If there are several
parameters, the substitutions of actual parameter types are
performed simultaneously.  The property part of the meaning of
$DiskPool$ includes the fact that $owner$ is a partial function from
$0 \upto 7$ to $USER$, and that $free$ is a subset of $0 \upto 7$:
\[ owner \in 0 \upto 7 \pfun USER \\
   free \in \power (0 \upto 7). \]
These constraints are implicit in the declaration of $owner$ and
$free$.  The property of $DiskPool$ also includes instances of the
predicates from $Pool$:
\[ (\dom owner) \cup free = 0 \upto 7 \\
   (\dom owner) \cap free = \empty. \]

More useful than generic schemas are generic
constants:\index{generic constant} several dozen of them
are defined in Chapter~\ref{c:library} to capture such concepts
as relations, functions, and sequences, and the operations on them.
An example is the function $first$ for selecting the 
first element of an ordered pair:
\begin{gendef}[X,Y]
        first: X \cross Y \fun X
\where
        \forall x: X; y: Y @ \\
\t1		first(x,y) = x
\end{gendef}
This has two formal generic parameters $X$ and $Y$, and defines
a family of functions $first$. When one of these functions is used,
we may supply the actual generic parameters explicitly, as in
\[ first[\nat,\nat]~(3,4) = 3, \]
or leave them implicit\index{generic parameter: implicit},
as in the equivalent assertion
\[ first(3,4) = 3. \]
The rules for determining implicit parameters from the context
are given in Section~\ref{ss:genconst}.

A restriction must be obeyed by the definitions of generic constants
for them to be mathematically sound: the definition must uniquely
determine\index{generic constant: uniquely defined} the
value of the constant for each possible value of the formal
parameters. For example, the following definition would not be
allowed, because it does not specify which two elements of $X$ are
chosen as the values of $left$ and $right$ when there are more than
two, nor which is chosen as $left$ when there are exactly two.
What's worse, no choice at all is possible when $X$ is empty or has
only one element.
\begin{gendef}[X]
        left, right: X
\where
        left \neq right
\end{gendef}
The requirement that generic definitions of constants uniquely
determine the values of the constants places a proof obligation
on the author of a specification, but it is one that is easily
repaid when, for example, the constant is a function, and the
predicate part of the definition contains an equation giving its
value at each point of its domain.

\section{Partially-defined expressions}\label{s:partial}

The meaning of a mathematical expression can be explained by saying
what value it takes in each binding: for example, the expression 
$x + y$ takes the value 5 in the binding $<x \bind 2, y \bind 3>$.
An expression need not have a defined value in every binding: for
example, the value of $x \div y$ is not defined in any binding where
$y$ is 0. We call such expressions {\em
partially-defined}.\index{partially-defined expression}%
	\glossary{[partially-defined] A partially-defined expression
	is one that does not have a defined value in every binding
	for its signature.}
The precise meaning of a partially-defined expression can be
explained by saying in which bindings its value is defined, and for
each of these, what value the expression takes.

There are two constructs in Z which form expressions that may be
partially-defined.  One is the application of a partial function
such as the `$\div$' operator to arguments which may not be in its
domain, and the other is the definite-description construct $\mu$
(see page~\pageref{p:lambdamu}).

\new Partially-defined expressions may appear in predicates of the
form $E_1 = E_2$ or $E_1 \in E_2$, and it is necessary to say under
what bindings these predicates are true.  Whenever both $E_1$ and
$E_2$ are defined, the predicates mean exactly what we expect: $E_1
= E_2$ is true if and only if the values of $E_1$ and $E_2$ are equal,
and $E_1 \in E_2$ is true if and only if the value of $E_1$ is a
member of whatever set is the value of $E_2$.  If one or both of
$E_1$ and $E_2$ are undefined, then we say that the predicates $E_1
= E_2$ and $E_1 \in E_2$ are {\em undetermined}:  we do not know
whether they are true or false.  This does not mean that the
predicates have some intermediate status in which they are `neither
true nor false', simply that we have chosen not to say whether they
are true or not.

A common usage in Z specification is the predicate
\[ x \in \dom f \land f(x) = y. \]
As might be expected, this predicate asserts that $x$ is in the domain
of $f$ and the value of $f$ for argument $x$ is $y$. We can reason as
follows: if $x$ is in the domain of $f$, then the conjunct $x \in
\dom f$ will be true, so the whole predicate will be true exactly if
the other conjunct, $f(x) = y$, is true also. If $x$ is not in the
domain of $f$, then $x \in \dom f$ is false, so the whole predicate is
false whether $f(x) = y$ is true or not (in fact, it is undetermined).
The predicate
\[ x \in \dom f \implies f(x) = y \]
is true if {\em either\/} $x$ is outside the domain of $f$, {\em or\/}
the value of $f$ at $x$ is $y$. If the antecedent $x \in \dom f$ is
false, the whole predicate is true, whatever the (undetermined) status
of $f(x) = y$.

Partial functions may be defined by giving their domain and their
value for each argument in the domain. A typical definition might look
like this:
\begin{axdef}
	f: X \pfun Y
\where
	\dom f = S
\also
	\forall x: S @ f(x) = E
\end{axdef}
Here, $E$ is an expression which need only be defined in bindings
satisfying $x \in S$. By fixing the domain of $f$ and its value at
each point on the domain, this definition completely determines the
partial function~$f$.

\chapter{The Z Language}\label{c:language}

The specification language described in this chapter is a minimal
language for specification in the Z style. For practical use, it
needs to be augmented with the basic mathematical definitions in
Chapter~\ref{c:library}, and for some purposes it will need to be
extended, perhaps with programming notations for expressing
operation refinements\index{operation refinement}, or with notations
for expressing synchronization of concurrent
processes\index{concurrency};  but the minimal language described
here will be part of all these extensions, and any extension should
be constructed on a mathematical foundation consistent with the one
used here and presented in Chapter~\ref{c:background}.

\section{Syntactic conventions}

The syntactic description of Z constructs given in this chapter is
intended as a guide to the way the constructs look on paper:  it
treats each construct in isolation, and does not properly respect
the relative binding powers of connectives and quantifiers, for
example.  Also not fully treated are the rules that allow an operator
symbol $\op$ to appear wherever an identifier is allowed, using a
notation such as `$\_ \op \_$'.  A full grammar for Z is given in
Chapter~\ref{c:syntax}, and you should refer to this for the answers
to any detailed syntactic questions like these.

A few extensions\index{syntactic conventions} to
BNF\index{Backus--Naur Form (BNF)} are used to make the syntax
descriptions more readable. The notation \(S; \ldots; S\) stands for
one or more instances of the syntactic class \(S\), separated by
semicolons; similarly, the notation \(S, \ldots, S\) stands for one
or more \(S\)'s separated by commas. Slanted square brackets
\(\lopt~\ropt\) enclose items which are optional. Lists of items
which may be empty are indicated by combining these two notations.

\subsection{Words, decorations and identifiers}

\new A word (\(Word\)\index{\(Word\)}) is the simplest kind of name in a Z
specification:  it is either a non-empty sequence of upper and lower
case letters, digits, and underscores beginning with a letter, or a
special symbol. Words are used as the names of schemas.  An identifier
(\(Ident\)) is a word followed by a decoration
(\(Decoration\)\index{\(Decoration\)}), which is a possibly empty
sequence of ${}'$, $?$ or $!$ characters and subscript digits:
\begin{syntax}
        Ident\index{\(Ident\)} & ::= & Word\;Decoration
\end{syntax}
If a word is used in a specification as the name of a schema, it is
called a {\em schema name\/}\index{schema: name} and is no longer
available for use as in an ordinary identifier. Schemas are named
with words rather than identifiers to allow for systematic
decoration: if $A$ is a schema and we write $A'$, this means a copy
of $A$ in which all the component names have been decorated with
${}'$.  When an identifier which already has a non-empty decoration
is decorated, the two decorations are juxtaposed\index{juxtaposition
of decorations}, with the new decoration on the right.

Some words are given the special status of operator symbols. They
are classified as function symbols (\(In-Fun\)\index{\(In-Fun\)} or
\(Post-Fun\)\index{\(Post-Fun\)}), relation symbols
(\(Pre-Rel\)\index{\(Pre-Rel\)} or \(In-Rel\)\index{\(In-Rel\)}) or
generic symbols (\(Pre-Gen\)\index{\(Pre-Gen\)} or
\(In-Gen\)\index{\(In-Gen\)}).

\subsection{Operator symbols}\label{ss:infix}

\new The\index{operator symbol+}
mathematical notation of Z contains only a few basic
forms of expression, but these are enough to express almost any
mathematical property of interest. For example, here is a predicate which
expresses the fact that the sum of $a$ and $b$ is at least $a$:
\[ (plus(a,b),a) \in geq, \]
and here is a predicate which expresses the
associativity\index{associativity} of addition:
\[ plus(plus(a,b),c) = plus(a,plus(b,c)). \]
These predicates look quite unfamiliar, and any predicate much
more complicated than these would become very difficult to read
if expressed purely in these basic notations.

\new We can sugar the pill by allowing infix symbols as
abbreviations for the basic forms. If instead of $plus(x,y)$ we
write $x + y$, and instead of $(x,y) \in geq$ we write $x \geq y$,
then the two predicates take on a more familiar form:
\[
        a + b \geq a
\also
        (a + b) + c = a + (b + c).
\] 
This is possible because $+$ is an {\em infix function symbol\/} in
Z, and $\geq$ is an {\em infix relation symbol}.  We call all such
special symbols {\em operator symbols}, and classify then into three
groups: function symbols, relation symbols, and generic symbols.

Function symbols are of two kinds: infix function symbols, which
appear between two arguments, and postfix function symbols, such as
the transitive closure operators ${}^+$ and ${}^*$, which follow a
single argument.
An expression such as $a+b$ is an abbreviation for applying the $+$
function to the ordered pair $(a,b)$.
An expression such as $R\star$ is an abbreviation for applying the
${}\star$ function to argument $R$.
There is no need for prefix function
symbols, because ordinary symbols are taken as functions when they
precede an argument.

Each infix function symbol has a priority, a number from 1 to 6
which determines its binding power, with higher
numbers indicating tighter binding. When function symbols of equal
priority are used in the same expression, they
associate\index{association of infix symbols} to the left, so that
$x + y + z$ means $(x + y) + z$.

There are two kinds of relation symbols\index{relation symbol}:
infix and prefix.  Infix
relation symbols have binary relations -- sets of ordered pairs --
as their values.  A predicate may consist of two expressions
separated by an infix relation symbol: the predicate is true if the
values of the two expressions form an ordered pair in the relation.
Prefix relation symbols simply have sets as their values. A
predicate which consists of a prefix relation symbol followed by an
expression is true if the value of the expression is an element of
the set.

Infix relation symbols have no priority or association; instead, a sequence
\[ E_1\;R_1\;E_2\;R_2\;\ldots\;R_{n\minus1}\;E_n, \]
where the $E_i$ are expressions and each $R_i$ is `$=$' or `$\in$'
or an infix relation symbol, is equivalent to the conjunction
\[ E_1\;R_1\;E_2 \land E_2\;R_2\;E_3 \land \cdots
                \land E_{n\minus1}\;R_{n\minus1}\;E_n. \]

Both function and relation symbols may be generic, and when they appear
in expressions or predicates they are implicitly supplied with
actual generic parameters, as described in Section~\ref{ss:genconst}.
In addition, there are infix generic symbols such as $\fun$.
These appear between two set-valued expressions which are actual generic
parameters of the symbol.
For example, in the expression $X \cross Y \fun Z$, the sub-expressions
$X \cross Y$ and $Z$ are generic parameters of the symbol $\fun$.
There are also unary prefix generic
symbols such as $\finset$, which precede a single set-valued expression.
There is no priority among infix generic symbols. They bind less tightly
than any function symbol, and they associate to the right, so
that $A \fun B \fun C$ means $A \fun (B \fun C)$.

\new If a symbol is an operator, then so are all decorated variants
of the symbol: so as well as $+$, the symbols $+'$, $+!$, etc.,\ are
all infix function symbols.  If a schema has an operator symbol as a
component, then decorating the schema produces a new schema that has
a decorated operator symbol as a component.  The syntax in
Chapter~\ref{c:syntax} makes full allowance for this use of
decorated operator symbols, but they are used rarely, so for
simplicity they are omitted in this chapter.

Sometimes it is necessary to name a symbol without applying it to
arguments; this can be done by replacing the arguments with the
special marker `$\_$'. So, for example, $\_ + \_$ is the name of a
function which takes a pair of numbers and adds them; $\_ \geq \_$
is the name of an ordering relation on numbers; and $\_ \fun \_$ is
a generic symbol whose value is the total function space between its
parameters. To avoid confusion, these names must be enclosed within
parentheses\index{parentheses: required around operator symbols}
whenever they appear as part of an expression. Using this notation,
we can make explicit the expression for which each
abbreviation stands:
\begin{display}
    \openup1\jot
    \halign{\strut$#$\hfil
	    &\quad is an abbreviation for\quad $#$\hfil\cr
        \noalign{\vskip -\jot}
        x + y           & (\_ + \_)(x,y)\cr
        x \geq y        & (x,y) \in (\_ \geq \_)\cr
        X \fun Y        & (\_ \fun \_)[X,Y]\cr
        \disjoint x     & x \in (\disjoint \_)\cr
        \finset X       & (\finset \_)[X]\cr
        R^*             & (\_^*) R.\cr}%
\end{display}
The names are also used in declarations: for example,
\[
        \_ + \_: \num \cross \num \fun \num \\
        \_ \geq \_: \num \rel \num
\]
are the declarations of $+$ and $\geq$.

\new Some operator symbols are standard; they are shown in the table
below. Others may be introduced as they are needed, but each symbol
should be used consistently throughout a document. Some
specification tools work with a table of symbols which can be
extended, but there is no standard way of doing this.  Infix
relation symbols are a little special, because any identifier may be
underlined and used as a relation, so that $x \inrel{R} y$ means
$(x, y) \in R$, just as $x \geq y$ means $(x, y) \in (\_ \geq \_)$.
This notation is provided because specifications that use a lot of
binary relations commonly use them both between arguments and as
objects in their own right.

A few standard symbols do not fit in with the pattern described so
far.  The minus sign\index{minus sign ($\minus$)} appears to be both
an infix function symbol, and an ordinary symbol that may be applied
as a function to a single number. The role played by the minus sign
is decided by syntactic context, and its two roles are, in effect,
two different symbols $(\_ \minus \_)$ and $(\minus)$, rather than
two meanings overloaded on one symbol. Wherever possible, the minus
sign is interpreted as an infix operator; the other interpretation
can always be forced with parentheses.

The notation $R\limg S\rimg$ for the relational
image\index{relational image} of $S$ through $R$ is an abbreviation
for the expression $(\_\limg \_\rimg)(R,S)$, in which the symbol
$\_\limg \_\rimg$ is applied to the pair $(R,S)$.  The notation
$R^k$ for iteration\index{iteration} of a relation is an
abbreviation for the expression $iter~k~R$, the application of the
curried function $iter$ to the two arguments $k$ and $R$.  Finally,
the symbols $\filter$, $\setminus$, and $\comp$ are used as
operations in schema expressions as well as infix function symbols
in ordinary expressions.

\paragraph{Standard operator symbols}

Here are the standard operator symbols\index{operator symbol:
standard} of the various kinds:

\begin{list}{}{\labelwidth=0pt \itemindent=-\leftmargin
	\def\makelabel#1{\hspace\labelsep \em #1}}
\zedindent=0pt
\item[Infix function symbols \rm (\(In-Fun\))]%
	\index{\(In-Fun\)}\label{p:stdsym}
\begin{display}
    \openup 1\jot
    \halign{\strut Priority #:\hfil&\quad#\hfil\cr
        \noalign{\vskip -\jot}
        1& $\mapsto$\cr
        2& $\upto$\cr
        3& \symlist{+ \minus \cup \setminus \cat \uplus \uminus}\cr
        4& \symlist{* \div \mod \cap \extract \filter \comp \circ
			\otimes}\cr
        5& \symlist{\oplus \bcount}\cr
        6& \symlist{\dres \rres \ndres \nrres}\cr}%
\end{display}

\item[Postfix function symbols \rm (\(Post-Fun\))]\index{\(Post-Fun\)}
    \[\symlist{{\_^\sim} {\_^*} {\_^+}}\]

\item[Infix relation symbols \rm (\(In-Rel\))]\index{\(In-Rel\)}
    \[\symlist{\neq \notin \subseteq \subset \lt \leq \geq \gt
		\prefix \suffix \inseq \inbag \subbageq \partition}\]

\item[Prefix relation symbols \rm (\(Pre-Rel\))]\index{\(Pre-Rel\)}
    \[\disjoint\]

\item[Infix generic symbols \rm (\(In-Gen\))]\index{\(In-Gen\)}
    \[\symlist{\rel \pfun \fun \pinj \inj \psurj \surj \bij \ffun \finj}\]

\item[Prefix generic symbols \rm (\(Pre-Gen\))]%
				\index{\(Pre-Gen\)}\index{operator symbol-}
    \[\symlist{{\powerone} \id \finset {\finsetone} 
		\seq {\seq_1} \iseq \bag}\]
\end{list}

\kern-\baselineskip

\subsection{Layout}\label{ss:layout}

In the formal text of a Z specification, spaces\index{space: not
significant} are generally not considered to be significant, except
where they serve to separate one symbol from the next.  The break
between one line and the next is significant, because in an
axiomatic description, vertical schema, or generic definition, such
breaks may be used instead of semicolons\index{semicolon: elision
of} in both the declaration part and the predicate part. Several
newlines\index{newline} in succession are always regarded as
equivalent to one, and newlines are ignored before and after infix
function, relation and generic symbols, and before and after the
following symbols:
\[
    \symlist{\semicolon : , | @ {==} {\widehat=} {::=}
		= \in \land \lor \implies \iff \THEN \ELSE \cross %FIXED
		\hide \project \semi \pipe}
\]
In all these places, a semicolon would not be syntactically valid
in any case.

\section{Specifications}\label{s:spec}

A Z specification document consists of interleaved passages of formal,
mathematical text and informal prose explanation.  The formal text
consists of a sequence of paragraphs which gradually
introduce the schemas, global variables and basic types of the
specification, each paragraph building on the ones which come before
it. Except in the case of free type definitions (see
Section~\ref{s:freetype}), recursion is not allowed.

Each paragraph may define one or more names for schemas, basic types,
global variables or global constants.  It may use the names defined
by preceding paragraphs, and the names it defines are available in the
paragraphs which follow.  This gradual building-up of the vocabulary
of a specification is called the principle of {\em definition before
use}.\index{definition before use}%
	\glossary{[definition before use] The principle that the
	first occurrence of a name in a specification must be its
	definition.}
The scope of each global name
extends from its definition to the end of the specification.

In presenting a formal specification, it is often convenient to show
the paragraphs in an order\index{order of paragraphs} different from
the one they would need to have for the rule of definition before use
to be obeyed. Some software tools may be able to perform analysis on
specifications presented in this way, but there must always exist a
possible order which obeys the rule. The account of the language in
this manual assumes that the paragraphs of a specification are
presented in this order.

There are several kinds of paragraph. Basic type definitions,
axiomatic descriptions, constraints, schema definitions, and
abbreviation definitions are described here; generic schema and
constant definitions are described in Section \ref{s:gendefs}; and
free type definitions are described in Section \ref{s:freetype}.

\subsection{Basic type definitions}\label{ss:basictype}

\begin{syntax}
       Paragraph\index{\(Paragraph\)+} & ::= & [ Ident, \ldots, Ident ]
\end{syntax}
A basic type definition\index{basic type: definition} introduces one
or more basic types.  These names must not have a previous global
declaration, and their scope extends from the definition to the end
of the specification.  The names become part of the global
vocabulary of basic types.

An example of a basic type definition is the introduction of $NAME$
and $DATE$ in the birthday book example of Chapter \ref{c:tutorial}:
\[ [NAME, DATE]. \]

\subsection{Axiomatic descriptions}\label{ss:axdef}

\begin{syntax}
    Paragraph\index{\(Paragraph\)} & ::= &
	\qquad \vcenter{\boxpream
	    \boxcontents{Declaration}{Predicate; \ldots; Predicate}
	    \egroup}
\end{syntax}
An axiomatic description\index{axiomatic description} introduces one
or more global variables, and optionally specifies a constraint on
their values. These variables must not have a previous global
declaration, and their scope extends from their declaration to the
end of the specification. The variables become part of the global
signature of the specification. The predicates relate the values of
the new variables to each other and to the values of
variables that have been declared previously, and they become part
of the global property.  

The slanted square brackets \(\lopt\ldots\ropt\) indicate
that the dividing line and the predicate list below it are optional,
so that
\begin{axdef}
        \sf Declaration
\end{axdef}
is an acceptable form of axiomatic description. If the predicate
part is absent, the default is the predicate $true$.

An example of an axiomatic description is the following definition of
the function $square$:
\begin{axdef}
	square: \nat \fun \nat
\where
	\forall n: \nat @ \\
\t1		square(n) = n * n
\end{axdef}

\subsection{Constraints}\label{ss:constraint}

\begin{syntax}
       Paragraph\index{\(Paragraph\)} & ::= & Predicate
\end{syntax}
A predicate\index{predicate: as paragraph} may appear on its own as a
paragraph; it specifies a constraint\index{constraint: as paragraph}
on the values of the global variables that have been declared
previously.  The effect is as if the constraint had been stated as
part of the axiomatic description in which the variables were
introduced.

An example of a constraint is the following predicate, which asserts
that the variable $n\_disks$ has a value less than five:
\[ "n\_disks" \lt 5. \]

\subsection{Schema definitions}\label{ss:schemadef}

\begin{syntax}
Paragraph\index{\(Paragraph\)} & ::= & \qquad \vcenter{
	\boxpream\boxtop{$\sf Schema-Name$}
        \boxcontents{Declaration}{Predicate; \ldots; Predicate}
	\boxend\egroup} \\
\noalign{\vskip\normalbaselineskip}
     Paragraph\index{\(Paragraph\)} & ::= & Schema-Name \defs Schema-Exp
\end{syntax}
These forms\index{schema: definition} introduce a new 
schema name. The word heading the
box or appearing on the left of the definition sign becomes associated
with the schema which is the contents of the box or appears to
the right of the definition sign. It must not have appeared previously
in the specification, and from this point to the end of the specification
it is a schema name. Again, if the predicate part in the vertical
form is absent, the default is $true$.

The right-hand side of the horizontal form of schema
definition\index{schema: horizontal ($\defs$)}\symdex{$\defs$} is a
schema expression\index{schema: expression}, so new schemas may be
defined by combining old ones with the operations of the schema
calculus (see Section~\ref{s:schemaexp}).  The vertical form
\begin{schema}{S}
        D_1; \ldots; D_m
\where
        P_1; \ldots; P_n
\end{schema}
is equivalent to the horizontal form
\[ S \defs [~D_1; \ldots; D_m | P_1; \ldots; P_n~], \]
except that semicolons\index{semicolon: elision of}
may be replaced by line breaks in the vertical
form. The right-hand side of the horizontal form is a simple schema
expression consisting of a schema text in square brackets.

Here is an example of a schema definition taken from the birthday-book
specification:
\begin{schema}{BirthdayBook}
    known: \power NAME \\
    birthday: NAME \pfun DATE
\where
    known=\dom birthday
\end{schema}
This definition may also be written in horizontal form:
\[
	BirthdayBook \defs \\
\t1		[~known: \power NAME; birthday: NAME \pfun DATE | \\
\t2			known=\dom birthday~].
\]
The following definition of $RAddBirthday$ uses a more complex schema
expression on the right-hand side:
\[ RAddBirthday \defs (AddBirthday \land Success) \lor AlreadyKnown. \]

\subsection{Abbreviation definitions}\label{ss:abbrev}

\begin{syntax}
       Paragraph\index{\(Paragraph\)-} & ::= & Ident == Expression
\end{syntax}
An abbreviation definition\index{abbreviation definition
($==$)}\symdex{$==$} introduces a new global constant. The identifier
on the left becomes a global constant; its value is given by the
expression on the right, and its type is the same as the type of the
expression. The scope of the constant extends from here to the end of
the specification. (In fact, this notation is a special case of the
notation for defining generic constants, in which the list of
generic parameters is empty: compare Section~\ref{ss:genconst}.)

This example of an abbreviation definition introduces the name
$DATABASE$ as an abbreviation for the set of functions from $ADDR$ to
$PAGE$:
\[ DATABASE == ADDR \fun PAGE. \]

\section{Schema references}\label{s:schemaref}

\new When a name has been attached to a schema as described in
Section~\ref{ss:schemadef}, it can be used in a schema
reference\index{schema reference} to refer to the schema. A schema
reference can be used as a declaration, an expression, or a predicate,
and it forms a basic element of schema expressions.
\begin{syntax}
Schema-Ref\index{\(Schema-Ref\)}
	& ::= & Schema-Name\;Decoration\;\lopt Renaming \ropt
\also
Renaming & ::= & [Ident/Ident, \ldots, Ident/Ident]
\end{syntax}
A schema reference consists of a schema name, followed by a
decoration\index{decoration} (which may be empty), and an optional
list of renamings. It stands for a copy of the named schema that has
been modified by applying the decoration to all the components, then
renaming components in the result according to the renamings, if any.

\new If a list of renamings $[y_1/x_1, \ldots, y_n/x_n]$ is given,
then the $x_i$'s must be distinct identifiers, and they must all be
components of the schema after the decoration has been applied.  The
$y_i$'s need not be distinct from each other, nor from the
components of the schema being renamed, but if two components of the
schema coincide after renaming, then their types must agree: see
Section~\ref{ss:decor}.

There is another form of schema reference in which actual generic
parameters are supplied to a generic schema: see
Section~\ref{ss:genschema}.

\section{Declarations}\label{s:decl}

Variables are introduced and associated with types by
declarations\index{declaration}.  As explained in
Section~\ref{s:propschema}, a declaration may also require that the
values of the variables satisfy a certain property, which we call the
constraint\index{constraint: of declaration} of the declaration.
There are two kinds of declaration in Z:
\begin{syntax}
        Basic-Decl\index{\(Basic-Decl\)}
                & ::= & Ident, \ldots, Ident : Expression \\
                &  |  & Schema-Ref
\end{syntax}
The first kind introduces a collection of variables that are listed
explicitly in the declaration. In the declaration
\[ x_1, \ldots, x_n: E \]
the expression $E$ must have a set type $\power t$ for some type
$t$. The variables $x_1$, \dots,~$x_n$ are introduced with type
$t$. The values of the variables are constrained to lie in the
set $E$. For example, the declaration
\[ p, q: 1 \upto 10 \]
introduces two variables $p$ and $q$. The expression $1 \upto 10$ has
type $\power \num$, for it is a set of integers. The type of $p$
and $q$ is therefore taken to be $\num$: they are simply integers.
This declaration constrains the values of $p$ and $q$ to lie between
$1$ and~$10$.

The second kind of declaration is a schema reference\index{schema
reference: as declaration}; it introduces the components of the
schema as variables, with the same types as they have in the schema,
and constrains their values to satisfy its property. For example, if
$A$ is the schema
\begin{schema}{A}
        x, y: \num
\where
        x \gt y
\end{schema}
then the declaration $A$ introduces the variables $x$ and $y$,
both of type $\num$, with the property that the value of $x$ is
greater than the value of $y$.

In every context where a single declaration is allowed, a sequence
of declarations may also appear:
\begin{syntax}
        Declaration\index{\(Declaration\)}
                & ::= & Basic-Decl; \ldots; Basic-Decl
\end{syntax}
This declaration introduces all the variables introduced by each
of its constituent basic declarations, with the same types. The
values of these variables are constrained to satisfy all the properties
from the basic declarations. The same variable may be introduced by
several of the basic declarations, provided it is given the 
same type\index{type: compatibility of}\label{p:combdecl}
in each of them: this rule allows schemas that have components in
common to appear in the same declaration. The rule that types must match
allows for the merging of common components.

The scope of the variables introduced by a declaration is
determined by the context in which it appears: the variables may
be global to the whole of the succeeding specification, they may form the
components of a schema, or they may be local to a predicate or expression.
However, the scope\index{declaration: scope rules} never includes the
declaration itself: variables may not be used on the right of a colon,
nor as an actual generic parameter\index{generic parameter} (see
Section~\ref{s:gendefs}), in the declaration which introduces them.

\subsection{Characteristic tuples}\label{ss:chartuple}

A set-comprehension expression has the form
\[ \sf \{~Declaration | Predicate; \ldots; Predicate @ Expression~\} \]
and its value is the set of values taken by the expression when
the variables introduced by the declaration take all values which
satisfy both the constraint of the declaration and the predicates (see
page~\pageref{p:setcomp}).
The expression part may be omitted, and the default is then the
{\em characteristic tuple\/}\index{characteristic tuple}%
	\glossary{[characteristic tuple] The pattern, derived from
	the declaration $D$, for elements of a set comprehension
	$\{~D | P~\}$ that contains no explicit expression.
	Characteristic tuples are also used in the definition of
	lambda- and mu-expressions.}
of the declaration.
Characteristic tuples are also used in the definitions of $\lambda$
and $\mu$ (see page~\pageref{p:lambdamu}).

To find the characteristic tuple of a declaration, first replace each
multiple declaration
\[ x_1, \ldots, x_n: E \]
by the following sequence of simple declarations:
\[ x_1: E; \ldots; x_n: E. \]
Now form the list of representatives\index{representative: in
characteristic tuple} of the basic declarations:
\begin{itemize}
\item The representative of a simple declaration $x: E$ is the
        variable $x$; in the scope of the declaration, this has
	whatever type is given to $x$ by the declaration.

\item The representative of a schema reference 
	\[ A'[E_1, \ldots, E_n][y_1/x_1, \ldots, y_k/x_k] \]
	to a schema $A$, with decoration ${}'$ (which may be empty),
	actual generic parameters $E_1$, \dots,~$E_n$ and renamings
	$y_1/x_1$, \dots,~$y_k/x_k$, is the $\theta$-expression 
	\[ \theta A'[y_1/x_1, \ldots, y_k/x_k] \]
	(see Section~\ref{s:exp}, page~\pageref{p:theta}). 
	In the scope of the declaration, this has a schema type
	whose components are the components of $A$, each with the
	type given to it by $A$.
\end{itemize}
If the list of representatives has exactly one member $E$, then
the characteristic tuple is $E$; its type is simply the type of $E$.
Otherwise, the list of expressions contains
$n \geq 2$ members $E_1$, \dots,~$E_n$, and the characteristic tuple
is the tuple
\[ (E_1, \ldots, E_n). \]
This has type $t_1 \cross \cdots \cross t_n$, where $t_1$, \dots,~$t_n$
are the types of the representatives $E_1$, \dots,~$E_n$ respectively.

\paragraph{Examples}

\begin{itemize}
\item The characteristic tuple of the declaration $x, y: X; z: Z$
        is $(x, y, z)$. If $X$ and $Z$ are basic types, the
        type of this tuple is $X \cross X \cross Z$.

\item The declaration $A$, where $A$ is a schema, has characteristic
        tuple $\theta A$; its type is
        $\lblot~x_1: t_1; \ldots; x_n: t_n~\rblot$,
        where $x_1$, \dots,~$x_n$ are the components of $A$ and
        $t_1$, \dots,~$t_n$ respectively are their types in $A$.

\item The characteristic tuple of the declaration $A[y/x]; A'; x, y: X$
	(where $X$ is a basic type) is the tuple $(\theta A[y/x], \theta
	A', x, y)$. Its type is $t \cross t \cross X \cross X$,
	where $t$ is the type of $\theta A[y/x]$ (and of $\theta A'$).
	This is so whether $x$ is a component of $A$ or not.
\end{itemize}
As the last example illustrates, a variable may be involved in more
than one element of a characteristic tuple. If $A$ is the schema
defined by
\[ A \defs [~x,y:\nat~], \]
then the set expression $\{~A; x:\nat~\}$ is another way of writing the
projection function $(\lambda A @ x)$.
The characteristic tuple of the declaration $A; x:\nat$ is 
$(\theta A, x)$, and both elements depend on $x$.

\section{Schema texts}\label{s:schematext}

A schema text\index{schema: text} consists of a declaration and an
optional list of predicates.
Most Z constructs which introduce variables allow a schema text
rather than simply a declaration, so that a relationship between
the values of the variables can be described. Schema texts appear
in vertical form in axiomatic descriptions and schema definitions,
but they also have a horizontal form:
\begin{syntax}
        Schema-Text\index{\(Schema-Text\)} & ::= &
		Declaration\;\lopt | Predicate; \ldots; Predicate \ropt
\end{syntax}
This form is used after the quantifiers $\forall$, $\exists$,
and $\exists_1$, and in expressions formed with $\lambda$, $\mu$,
and $\{~\}$ (set comprehension).
A schema text in square brackets is the simplest kind of schema expression
(see Section~\ref{s:schemaexp}).
If the optional list of predicates is absent, the default
is the predicate $true$. The scope\index{schema: scope rules}
of the variables introduced
by a schema text depends on the context in which it appears, but
it always includes the predicate part of the schema text.

\section{Expressions}\label{s:exp}

The following pages contain concise descriptions of the basic
forms of expression in Z:
\begin{menu}
        $(\ldots)$, $\{\ldots\}$ & Tuple and set display \prf{tuple} \\
        $\power$, $\cross$      & Power set, Cartesian product \prf{Pcross} \\
        $\{\;|\;@\;\}$   	& Set comprehension \prf{setcomp} \\
        $\lambda$, $\mu$        & Lambda- and mu-expressions \prf{lambdamu} \\
	$\bf let$		& Local definition \prf{letexp} \\
        $\sf Application$       & Function application \prf{apply} \\
        $.$                     & Selection \prf{select} \\
        $\theta$                & Binding formation \prf{theta} \\
        $\sf Schema-Ref$        & Schema reference \prf{srefexp} \\
	$\bf if\;then\;else$	& Conditional \prf{condexp}
\end{menu}
These are the basic forms of expression, but as 
Section~\ref{ss:infix} explains,
operator symbols allow the convenient abbreviation of certain
common kinds of expression. For ease of reference, the rules for
these are given explicitly on their own page:
\begin{menu}
        $\sf Operators$         & Rules for operator symbols \prf{operators}
\end{menu}
Finally, some extra notations are introduced as part of the
mathematical tool-kit in Chapter~\ref{c:library}, and
there is a page which summarizes these:
\begin{menu}
        $<\ldots>$, $\lbag\ldots\rbag$
				& Sequence and bag displays \prf{displays}
\end{menu}
Each page shows the syntax of the expressions, states any scope and
type rules which must be obeyed, and describes the meaning.  The
meaning is fixed by saying in what bindings an expression is
defined, and when it is defined, what its value is (see
Section~\ref{s:propschema} for an explanation of bindings).  For
expressions with optional parts, the defaults are stated.

\new The simplest kinds of expression are
identifiers\index{identifier} and natural numbers
(which are written in decimal notation);
parentheses\index{parentheses: used for grouping} may be used for
grouping in expressions:
\begin{syntax}
        Expression\index{\(Expression\)+}
                & ::= & Ident \\
                &  |  & Number \\
                &  |  & ( Expression )
\end{syntax}
% An identifier may be a local variable of an enclosing expression
% or predicate, a component of a schema (if the expression occurs in
% the definition of the schema), a global variable, a global
% abbreviation or generic constant, a basic type name, or a formal
% generic parameter. 
% Any identifier used must be in scope at the point of use, and the
% scope rules identify a unique declaration or definition for it.  
% Its type and its value in a given binding are
% determined as follows:
% \begin{itemize}
% \item a local or global variable $x$ has the type given to it by its
% 	declaration.  Its value in a binding $z$ is the value $z.x$ assigned
% 	to it by~$z$.
% \item a basic type name or formal generic parameter $X$ has type
% 	$\power X$, because it is a set of $X$'s -- actually the set
% 	of {\em all\/} $X$'s.
% \item an abbreviation has the same type and value as the expression
% 	on the right-hand side of its definition.
% \item a generic constant has the type and value defined in
% 	Section~\ref{ss:genconst}. 
% \end{itemize}
%-----------------------------------------------------
\begin{manpage}\label{p:tuple}
\item[Name]
\begin{name}
        (\ldots) & Tuple\index{tuple ($(\ldots)$)}\symdex{$(\ldots)$} \\
        \{\ldots\} & Set display%
		\index{set: display ($\{\ldots\}$)}\symdex{$\{\ldots\}$}
\end{name}

\item[Syntax]
\begin{syntax}
        Expression\index{\(Expression\)} %
                & ::= & ( Expression, \ldots, Expression ) \\
                &  |  & \{\;\lopt Expression, \ldots, Expression \ropt\;\}
\end{syntax}
To avoid ambiguity with parenthesized expressions, at least two
expressions must appear in a tuple. There is no way to write a tuple
containing fewer than two components. 

To avoid ambiguity with set comprehension (see
page~\pageref{p:setcomp}), the list of expressions in a set display
must not consist of a single schema reference.  A set display
containing a single schema reference $S$ can be written $\{(S)\}$.

\item[Type rules]
In the expression $(E_1, \ldots, E_n)$, if the arguments $E_i$
have types $t_i$, then the expression itself has type
$t_1 \cross \cdots \cross t_n$.

In the expression $\{ E_1, \ldots, E_n \}$, each sub-expression
$E_i$ must have the same type $t$. The type of the expression
is then $\power t$. See Section~\ref{ss:genconst} for
an explanation of what happens when $n = 0$.

\item[Description]
The expression $(x_1, \ldots, x_n)$ denotes an $n$-tuple whose
components are $x_1$, \dots,~$x_n$.

The set $\{ x_1, \ldots, x_n \}$ has as its only members the objects
$x_1$, \dots,~$x_n$. Several of the $x_i$'s may be equal, in which case
the repetitions are ignored; since a set is characterized solely by which
objects are members and which are not, the order in which the
members are listed does not matter.

\item[Laws]
\begin{laws}
        (x_1, \ldots, x_n) = (y_1, \ldots, y_n) %
                \iff x_1 = y_1 \land \cdots \land x_n = y_n
\also
        y \in \{ x_1, \ldots, x_n \} \iff y = x_1 \lor \cdots \lor y = x_n
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:Pcross}
\item[Name]
\begin{name}
        \power & Power set\index{power set ($\power$)}\symdex{$\power$} \\
        \cross & Cartesian product%
		\index{Cartesian product ($\cross$)}\symdex{$\cross$}
\end{name}

\item[Syntax]
\begin{syntax}
        Expression\index{\(Expression\)} %
                & ::= & \power Expression \\
                &  |  & Expression \cross \cdots \cross Expression
\end{syntax}

\item[Type rules]
In the expression $\power E$, the argument $E$ must have a set
type $\power t$. The type of the expression is then $\power (\power
t)$. For example, if $E$ is a set of integers having type $\power
\num$, then $\power E$ has type $\power (\power \num)$ -- it
is a set of sets of integers. 

In the expression $E_1 \cross \cdots \cross E_n$,
each argument $E_i$ must have a set type $\power t_i$. The type
of the expression is then $\power (t_1 \cross \cdots \cross t_n)$.
For example, if $E_1$ has type $\power \num$, and $E_2$ has type
$\power CHAR$, then $E_1 \cross E_2$ has type $\power (\num \cross
CHAR)$ -- it is a set of pairs, each containing an integer and
a character.

\item[Description]
If $S$ is a set, $\power S$ is the set of all subsets of $S$.

If $S_1$, \dots,~$S_n$ are sets, then $S_1 \cross \cdots \cross S_n$
is the set of all $n$-tuples of the form
$(x_1, \ldots, x_n)$,
where $x_i \in S_i$ for each $i$ with $1 \leq i \leq n$. Note 
that, for example, the sets $S \cross T \cross V$ and 
$S \cross (T \cross V)$ and $(S \cross T) \cross V$ 
are considered to be different.

\item[Laws]
\begin{laws}
        S_1 \cross \cdots \cross S_n = %
                \{~x_1: S_1; \ldots; x_n: S_n @ (x_1, \ldots, x_n)~\}
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:setcomp}
\item[Name]
\begin{name}
        \{\;|\;@\;\} & Set comprehension%
		\index{set: comprehension ($\{\;\mid\;\spot\;\}$)}%
		\symdex{$\{\ldots\}$}
\end{name}

\item[Syntax]
\begin{syntax}
        Expression\index{\(Expression\)} %
                & ::= & \{\;Schema-Text\;\lopt @ Expression\ropt\;\}
\end{syntax}

\item[Defaults]
If the expression part is omitted, the default is the characteristic
tuple of the declaration appearing in the schema text part (see
Section~\ref{ss:chartuple}).

\item[Scope rules]
In the expression $\{~S @ E~\}$, the schema text $S$ introduces
local variables; their scope includes the
expression $E$.

\item[Type rules]
In the expression $\{~S @ E~\}$, if the type of the sub-expression
$E$ is $t$, then the type of the expression is $\power t$.

\item[Description]
The members of the set $\{~S @ E~\}$ are the values taken by the
expression $E$ when the variables introduced by $S$ take all possible
values which make the property of $S$ true.

\item[Laws]
\begin{laws}
        y \in \{~S @ E~\} \iff (\exists S @ y = E)
\end{laws}
provided $y$ is not a variable of $S$.
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:lambdamu}
\item[Name]
\begin{name}
        \lambda & Lambda-expression%
		\index{lambda-expression ($\lambda$)}\symdex{$\lambda$} \\
        \mu & Mu-expression\index{definite description ($\mu$)}%
		\index{mu-expression ($\mu$)}\symdex{$\mu$}
\end{name}

\item[Syntax]
\begin{syntax}
        Expression\index{\(Expression\)} %
                & ::= & (\lambda Schema-Text @ Expression) \\
                & ::= & (\mu Schema-Text \;\lopt @ Expression \ropt)
\end{syntax}

\item[Defaults]
In a mu-expression, if the expression part is omitted, the default is
the characteristic tuple\index{characteristic tuple} of the
declaration appearing in the schema text part (see
Section~\ref{ss:chartuple}).

\item[Scope rules]
\new In the expressions $(\lambda S @ E)$ and $(\mu S @ E)$, the schema
text $S$ introduces local variables; their scope
includes the expression $E$.

\item[Type rules]
In the expression $(\lambda S @ E)$, let $t$ be the type of
$E$, and let $t'$ be the type of the characteristic
tuple\index{characteristic tuple} of the
declaration appearing in $S$ (see Section~\ref{ss:chartuple}).
The type of the whole expression is $\power(t' \cross t)$.

In the expression $(\mu S @ E)$, if the type of the sub-expression
$E$ is $t$, then the type of the expression is $t$ also. If the
expression $E$ is omitted, the type of the expression is the type of
the characteristic tuple of the declaration appearing in $S$.

\item[Description]
The expression $(\lambda S @ E)$ denotes a function which takes
arguments of a shape determined by $S$, and yields as its result the
value of $E$. It is equivalent to the set comprehension\index{set:
comprehension} $\{~S @ (T, E)~\}$, where $T$ is the characteristic
tuple of~$S$.

The expression $(\mu S @ E)$ is defined only if there is a unique
way of giving values to the variables introduced by $S$ which makes
the property of $S$ true; if this is so, then its value is the value
of $E$ when the variables introduced by $S$ take these values.

\new Lambda- and mu-expressions must (almost) always be put in
parentheses to avoid ambiguity\index{parentheses: required around
$\lambda$ and $\mu$}.  Without parentheses there would be a
danger of confusion about which `$@$' sign should be associated
with which quantifier.
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:letexp}
\item[Name]
\begin{name}
	{\bf let} & Local definition\index{let-expression ($\bf let$)}%
			\index{local definition ($\bf let$)}
\end{name}

\item[Syntax]
\begin{syntax}
	Expression\index{\(Expression\)}
		& ::= & (\LET Let-Def; \ldots; Let-Def @ Expression)
\also
	Let-Def & ::= & Ident == Expression
\end{syntax}

\item[Scope rules]
In the let-expression $(\LET x_1 == E_1; \ldots; x_n == E_n @ E)$,
the variables $x_1$, \dots,~$x_n$ are local; their scope includes
the expression $E$, but not the expressions $E_1$, \dots,~$E_n$ that
are the right-hand sides of the local definitions.

\item[Type rules]
In the let-expression $(\LET x_1 == E_1; \ldots; x_n == E_n @ E)$,
each local variable $x_i$ has the same type as the corresponding
expression $E_i$. The type of the whole expression is the type of $E$.

\item[Description]
The value of a let-expression is the value taken by its body when
the local variables take the values given by the right-hand sides of
their definitions.  The expression $(\LET x_1 == E_1; \ldots; x_n
== E_n @ E)$ is defined if all the right-hand sides $E_1$, \dots,
$E_n$ are defined and $E$ is also defined in the binding where
$x_1$, \dots,~$x_n$ take the values of $E_1$, \dots,~$E_n$
respectively.  The value of the expression is the value of $E$ in
this binding.

To avoid ambiguity with the predicate form of the ${\bf let}$
operator (see page~\pageref{p:letpred}), a let-expression must
always be enclosed in parentheses\index{parentheses: required around
$\bf let$}.

\item[Laws]
\begin{laws}
(\LET x_1 == E_1; \ldots; x_n == E_n @ E) \\
\t1	= (\mu x_1: t_1; \ldots; x_n: t_n 
		| x_1 = E_1; \ldots; x_n = E_n @ E),
\end{laws}
provided all the $E_i$'s are defined and there is no capture of
variables.
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:apply}
\item[Name]
\begin{name}
        \sf Application & Function application%
		\index{application: of functions}\index{function: application}
\end{name}

\item[Syntax]
\begin{syntax}
        Expression\index{\(Expression\)} & ::= & Expression\;Expression
\end{syntax}

\item[Type rules]
In the expression $E_1~E_2$, the sub-expression $E_1$ must have
type $\power (t_1 \cross t_2)$, and $E_2$ must have type $t_1$,
for some types $t_1$ and $t_2$. The type of the whole expression
is then $t_2$.

\item[Description]
The expression $f~x$ denotes the application of the function
$f$ to the argument $x$. Strictly speaking, $f$ does not
have to be a function, but the expression $f(x)$ is defined if and
only if there is a unique value $y$ such that $(x, y) \in f$ (i.e.\
$f$ is `functional at $x$'), and the value of the expression
is this value $y$. 

Function applications are often written with parentheses
around the argument, as in $f(x)$, but this is just a special case
of the rule that allows brackets to be added around any expression.
Application associates to the left, so $f~x~y$ means $(f~x)~y$:
when $f$ is applied to $x$, the result is a function, and this
function is applied to $y$.

\item[Laws]
\begin{laws}
        (\exists_1 y: Y @ (x, y) \in f) \implies \\
\t1		(x, f(x)) \in f \land \\
\t1		f(x) = (\mu y: Y | (x, y) \in f)
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:select}
\item[Name]
\begin{name}
        . & Selection\index{selection ($.$)}\symdex{$.$}
\end{name}

\item[Syntax]
\begin{syntax}
        Expression\index{\(Expression\)} & ::= & Expression\;.\;Ident
\end{syntax}

\item[Type rules]
In the expression $E.y$, the sub-expression $E$ must have a
schema type of the form $\lblot~x_1: t_1; \ldots; x_n: t_n~\rblot$, and
the identifier $y$ must be identical with one of the component
names $x_i$, for some $i$ with $1 \leq i \leq n$. The type of
the expression is the corresponding type $t_i$.

\item[Description]
This is the notation for selecting a component from a binding.
If $a$ is the binding $<x_1 \bind v_1, \ldots, x_n \bind v_n>$
and $y$ is identical with $x_i$, then the value of $a.y$ is $v_i$.

Normally the component selected is named by an identifier, but if a
schema has components named by infix symbols such as $+$, the
notation $a.(\_ + \_)$ may be used to select them also.

\item[Laws]
\begin{laws}
        a \in S \implies a.y = (\lambda S @ y)(a)
\end{laws}
If $a$ and $b$ have type $\lblot~x_1: t_1; \ldots; x_n: t_n ~\rblot$,
then
\begin{laws}
	a = b \iff a.x_1 = b.x_1 \land \cdots \land a.x_n = b.x_n~.
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:theta}
\item[Name]
\begin{name}
        \theta & Binding formation%
		\index{binding: formation ($\theta$)}\symdex{$\theta$}
\end{name}

\item[Syntax]
\begin{syntax}
Expression\index{\(Expression\)} 
	& ::= & \theta\;Schema-Name\;Decoration\;\lopt Renaming \ropt \\
Renaming & ::= & [Ident/Ident, \ldots, Ident/Ident]
\end{syntax}

\item[Type rules]
In the expression $\theta S'$, in which the symbol ${}'$ stands for
a (possibly empty) decoration, let the components of $S$ be
$x_1$, \dots,~$x_n$. The variables $x_1'$, \dots,~$x_n'$ must be in scope:
let their types be $t_1$, \dots,~$t_n$. The type of the expression
is the schema type
$\lblot~x_1: t_1; \ldots; x_n: t_n~\rblot$.
Note that the components in this type have names without the decoration:
this means that $\theta S'$ has the same type as~$\theta S$.

If a renaming $[q_1/p_1, \ldots, q_k/p_k]$ is present, then the list
of variables that must be in scope is modified by substituting each
$q_j$ for the corresponding $p_j$.  The type of the
expression is formed from the types of these variables; as before, its
components are the undecorated and unmodified names~$x_1$,
\dots,~$x_n$.

\item[Description]
The value of the expression $\theta S'$ in a binding $u$ is itself
a binding $z$ with the schema type shown above; for each $i$,
$1 \leq i \leq n$, the component $z.x_i$ is the value of the variable
$x_i'$ in the binding $u$, so that $z$ is the binding
$<x_1 \bind u.x_1', \ldots, x_n \bind u.x_n'>$.
If the decoration is empty, then it is the values of the undecorated
variables $x_i$ themselves which form the components~$z.x_i$.

Note that the types of the components $x_1$, \dots,~$x_n$ are taken
from the current environment, and not from the schema $S$. There is no
guarantee that their values satisfy the property of $S$, or even that
the predicate $\theta S \in S$ is well-typed. If a new schema $T$ is
defined~by
\[ T \defs S' \]
then $\theta T$ will have the type $\lblot~x_1': t_1; \ldots; x_n':
t_n~\rblot$, in which the component names are decorated; this is
different from the type of~$\theta S'$.

The form of theta-expression that contains a renaming is allowed
mostly to provide a characteristic tuple for schema references that
use renaming (see Section~\ref{ss:chartuple}).  In this form, it is
the values of the components after decoration and renaming that are
used to form the value of the expression.

\item[Laws]
\begin{laws}
        (\theta S').x_i = x_i' \\
        \theta S' = \theta S \iff x_1' = x_1 \land \cdots \land x_n' = x_n
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:srefexp}
\item[Name]
\begin{name}
        \sf Schema-Ref & Schema references as
		expressions\index{schema reference: as expression}
\end{name}

\item[Syntax]
\begin{syntax}
        Expression\index{\(Expression\)} & ::= & Schema-Ref
\end{syntax}
where the decoration and renaming parts of the schema reference are empty.

\item[Type rules]
The type of the schema reference $S$ used as an expression is
\[ \power~\lblot~x_1: t_1; \ldots; x_n: t_n~\rblot. \]
where $S$ has components $x_1$, \dots,~$x_n$ with types $t_1$,
\dots,~$t_n$ respectively.

\item[Description]
A schema reference may be used as an expression: its value is
the set of bindings in which the values of the components obey
the property of the schema. The schema reference $S$ used as an
expression is equivalent to the set comprehension 
$\{~S @ \theta S~\}$.
The generic case is given as a law below.

\item[Laws]
\begin{laws}
     S[E_1, \ldots, E_n] = \{~S[E_1, \ldots, E_n] @ \theta S~\}
\end{laws}
\end{manpage}
%-----------------------------------------------------
\def\IFTHENELSE{$\bf if\;then\;else$}
\begin{manpage}\label{p:condexp}
\item[Name]
\begin{name}
	\bf if\;then\;else & Conditional expression%
		\index{\IFTHENELSE}%
		\index{conditional expression ($\bf if$)}
\end{name}

\item[Syntax]
\begin{syntax}
	Expression & ::= &
		\IF Predicate \THEN Expression \ELSE Expression
\end{syntax}

\item[Type rules]
In the expression $\IF P \THEN E_1 \ELSE E_2$, the types of $E_1$
and $E_2$ must be the same.  Their common type is the type of the
whole expression.

\item[Description]
If the predicate $P$ is true, then the value of the conditional
expression 
\[ \IF P \THEN E_1 \ELSE E_2 \]
is the same as the value of $E_1$; otherwise, its value is the same
as the value of $E_2$.

\item[Laws]
\begin{laws}
	P \implies \IF true \THEN E_1 \ELSE E_2 = E_1
\also
	\lnot P \implies \IF false \THEN E_1 \ELSE E_2 = E_2
\also
	\IF P \THEN E \ELSE E = E
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:operators}
\item[Name]
\begin{name}
        \sf Operators & Rules for infix and postfix function symbols%
		\index{function: symbol}\index{operator symbol}
\end{name}

\item[Syntax]
\begin{syntax}
        Expression\index{\(Expression\)}
                & ::= & Expression\;In-Fun\;Expression \\
                &  |  & Expression\;Post-Fun
\end{syntax}

\item[Type rules]
In the expression $E_1 \op E_2$, where $\op$ is an infix function
symbol, the type of $\op$ must be
$\power ((t_1 \cross t_2) \cross t_3)$,
and the types of the sub-expressions $E_1$ and $E_2$ must
be $t_1$ and $t_2$ respectively, for some types $t_1$, $t_2$ and
$t_3$. The type of the whole expression is $t_3$. Note that the
type of $\op$ is that of a function from $t_1 \cross t_2$ to~$t_3$.

In the expression $E~\op$, where $\op$ is a postfix function symbol,
the type of $\op$ must be $\power (t_1 \cross t_2)$, and the type
of $E$ must be $t_1$, for some types $t_1$ and $t_2$. The type
of the whole expression is $t_2$. Note that the type of $\op$ is
that of a function from $t_1$ to $t_2$.

\item[Description]
The expression $E_1 \op E_2$ is an abbreviation for
$(\_ \op \_)(E_1, E_2)$, the application of $\op$ to the pair
$(E_1, E_2)$. According
to the rules for function application, it is defined exactly when
there is a unique $y$ such that $((E_1, E_2), y) \in (\_ \op \_)$,
and its value is this $y$.

The expression $E~\op$ is an abbreviation for $(\_~\op)~E$, the application
of $\op$ to $E$. It is defined exactly when there is a unique $y$
such that $(E, y) \in (\_~\op)$, and its value is this $y$.

\item[Laws]
\begin{laws}
	E_1 \op E_2 = (\_ \op \_)(E_1, E_2)
\also
	E~\op = (\_~\op)~E
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:displays}
\item[Name]
\begin{name}
	<\ldots> & Sequence display%
		\index{sequence: display ($<\ldots>$)}%
		\symdex{$<\ldots>$} \\
	\lbag\ldots\rbag & Bag display%
		\index{bag: display ($\lbag\ldots\rbag$)}%
		\symdex{$\lbag\ldots\rbag$}
\end{name}

\item[Syntax]
\begin{syntax}
        Expression\index{\(Expression\)-} %
                & ::= & <\;\lopt Expression, \ldots, %
                                                Expression \ropt\;> \\
                &  |  & \lbag\;\lopt Expression, \ldots, %
                                                Expression \ropt\;\rbag
\end{syntax}

\item[Type rules]
In the expression $< E_1, \ldots, E_n >$, all the sub-expressions
$E_i$ must have the same type $t$. The type of the whole expression
is $\power (\num \cross t)$; note that this is the type of a sequence
of elements of $t$.

In the expression $\lbag E_1, \ldots, E_n \rbag$, all the sub-expressions
$E_i$ must have the same type $t$. The type of the whole expression
is $\power (t \cross \num)$; note that this is the type of a bag
of elements of $t$.

\item[Description]
For a full description of these forms of expression, see the pages in
Chapter~\ref{c:library} about sequences (page~\pageref{p:seq}) and
bags (page~\pageref{p:bag}) respectively.
The expressions are defined only if all the
sub-expressions $E_i$ are defined, and the value is then a sequence or
bag made from these elements.

\item[Laws]
\begin{laws}
        < x_1, x_2, \ldots, x_n> = %
                \{1 \mapsto x_1, 2 \mapsto x_2, \ldots, n \mapsto x_n\}
\also
        \lbag x_1, x_2, \ldots, x_n\rbag = %
                \{x_1 \mapsto k_1, x_2 \mapsto k_2, \ldots, x_n \mapsto k_n\}
\end{laws}
where for each $i$, the element $x_i$ appears $k_i$ times in the list
$x_1$, $x_2$, \dots,~$x_n$.
\end{manpage}
%-----------------------------------------------------
\section{Predicates}\label{s:pred}

The following pages contain concise descriptions of the various
forms of predicate\index{predicate} in Z:
\begin{menu}
        $=, \in$			& Equality, membership \prf{eqmem} \\
        $\neg, \land, \lor, \implies, \iff$
					& Propositional connectives
						\prf{propcon} \\
        $\forall, \exists, \exists_1$	& Quantifiers \prf{quant} \\
	$\bf let$			& Local definition \prf{letpred} \\
        $\sf Schema-Ref$                & Schema reference \prf{srefpred}
\end{menu}
These are the basic forms of predicate, but as Section~\ref{ss:infix}
explains, relation symbols allow the convenient abbreviation of
certain common kinds of predicate. For ease of reference, the rules
for these are given explicitly:
\begin{menu}
        $\sf Relations$			& Rules for relation symbols
						\prf{relations}
\end{menu}
Each page shows the syntax of the predicates, states any scope and
type rules which must be obeyed, and describes the meaning.  The
meaning is fixed by saying what bindings satisfy the predicate; see
Section~\ref{s:propschema} for an explanation of `bindings'.
For predicates with optional parts, the defaults are stated.

Parentheses\index{parentheses: used for grouping} may be used
for grouping in predicates, and the predicates $true$ and $false$ are
the logical constants, satisfied by every binding and by no
binding respectively:
\begin{syntax}
        Predicate\index{\(Predicate\)+} %
		& ::= & ( Predicate ) \\
		&  |  & {\it true}\index{$true$} \\
		&  |  & {\it false}\index{$false$}
\end{syntax}
%-----------------------------------------------------
\begin{manpage}\label{p:eqmem}
\item[Name]
\begin{name}
        =   & Equality\index{equality ($=$)}\symdex{$=$} \\
        \in & Membership\index{membership ($\in$)}\symdex{$\in$}
\end{name}

\item[Syntax]
\begin{syntax}
        Predicate\index{\(Predicate\)} %
                & ::= & Expression = Expression \\
                &  |  & Expression \in Expression
\end{syntax}

\item[Type rules]
In the predicate $E_1 = E_2$, the expressions $E_1$ and $E_2$
must have the same type. In the predicate $E_1 \in E_2$, if $E_1$
has type $t$, then $E_2$ must have type $\power t$.

\item[Description]
The predicate $x = y$ is true if $x$ and $y$ are the same object.
The predicate $x \in S$ is true if the object $x$ is a member
of the set $S$.

\item[Laws]
\begin{laws}
        x = x \\
        x = y \implies y = x \\
        x = y \land y = z \implies x = z
\end{laws}
If $S$ and $T$ are subsets of $X$,
\begin{laws}
        (\forall x: X @ x \in S \iff x \in T) \iff S = T.
\end{laws}
If $x$ and $y$ are elements of $X$,
\begin{laws}
	(\forall S: \power X @ x \in S \iff y \in S) \iff x = y.
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:propcon}
\item[Name]
\begin{name}
        \lnot    & Negation\index{negation: of predicates} \\
        \land    & Conjunction\index{conjunction: of predicates} \\
        \lor     & Disjunction\index{disjunction: of predicates} \\
        \implies & Implication\index{implication: for predicates} \\
        \iff     & Equivalence\index{equivalence: of predicates}%
	\index{negation ($\neg$)!}\index{conjunction ($\land$)!}%
	\index{disjunction ($\lor$)!}\index{implication ($\implies$)!}%
	\index{equivalence ($\iff$)!}%
	\symdex{$\lnot$}\symdex{$\land$}\symdex{$\lor$}%
	\symdex{$\implies$}\symdex{$\iff$}
\end{name}

\item[Syntax]
\begin{syntax}
        Predicate\index{\(Predicate\)} %
                & ::= & \lnot Predicate \\
                &  |  & Predicate \land Predicate \\
                &  |  & Predicate \lor Predicate \\
                &  |  & Predicate \implies Predicate \\
                &  |  & Predicate \iff Predicate
\end{syntax}
These connectives are shown in decreasing order of binding power; the
connective $\implies$ associates to the right, and the other binary
ones associate to the left.

\item[Description]
These are the connectives\index{connective} of propositional
logic; they allow complex
predicates to be built from simpler ones in such a way that the
truth or falsity of the compound in some binding depends only
on the truth or falsity of the parts in the same binding. For
example, the predicate $P_1 \land P_2$ is true in any binding
if and only if both $P_1$ and $P_2$ are true in that binding.
The following table lists the circumstances under which each kind
of compound predicate is true:
\begin{display}
    \openup1\jot
    \halign to\linewidth{\strut#\hfil&\qquad#\hfil\tabskip=0pt plus1fil\\
	\noalign{\vskip-1\jot}
        $\lnot P$               & $P$ is not true\\
        $P_1 \land P_2$         & Both $P_1$ and $P_2$ are true\\
        $P_1 \lor P_2$          & Either $P_1$ or $P_2$ or both are true\\
        $P_1 \implies P_2$      & Either $P_1$ is not true or
                                  $P_2$ is true or both\\
        $P_1 \iff P_2$          & Either $P_1$ and $P_2$ are both true,
				  or they are both false.\\}%
\end{display}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:quant}
\item[Name]
\begin{name}
        \forall   & Universal quantifier%
			\index{universal quantifier: for predicates} \\
        \exists   & Existential quantifier%
			\index{existential quantifier: for predicates} \\
        \exists_1 & Unique quantifier%
			\index{unique quantifier: for predicates}%
	\index{universal quantifier ($\forall$)!}%
	\index{existential quantifier ($\exists$)!}%
	\index{unique quantifier ($\exists_1$)!}%
	\symdex{$\forall$}\symdex{$\exists$}\symdex{$\exists_1$}
\end{name}

\item[Syntax]
\begin{syntax}
        Predicate\index{\(Predicate\)} %
                & ::= & \forall Schema-Text @ Predicate \\
                &  |  & \exists Schema-Text @ Predicate \\
                &  |  & \rm\exists_1\sf Schema-Text @ Predicate
\end{syntax}
The predicate governed by these quantifiers extends as far as possible
to the right; the quantifiers bind less tightly than any of the
propositional connectives.

\item[Scope rules]
In the predicates $\forall S @ P$, $\exists S @ P$, and
$\exists_1 S @ P$, the schema text $S$ introduces
local variables; their scope includes the
predicate $P$.

\item[Description]
These are the quantifiers\index{quantifier} of predicate
logic.  The predicate $\forall S @ P$
(pronounced `For all $S, P$') is true if, whatever values are taken by
the variables introduced by $S$ which make the property of $S$ true,
the predicate $P$ is true as well. The predicate $\exists S @ P$
(pronounced `There exists $S$ such that $P$') is true if there is at
least one way of giving values to the variables introduced by $S$ so
that both the property of $S$ and the predicate $P$ are true; the
predicate $\exists_1 S @ P$ (pronounced `There is exactly one $S$
such that $P$') is true if there is exactly one such way of giving
values to the variables introduced by $S$.

\item[Laws]
\begin{laws}
     (\forall D | P @ Q) \iff (\forall D @ P \implies Q) \\
     (\exists D | P @ Q) \iff (\exists D @ P \land Q) \\
     (\exists_1 D | P @ Q) \iff (\exists_1 D @ P \land Q)
\also
     (\exists S @ P) \iff \lnot (\forall S @ \lnot P)
\also
     (\exists_1 x: A @ \ldots x \ldots) \iff \\
\t1       (\exists x: A @ \ldots x \ldots \land %
                (\forall y: A | \ldots y \ldots @ y = x))
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:letpred}
\item[Name]
\begin{name}
	{\bf let} & Local definition\index{let-expression ($\bf let$)}
\end{name}

\item[Syntax]
\begin{syntax}
	Predicate\index{\(Predicate\)}
		& ::= & (\LET Let-Def; \ldots; Let-Def @ Predicate)
\also
	Let-Def & ::= & Ident == Expression
\end{syntax}

\item[Scope and type rules] 
In the predicate $(\LET x_1 == E_1; \ldots; x_n == E_n @ P)$, the
variables $x_1$, \dots,~$x_n$ are local; their scope includes the
predicate $P$, but not the expressions $E_1$, \dots,~$E_n$ that are
the right-hand sides of the local definitions. Each local variable
$x_i$ has the same type as the corresponding expression $E_i$.

\item[Description]
The predicate $(\LET x_1 == E_1; \ldots; x_n == E_n @ P)$ is true
in a binding $z$ if and only if the predicate $P$ is true in the
binding obtained by augmenting $z$ so that each variable $x_i$ takes
the value of the corresponding expression $E_i$.  If any of the
$E_i$'s is undefined, the truth of the whole predicate is
undetermined.

The $\bf let$ operator may also be used to form expressions: see
page~\pageref{p:letexp}.

\item[Laws]
\begin{laws}
(\LET x_1 == E_1; \ldots; x_n == E_n @ P) \\
\t1	\iff (\exists_1 x_1: t_1; \ldots; x_n: t_n 
		| x_1 = E_1; \ldots; x_n = E_n @ P),
\end{laws}
provided all the $E_i$'s are defined and there is no capture of
variables.
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:srefpred}
\item[Name]
\begin{name}
        \sf Schema-Ref & Schema references as predicates%
			\index{schema reference: as predicate}%
			\index{pre-condition: operator}
\end{name}

\item[Syntax]
\begin{syntax}
        Predicate\index{\(Predicate\)} 
		& ::= & Schema-Ref \\
		&  |  & \pre Schema-Ref
\end{syntax}
% In the form `\(\pre Schema-Ref\)', the decoration and renaming parts
% of the schema reference must be empty.

\item[Scope and type rules]
In the predicate $S'$, where $S$ is a schema name, all the components
of the decorated schema $S'$ must be in scope, and they must have the
same types as in the signature of the schema.

In the predicate `$\pre S$', where $S$ is a schema, all the components
of the schema $S$ except those decorated with a single ${}'$ or $!$
must be in scope, and must have the same type as in the signature of
the schema.

\item[Description]
A schema reference $S'$ may be used as a predicate: it is true
in exactly those bindings which, when restricted to the signature
of the schema, satisfy its property. It is effectively equivalent
to the predicate $\theta S' \in S$; here $S$ as an expression
means $\{~S @ \theta S~\}$. The generic case is given as a
law below.

The predicate `$\pre S$' is used when $S$ is a schema describing an
operation under the conventions explained in Chapter~\ref{c:seqprog}.
It is equivalent to the predicate $PreS$, where $PreS$ is a schema
defined by
\[ PreS \defs \pre S \]
(compare page~\pageref{p:pre}). If $State: Exp $ describes the state
space of the operation $S$ and its output is $y!: Y$, the predicate
`$\pre S$' is also equivalent to
\[ (\exists State'; y!: Y @ S). \]

\item[Laws]
\begin{laws}
        S'[E_1, \ldots, E_n] \iff
		\theta S' \in \{~S[E_1, \ldots, E_n] @ \theta S~\}
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:relations}
\item[Name]
\begin{name}
        \sf Relations & Rules for relation symbols\index{relation: symbol}
\end{name}

\item[Syntax]
\begin{syntax}
        Predicate\index{\(Predicate\)-} %
                & ::= & Expression\;Rel\;Expression\;Rel\;\ldots\;%
						Rel\;Expression \\
                &  |  & Pre-Rel\;Expression
\end{syntax}

\item[Type rules]
In the predicate $E_1\;R\;E_2$, where $R$ is an infix relation symbol,
$R$ must have type $\power (t_1 \cross t_2)$, and $E_1$ and $E_2$
must have types $t_1$ and $t_2$ respectively, for some types $t_1$
and $t_2$.

In the predicate $R\;E$, where $R$ is a prefix relation symbol,
$R$ must have type $\power t$, and $E$ must have type $t$,
for some type $t$.

\item[Description]
\new As explained in Section~\ref{ss:infix}, the chain of relationships
\[ E_1\;R_1\;E_2\;R_2\;E_3\;\ldots\;E_{n\minus1}\;R_{n\minus1}\;E_n \]
is equivalent to the conjunction of the individual relationships:
\[ E_1\;R_1\;E_2 \land E_2\;R_2\;E_3 \land \cdots % 
                    \land E_{n\minus1}\;R_{n\minus1}\;E_n. \] 
The equality and membership signs $=$ and $\in$ may also appear in
such a chain: they can be used as if they were built-in relation
symbols.  Also, any ordinary identifier may be used as an infix
relation symbol if it is underlined. 

This rule explains arbitrary chains of relations in terms of simple
relationships $E_1\;R\;E_2$, for which the type rule is given above.
Such a relationship is a shorthand for $(E_1, E_2) \in (\_\;R\;\_)$,
and it is satisfied exactly if whatever set is the value of $R$
contains the ordered pair $(E_1, E_2)$.

The predicate $R\;E$, where $R$ is a prefix relation symbol, is
a shorthand for $E \in (R\;\_)$; it is satisfied exactly if the
value of $E$ is a member of whatever set is the value of $R$.
\end{manpage}
%-----------------------------------------------------
\section{Schema expressions}\label{s:schemaexp}

The following pages describe the syntax of the various kinds of
schema expression\index{schema: expression}:
\begin{menu}
        $\neg, \land, \lor, \implies, \iff$
		& Logical schema operations \prf{slogic} \\
        $\forall, \exists, \exists_1, \hide, \project$
		& Hiding operations \prf{hide} \\
        $\rm pre$
		& Pre-condition \prf{pre} \\
	$\semi$, $\pipe$
		& Composition of operations \prf{opcomp}
\end{menu}
The meanings of these forms of expression are given in
Section~\ref{ss:combschema}.

Although the same symbols are used both as the connectives and
quantifiers of logic in forming predicates (see
Section~\ref{s:pred}) and as operators in forming schema
expressions, the syntax of Z always allows it to be deduced from the
context which operator is meant, since schema expressions appear
only on the right-hand side of the definition sign $\defs~$.

The simplest kinds of schema expression are 
schema references\index{schema reference: as schema expression}
(see Section~\ref{s:schemaref}) and schema texts\index{schema: text}
(see Section~\ref{s:schematext}).
Parentheses\index{parentheses: used for grouping}
may be used for grouping in schema expressions:
\begin{syntax}
        Schema-Exp\index{\(Schema-Exp\)+} %
                & ::= & Schema-Ref \\
                &  |  & [~Schema-Text~] \\
                &  |  & ( Schema-Exp )
\end{syntax}
%-----------------------------------------------------
\begin{manpage}\label{p:slogic}
\item[Name]
\begin{name}
        \lnot    & Schema negation\index{negation: of schemas} \\
        \land    & Schema conjunction\index{conjunction: of schemas} \\
        \lor     & Schema disjunction\index{disjunction: of schemas} \\
        \implies & Schema implication\index{implication: for schemas} \\
        \iff     & Schema equivalence\index{equivalence: of schemas}%
	\symdex{$\lnot$}\symdex{$\land$}\symdex{$\lor$}%
	\symdex{$\implies$}\symdex{$\iff$}
\end{name}

\item[Syntax]
\begin{syntax}
        Schema-Exp\index{\(Schema-Exp\)} %
                & ::= & \lnot Schema-Exp \\
                &  |  & Schema-Exp \land Schema-Exp \\
                &  |  & Schema-Exp \lor Schema-Exp \\
                &  |  & Schema-Exp \implies Schema-Exp \\
                &  |  & Schema-Exp \iff Schema-Exp
\end{syntax}

\item[Description]
These are the logical operations on schemas which were introduced in
Section~\ref{ss:combschema}.  The negation $\lnot S$ of a schema $S$
has the same signature as $S$, but its property is true in just those
bindings where the property of $S$ is not true.

\new For one of the binary operations to be allowed, its two
arguments must have type compatible signatures. The signatures are
joined to form the signature of the result.  The truth of its
property in any binding $z$ is defined in terms of the truth in the
argument schemas of the restrictions of $z$ to their signatures.  For
example, the property of $S \lor T$ is true in a binding $z$ if and
only if either the property of $S$ is true in the restriction of $z$
to the signature of $S$, or the property of $T$ is true in the
restriction of $z$ to the signature of $T$ (or both).  The other
operations follow the rules for propositional connectives (see
page~\pageref{p:propcon}).
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:hide}
\item[Name]
\begin{name}
        \forall  & Universal schema quantifier%
		\index{universal quantifier: for schemas}%
		\symdex{$\forall$} \\
        \exists  & Existential schema quantifier%
		\index{existential quantifier: for schemas}%
		\symdex{$\exists$} \\
        \exists_1  & Unique schema quantifier%
		\index{unique quantifier: for schemas}%
		\symdex{$\exists_1$} \\
        \hide    & Schema hiding%
		\index{schema: hiding ($\hide$)}\symdex{$\hide$} \\
        \project & Schema projection%
		\index{schema: projection ($\project$)}\symdex{$\project$}
\end{name}

\item[Syntax]
\begin{syntax}
        Schema-Exp\index{\(Schema-Exp\)} %
                & ::= & \forall Schema-Text @ Schema-Exp \\
                &  |  & \exists Schema-Text @ Schema-Exp \\
                &  |  & \rm\exists_1\sf Schema-Text @ Schema-Exp \\
                &  |  & Schema-Exp \hide (Ident, \ldots, Ident) \\
                &  |  & Schema-Exp \project Schema-Exp
\end{syntax}

\item[Description]
These operations are grouped together because they all hide some of
the components of their argument schemas.

For the schema expression $\forall D | P @ S$ to be allowed, all
variables introduced by the schema text `$D | P$' must be among the
components of the schema $S$ and have the same types; they are
removed from the signature of the result in the way described in
Section~\ref{ss:combschema}.  Similar rules apply to the other
quantifiers $\exists$ and $\exists_1$.

The hiding operation $S \hide (x_1, \ldots, x_n)$ removes from the
schema $S$ the components $x_1$, \dots,~$x_n$ explicitly listed,
which must exist.  The schema projection operator $S \project T$
hides all the components of $S$ except those that are also
components of $T$. The schemas $S$ and $T$ must be type compatible,
but $T$ may have components that are not shared by $S$; the
signature of the result is the same as the signature of $T$.  Again,
for the definitions of these operations, see
Section~\ref{ss:combschema}.

\item[Laws]
\begin{laws}
	S \hide (x_1, \ldots, x_n)
	\hbox{\quad is equivalent to\quad}
	(\exists x_1: t_1; \ldots; x_n: t_n @ S),
\end{laws}
where $x_1$, \dots,~$x_n$ have types $t_1$, \dots,~$t_n$ in $S$.
\begin{laws}
	S \project T 
	\hbox{\quad is equivalent to\quad}
	(S \land T) \hide (x_1, \ldots, x_n),
\end{laws}
where $x_1$, \dots,~$x_n$ are the components of $S$ not shared by
$T$.
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:pre}
\item[Name]
\begin{name}
        \pre    & Pre-condition%
		\index{pre-condition operator ($\rm pre$)} \\
\end{name}

\item[Syntax]
\begin{syntax}
        Schema-Exp & ::= & \pre Schema-Exp
\end{syntax}

\item[Description]
This operation gives the pre-condition of an operation described by
a schema according to the conventions of Chapter~\ref{c:seqprog}.

If $S$ is a schema, and $x_1'$, \dots,~$x_m'$ are the components of
$S$ that have the decoration~${}'$, and $y_1!$, \dots,~$y_n!$ are
the components that have the decoration~$!$, then the schema
`$\pre S$' is the result of hiding these variables of~$S$:
\[ S \hide (x_1', \ldots, x_m', y_1!, \ldots, y_n!). \]
It contains only the components of $S$ corresponding to the state
before the operation and its input.
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:opcomp}
\item[Name]
\begin{name}
        \semi	& Sequential composition%
		\index{sequential composition ($\semi$)}\symdex{$\semi$} \\
	\pipe	& Piping%
		\index{piping ($\pipe$)}\symdex{$\pipe$}
\end{name}

\item[Syntax]
\begin{syntax}
        Schema-Exp\index{\(Schema-Exp\)-}
                & ::= & Schema-Exp \semi Schema-Exp \\
		&  |  & Schema-Exp \pipe Schema-Exp
\end{syntax}

\item[Description]
These schema operations are useful in describing sequential
systems. They depend on the conventions for decorating inputs and
outputs and the states before and after an operation: for details, see
Chapter~\ref{c:seqprog}.  

\new For the composition $S \semi T$ to be defined, for each word $x$
such that $x'$ is a component of $S$ and $x$ itself is a component
of $T$, the types of these two components must be the same.  We call
$x$ a {\em matching} state variable.  Also, the types of any other
components they share (including inputs, outputs, and state
variables that do not match) must be the same.

\new The schema $S \semi T$ has all the components of $S$ and $T$, except
for the components $x'$ of $S$ and $x$ of $T$, where $x$ is a % FIXED
% [Thanks to Sam Valentine]
matching state variable.  If $State: Exp $ is a schema containing just the
matching state variables, then $S \semi T$ is defined~as
\[
        \exists State'' @ \\
\t1         (\exists State' @
                [~\rlap{$S;$}\phantom{T;} State'' | \theta State'
				= \theta State''~]) \land \\
\t1         (\exists State\phantom{{}'} @
                [~T; State'' | \theta State\phantom{{}'}
				= \theta State''~]).
\]
In this definition, $State''$ is the hidden state in which $S$
terminates and $T$ starts. The definition assumes that the
components of $State''$ do not clash with other components of $S$
and $T$; otherwise, some other decoration than ${}''$ is to be used.

For the piping $S \pipe T$ to be defined, for each word $x$ such
that $S$ has an output $x!$ and $T$ has an input $x?$, the types of
these two components must be the same.  We call $x$ a {\em piped}
variable.  Any other components (including initial and final state
variables) that $S$ and $T$ share must have the same type in both.

The schema $S \pipe T$ has all the components of $S$ and $T$ except
for outputs $x!$ of $S$ and inputs $x?$ of $T$, where $x$ is a piped
variable.  If $Pipe$ is a schema containing just the piped variables,
then $S \pipe T$ is defined to be
\[
        \exists Pipe!? @ \\
\t1         (\exists Pipe\rlap{!}\phantom{?} @
                [~\rlap{$S;$}\phantom{T;} Pipe!? |
			\theta Pipe\rlap{!}\phantom{?}
				= \theta Pipe!?~]) \land \\
\t1         (\exists Pipe? @
                [~T; Pipe!? | \theta Pipe? = \theta Pipe!?~]).
\]
Again, this definition assumes that the
components of $Pipe!?$ do not clash with other components of $S$
and $T$.
\end{manpage}
%-----------------------------------------------------
\section{Generics}\label{s:gendefs}

The generic constructs\index{generic constructs} of Z allow generic
schemas and constants to be defined and applied. The ideas behind
generics are explained in Section~\ref{s:generics}. This section
contains a description of the syntax of the paragraphs which define
generic schemas and constants, and the rules for using them. The
type rules explained in Section~\ref{ss:genconst} are also used to
infer the types of empty set, sequence, and bag displays (see
Section~\ref{s:exp}).

\subsection{Generic schemas}\label{ss:genschema}

Generic schemas\index{generic schema} have definitions similar to
those of ordinary schemas, but with generic parameters:
\begin{syntax}
    Paragraph\index{\(Paragraph\)+} & ::= & %
        \qquad \vcenter{\boxpream
	    \boxtop{$\sf Schema-Name[Ident, \ldots, Ident]$}
            \boxcontents{Declaration}{Predicate; \ldots; Predicate}
	    \boxend\egroup} \\
\noalign{\vskip\normalbaselineskip}
    Paragraph\index{\(Paragraph\)} & ::= &
	Schema-Name[Ident, \ldots, Ident] \defs Schema-Exp
\end{syntax}
In the body or the right-hand side of the definition, the collection
of basic types is locally extended with the formal generic parameters.
As for ordinary schemas, the name must not have appeared before in the
specification, and it becomes a schema name.  Each
use of the name, except in a $\theta$-expression (see
page~\pageref{p:theta}), must be supplied with actual generic
parameters:
\begin{syntax}
Schema-Ref\index{\(Schema-Ref\)} ::= &&\\
\t1	Schema-Name\;Decoration\;
	\lopt [Expression, \ldots, Expression] \ropt\;
	\lopt Renaming \ropt &&
\also
Renaming ::= [Ident/Ident, \ldots, Ident/Ident] &&
\end{syntax} 
The signature of the resulting schema is obtained by applying the
decoration to the variables of the generic schema, performing the
renaming (if any) and substituting\index{substitution of types} the
types of the actual parameters for the formal parameters.  The
property of the result is augmented with the constraint that the
formal parameters take as their values whatever sets are the values
of the actual parameters.  For an explanation and an example of
this, see Section~\ref{s:generics}.

\subsection{Generic constants}\label{ss:genconst}

Generic constants\index{generic constant}
can be defined with a paragraph which looks
like an axiomatic description but has a double bar on top with
formal generic parameters:
\begin{syntax}
    Paragraph\index{\(Paragraph\)} & ::= & %
        \qquad \vcenter{
            \setbox0=\hbox{$\sf \lopt~[Ident, \ldots, Ident]~\ropt$}
            \setbox1=\null \wd1=\wd0
            \boxpream\boxtop{\box0}
            \noalign{\kern-\normalbaselineskip\kern-\doublerulesep}
            \boxtop{\box1} \noalign{\kern\doublerulesep}
	    \boxcontents{Declaration}{Predicate; \ldots; Predicate}
	    \boxend\egroup} \\
\end{syntax}
The formal generic parameters\index{generic parameter}
are local to the definition, and each variable introduced by the
declaration becomes a global generic constant.
These identifiers must not previously have been defined as global
variables or generic constants, and their scope extends from here
to the end of the specification. The predicates
must determine the values of the constants
uniquely\index{generic constant: uniquely defined}
for each value of the formal parameters.

Generic constants may also be introduced by an abbreviation
definition\index{abbreviation definition}\symdex{$==$}
in which the left-hand side has generic parameters:
\begin{syntax}
        Paragraph\index{\(Paragraph\)} & ::= & 
		Def-Lhs == Expression
\also
        Def-Lhs\index{\(Def-Lhs\)} %
                & ::= & Ident\;\lopt [Ident, \ldots, Ident] \ropt \\
                &  |  & Ident\;In-Gen\;Ident \\
                &  |  & Pre-Gen\;Ident
\end{syntax}
This is a generalization of the simple abbreviation facility described
in Section~\ref{ss:abbrev}.
The left-hand side may be a pattern containing
a prefix\index{prefix symbol} or infix\index{infix symbol} generic symbol:
an example is the definition
\[ X \rel Y == \power (X \cross Y) \]
of the relation sign $(\_ \rel \_)$.  The formal generic
parameters\index{generic parameter} are local to right-hand side, and
the left-hand side becomes a global generic constant.  It must not
previously have been defined as a global variable or generic constant,
and its scope extends from here to the end of the specification.

When a generic constant is used, the actual generic parameters
may be supplied explicitly or left implicit.\label{p:genuse}
\begin{syntax}
        Expression\index{\(Expression\)} & ::= & %
                Ident\;\lopt [Expression, \ldots, Expression] \ropt
\end{syntax}
The form $E_1 \op E_2$, where $\op$ is an infix generic symbol, is an
abbreviation for $(\_ \op \_)[E_1, E_2]$, and the form $\op~E$, where
$\op$ is a prefix generic symbol, is an abbreviation for
$(\op~\_)[E]$. If actual parameters\index{generic parameter: implicit}
are left implicit, they are inferred\index{type: inference} from the
typing information in the expression: they are chosen to be whatever
types make the expression obey the type rules.  If there are no such
types, the expression is wrong; likewise if there are several ways of
filling in types as the actual parameters, the expression is wrong,
and more information needs to be made explicit. For example, consider
the generic function $first$ defined by
\begin{gendef}[X,Y]
        first: X \cross Y \fun X
\where
        \forall x: X; y: Y @ first(x,y) = x
\end{gendef}
In the expression $first(3,4)$, the generic constant $first$ has the type
\[ \power ((\alpha \cross \beta) \cross \alpha), \]
where $\alpha$ and $\beta$ are the types filled in for the two generic
parameters $X$ and $Y$. Its argument $(3,4)$ has type $\num \cross
\num$, so $\alpha$ and $\beta$ must both be $\num$, and the type of the
whole expression $first(3,4)$ is $\num$:
\[ first(3,4) = first[\num,\num] (3,4). \]
One kind of error is illustrated by the expression $first~\{3,4\}$.
As before, $first$ has a type matching the pattern
$\power ((\alpha \cross \beta) \cross \alpha)$.  This time, however,
the type of the argument $\{3,4\}$ is $\power \num$, and this does not
match $\alpha \cross \beta$: there is no choice of the generic
parameters which makes the expression obey the type rules, and this is
not allowed.

The expression $first(3,\empty)$ illustrates the other kind of
error.\label{p:undetvar} Here the second component of the argument, 
$\empty$\index{empty set ($\empty$, $\{\}$)}, is
itself generic. It is defined by
\begin{gendef}[X]
        \empty: \power X
\where
        \empty = \{~x: X | false~\}
\end{gendef}
and its type is $\power \gamma$, where $\gamma$ is the type filled in
for the generic parameter $X$. So the argument $(3,\empty)$ has type
$(\num \cross \power \gamma)$, and matching this with $\alpha \cross
\beta$ gives $\alpha = \num$ and $\beta = \power \gamma$. We are
tempted to deduce that $\num$ is the type of the whole expression, but
this is not so, since the type $\gamma$ is undetermined: there is more
than one way of filling in the types. In this example, the
indefiniteness seems rather benign, since the value of the expression
$first(3,\empty)$ does not depend on the type chosen as $\gamma$; but
other cases are not so simple, and this is the reason for the general
rule. The error of leaving types undetermined can usually be avoided
by supplying one or actual parameters explicitly: as in the legal
expression $first(3, \empty[\num])$.

The method used in these examples can be used generally for
inferring\index{type: inference} actual generic parameters which have
been left implicit: the unknown types are represented by
place-markers\index{place-markers for types} like those written with
Greek letters above. When two types are required to be the same by
the type rules, the two types (possibly containing place-markers)
are matched with each other by unification\index{unification},
and an expression is correctly typed exactly if the type rules give
enough information to eliminate all the unknowns. The same method
allows type-checking of the empty set\index{empty set ($\empty$,
$\{\}$)} $\{\}$, the empty sequence\index{sequence: empty ($<>$)}
$<>$, and the empty bag\index{bag: empty ($\lbag \rbag$)} $\lbag
\rbag$: these can have any types $\power \alpha$, $\power (\num
\cross \beta)$, and $\power (\gamma \cross \num)$ respectively,
where $\alpha$, $\beta$, and $\gamma$ are unknowns.

\section{Free types}\label{s:freetype}

The notation for free type\index{free type: definition
($::=$)}\symdex{$::=$}\symdex{$\ldata\ldots\rdata$}
definitions adds nothing to the power of the Z language, but it makes
it easier to describe recursive structures\index{recursive structure}
such as lists and trees. The syntax of a free
type definition is as follows:
\begin{syntax}
        Paragraph\index{\(Paragraph\)-} & ::= &
		Ident ::= Branch | \ldots | Branch\label{p:cocoeq}
\also
        Branch\index{\(Branch\)} & ::= &
		Ident \;\lopt \ldata Expression \rdata \ropt
\end{syntax}
(In the first of these syntax rules, the second occurrence of the
symbol `$::=$' stands for exactly that symbol.)  The meaning of this
construct is given here by showing how to translate free type
definitions into the other Z constructs. In the translation, we shall
use for convenience some of the notation introduced in
Chapter~\ref{c:library} on `the mathematical tool-kit'.
A free type definition
\[ T ::= c_1 | \ldots | c_m | d_1 \ldata E_1[T] \rdata %
                        | \ldots | d_n \ldata E_n[T] \rdata \]
introduces a new basic type $T$, and $m+n$ new variables
$c_1$, \dots,~$c_m$ and $d_1$, \dots,~$d_n$, declared as if by
\[ [T] \]
\begin{axdef}
        c_1, \ldots, c_m: T \\
        d_1: E_1[T] \inj T \\
        \vdots \\
        d_n: E_n[T] \inj T \\
\where
        \hbox{\dots\ see below \dots}
\end{axdef}
The $c_i$'s are constants of type $T$, and the $d_j$'s, called the
{\em constructors}\index{constructor}, are injections from the sets
$E_j[T]$ to $T$. What makes things interesting is that the expressions
$E_j[T]$ may contain occurrences of $T$\index{definition before use};
I have used the notation $E_j[T]$ to make explicit the possibility
that these expressions depend on $T$.

\new A free type definition may appear to be circular, since
the name $T$ being defined appears on the right as well as on the
left. But this translation removes the circularity, introducing $T$
as a basic type {\em before\/} the $d_i$'s are declared.

There are two axioms constraining the constants and constructors.
First, all the constants are distinct, and the constructors have
disjoint ranges which do not contain any of the constants:
\[ \disjoint <\{c_1\}, \ldots, \{c_m\}, \ran d_1, \ldots, \ran d_n>. \]
\new Second, the smallest subset of $T$ which contains all the
constants and is {\em closed under the constructors\/} is $T$ itself.
Informally, a set is closed under the constructors if performing any
of them on elements of the set can only yield another element of the
set.  In the following axiom, $E_j[W]$ is the expression which
results from replacing all free occurrences of $T$ in $E_j[T]$ by
$W$, a name appearing nowhere else in the specification. The axiom
is an induction principle for the free type:
\[
        \forall W: \power T @ \\
\t1         \{c_1, \ldots, c_m\} \cup d_1\limg E_1[W]\rimg %
		\cup \cdots \cup d_n\limg E_n[W]\rimg \subseteq W \\
\t1	    \implies T \subseteq W.
\]
% Thanks to Alf Smith for the following:
If the constructions $E_j$ used in the definition are finitary
(see Section~\ref{ss:consistency}), then this induction principle
implies
% One consequence of this induction principle is 
that the constants and
constructors together exhaust the whole of $T$, so that
\[ <\{c_1\}, \ldots, \{c_m\}, \ran d_1, 
	\ldots, \ran d_n> \partition T. \]
If $n = 0$, so we are defining an enumerated type with no
constructors but only constants, then this property is actually
equivalent to the induction principle.  For general free types, the
induction principle justifies the method of proof by induction
described below.

\subsection{Example: binary trees}

The details of this axiomatization of free types may be a little
difficult to understand all at once, but perhaps a small example
will help to make things clear.  We can describe the set of binary
trees labelled with natural numbers by saying that the constant
$tip$ is a tree (the empty one), and that if $n$ is a number and
$t_1$ and $t_2$ are trees, then $fork(n,t_1,t_2)$ is a tree:
\[ TREE ::= tip | fork \ldata \nat \cross TREE \cross TREE \rdata. \]
This free type definition is equivalent to the following 
axiomatic description:
\[ [TREE] \]
\begin{axdef}
	tip: TREE \\
	fork: \nat \cross TREE \cross TREE \inj TREE
\where
        \disjoint <\{tip\}, \ran fork>
\also
        \forall W: \power TREE @ \\
\t1         \{tip\} \cup fork \limg \nat \cross W \cross W \rimg \subseteq W \\
\t1	    \implies TREE \subseteq W
\end{axdef}
The constructor $fork$ is declared as an injection, so
putting together different trees, or the same trees with a different
label, gives different results.  The range of $fork$ is disjoint
from the set $\{tip\}$, that is
\[ tip \notin \ran fork, \]
so $tip$ cannot result from putting two trees together with $fork$.

The induction principle justifies proofs by structural
induction\index{induction: proof by} on trees, which are analogous to
the proofs by induction on natural numbers, finite sets, and sequences
described in Chapter~\ref{c:library}. Suppose we want to prove that a
predicate $P(t)$ is true of all trees $t$.  The induction principle
says that it is enough to prove the following two facts:
\begin{oblig}{a}
\item $P(tip)$ holds.

\item If $P(t_1)$ and $P(t_2)$ hold, so does $P(fork(n,t_1,t_2))$:
        \[ \forall n: \nat; t_1, t_2: TREE @ \\
	\t1	P(t_1) \land P(t_2) \implies P(fork(n,t_1,t_2)). \]
\end{oblig}
If these facts hold, the induction principle lets us derive
$\forall t : TREE @ P(t)$.  Let $W$ be the set of trees satisfying
$P$: that is,
\[ W = \{~t: TREE | P(t)~\}. \]
Fact (a1) says that $tip \in W$, and fact (a2) says that
$fork\limg \nat \cross W \cross W\rimg \subseteq W$, so
by the induction principle, $TREE \subseteq W$.
This means that $\forall t: TREE @ t \in W$, or equivalently,
that $\forall t: TREE @ P(t)$.

\subsection{Consistency}\label{ss:consistency}

There is a snag with the notation for defining free types\index{free
type: consistency+}\index{consistency: of free types+}, and that is
the possibility that the definition will be inconsistent: that there
will be no sets and functions which satisfy the axioms given above.
The classic example of an inconsistent free type definition is that
of a type containing both natural numbers as atoms, and all the
functions from the type to itself:
\[ T ::= atom \ldata \nat \rdata | fun \ldata T \fun T \rdata. \]
Briefly, no such set $T$ can exist, because however large $T$
is, there are many more functions from $T$ to $T$ than there are
members of $T$.

A sufficient condition for a free type definition 
\[ T ::= c_1 | \ldots | c_m | d_1 \ldata E_1[T] \rdata %
                                | \ldots | d_n \ldata E_n[T] \rdata \]
to be consistent is that all the constructions $E_1[T]$,
\dots,~$E_n[T]$ which appear on the right-hand side are {\em
finitary}\index{finitary construction},%
	\glossary{[finitary construction] A construction $E[T]$ such
	that any element of $E[T]$ is also an element of $E[V]$ for
	some finite $V \subseteq T$.  Finitary constructions may
	be used on the right-hand side of a free type
	definition without danger of inconsistency. Many
	constructions which involve only finite	objects are
	finitary.}
in a sense explained below.
Examples of finitary constructions include Cartesian products $X
\cross Y$, finite sets $\finset X$, finite functions $X \ffun Y$, and
finite sequences $\seq X$, as well as set constants not containing the
type $T$ being defined.  Any composition of finitary constructions is
also finitary. Any construction on $T$ which involves objects
containing infinitely many elements of $T$ will not be finitary -- for
example, the power-set construction $\power T$ and infinite sequences
$\nat \fun T$ are not finitary.

\new The examples just given provide enough finitary constructions
for most practical purposes; but for completeness, here is a precise
definition of the concept.
We say that $E[T]$ is finitary if any element of $E[T]$ is
also an element of $E[V]$ for some {\em finite\/} subset $V$ of $T$:
\[ E[T] = \bigcup \{~V: \finset T @ E[V]~\}. \]
If each element $x$ of $E[T]$ involves only finitely many members of
$T$, then these members of $T$ form a finite subset $V$ of $T$, and
$x \in E[V]$, so $E[T]$ is finitary in this formal sense.%
\index{free type: consistency-}\index{consistency: of free types-}

\new If a construction is finitary, then it is {\em monotonic}, in
the sense that $S \subseteq T$ implies $E[S] \subseteq E[T]$.  It is
also {\em continuous}, in the sense that it preserves the limits of
ascending chains of sets:  if $S_0 \subseteq S_1 \subseteq \ldots~$,
then (thanks to monotonicity)
\[ E[S_0] \subseteq E[S_1] \subseteq \ldots~, \]
and also
\[ E[\bigcup \{~i: \nat @ S_i~\}]
	= \bigcup \{~i: \nat @ E[S_i]~\}. \]
Standard mathematical techniques can be used to show that this
continuity property of finitary constructions guarantees the
consistency of free type definitions that use them.

% \note Should I include the following proofs? \dots

% \new Any finitary construction is monotonic, because if
% $S \subseteq T$, then
% \begin{argue}
% 	E[S] = \bigcup \{~V: \finset S @ E[V]~\} \\
% \t1		\subseteq \bigcup \{~V: \finset T @ E[V]~\} \\
% \t1		= E[T].
% \end{argue}
% Also, if $S_0 \subseteq S_1 \subseteq \ldots$ then for each $i$,
% \[ S_i \subseteq \bigcup \{~i: \nat @ S_i~\}, \]
% so by monotonicity,
% \[ E[S_i] \subseteq E[\bigcup \{~i: \nat @ S_i~\}], \]
% and so
% \[ \bigcup \{~i: \nat @ E[S_i]~\}
% 	\subseteq E[\bigcup \{~i: \nat @ S_i~\}]. \]
% Conversely, if $x \in E[\bigcup \{~i: \nat @ S_i~\}]$ then $x \in
% E[V]$ for some finite subset $V$ of $\bigcup \{~i: \nat @ S_i~\}$,
% and because the chain of $S_i$'s is increasing, $V \subseteq S_N$
% for some $N$.  So
% \[ x \in E[V] \subseteq E[S_N]
% 	\subseteq \bigcup \{~i: \nat @ E[S_i]~\}, \]
% and it follows that
% \[ E[\bigcup \{~i: \nat @ S_i~\}]
% 	\subseteq \bigcup \{~i: \nat @ E[S_i]~\}. \]
% This completes the proof that finitary constructions are continuous.

% \new Now consider a free type definition,
% \[ T ::= c_1 | \ldots | c_m | d_1 \ldata E_1[T] \rdata
% 				| \ldots | d_n \ldata E_n[T] \rdata, \]
% in which all the constructions $E_i[T]$ are finitary.  Using untyped
% set theory, we can construct a model for the definition as follows.
% First, the free type definition can be simplified a little, by
% replacing the constants $c_k$ with constructors $c_k'$ acting on the
% one-point set $\{ 0 \}$: instead of $c_1 \mid \ldots \mid c_m$ we
% write $c_1' \ldata\{0\}\rdata \ldots \mid c_m'\ldata\{0\}\rdata$.
% This change means that we shall not have to consider constants in
% what follows; when we have built a model for the simplified
% definition, we can define $c_k$ to be $c_k'(0)$ and derive a model
% for the original one.

% \new We define an ascending chain of sets $S_0 \subseteq S_1 \subseteq
% \ldots$ by taking $S_0 = \empty$ and $S_{i+1} = F[S_i]$ for each
% $i$, where
% \[ F[X] = \{~x_1: E_1[X] \spot (1, x_1)~\}
% 			\cup \{~x_2: E_2[X] \spot (2, x_2)~\} \\
% \t3		{} \cup \ldots \cup \{~x_n: E_n[X] \spot (n, x_n)~\}. \]
% At stage $i+1$, each construction $E_k$ is performed on $S_i$, the
% results are labelled by pairing them with the number $k$ to ensure
% disjointness, and all these pairs are gathered together into $S_{i+1}$.
% Because the finitary constructions $E_k[X]$ are monotonic, so is
% $F[X]$, and $S_0$, $S_1$,~\dots really do form an ascending chain.

% \new Now take $T = \bigcup \{~i: \nat @ S_i~\}$, and for each $k$ define
% $d_k: E_k[T] \inj T$ by $d_k(x) = (k, x)$. It's obvious that $d_k$
% is an injection, and obvious too that the ranges of the $d_k$'s are
% disjoint, but if $x \in E_k[T]$, is it always true 
% that $d_k(x) \in T$? It is, for $E_k$ is continuous, so
% \[ x \in E_k[T] = \bigcup \{~i: \nat \spot E_k[S_i]~\}, \]
% and so $x \in E_k[S_i]$ for some $i$. Hence
% \[ d_k(x) = (k, x) \in F[S_i] = S_{i+1} \subseteq T. \]

% \new So now we have our set $T$ and injections $d_k: E_k[T] \inj T$.
% It remains to check that the induction principle holds: that if $W:
% \power T$ is such that
% \[ d_1\limg E_1[W]\rimg \cup d_2\limg E_2[W]\rimg \cup
% 		\cdots \cup d_n\limg E_n[W]\rimg \subseteq W, \]
% then $T \subseteq W$. The left-hand side of this inequality is just
% $F[W]$. We shall show by induction on $i$ that $S_i \subseteq W$; it
% follows that $T = \bigcup \{~i: \nat @ S_i~\} \subseteq W$.  For the
% base case, $S_0 = \empty \subseteq W$. For the step
% case, suppose $S_i \subseteq W$. Then
% \[ S_{i+1} = F[S_i] \subseteq F[W], \]
% since $F$ is monotonic; and because $F[W] \subseteq W$, it follows
% that $S_{i+1} \subseteq W$. This completes the proof.

% \new We had to step outside the Z world to perform this construction of a
% model for the free type definition, but the nice thing is that the
% conditions on which it depends, that the constructions $E_k$ are
% finitary, can be stated and proved inside the world of Z.%

\chapter{The Mathematical Tool-kit}\label{c:library}

\def\blank{\quad$\diamond$}

% Notational conventions:
%	S, T, U, V, W	Sets
%	A, B		Sets of sets
%	P, Q, R		Relations
%	f, g, h		Functions

An important part of the Z method is a standard library or tool-kit
of mathematical definitions.  This
tool-kit allows many structures in information systems to be
described very compactly, and because the data types it contains are
oriented towards mathematical simplicity rather than computer
implementation, reasoning about properties of the systems is made
easier.  This chapter consists almost entirely of independent manual
pages, each introducing an operation or group of related operations
from the tool-kit. Each page includes laws which relate its
operations to each other and to the operations defined on preceding
pages.  These laws are stated without explicitly declaring the
variables they contain; their types should be clear from the
context.  A number of pages consist entirely of laws of a certain
kind: for example, the induction principles for natural numbers and
for sequences are summarized on their own pages.

The tool-kit begins\label{p:symlist+} with the basic operations of set
algebra (Section~\ref{s:setlib}). Many of these operations have a
strong connection with the subset ordering $\subseteq$, and the laws
relating them are listed on a separate page.
\begin{menu}
	$\neq$, $\notin$
		& Inequality, non-membership		\prf{10} \\
	$\empty$, $\subseteq$, $\subset$, $\powerone$
		& Empty set, subsets, non-empty sets 	\prf{20} \\
	$\cup$, $\cap$, $\setminus$
		& Set algebra 				\prf{40} \\
	$\bigcup$, $\bigcap$
		& Generalized union and	intersection 	\prf{60} \\
	$first$, $second$
		& Projection functions			\prf{70} \\
	\blank
		& Order properties of set operations 	\prf{80}
\end{menu}
Next, the idea of a relation as a set of ordered pairs is introduced,
together with various operations on relations (Section~\ref{s:rellib}).
Again, the subset ordering plays a special part, in that many of the
operations are monotonic with respect to it: these laws are shown on
their own page.
\begin{menu}
	$\rel$, $\mapsto$ 	
		& Binary relations, maplet 		\prf{1010} \\
	$\dom$, $\ran$		
		& Domain and range of a relation 	\prf{1020} \\
        $\id$, $\comp$, $\circ$	
		& Identity relation, composition 	\prf{1030} \\
        $\dres$, $\rres$
		& Domain and range restriction 		\prf{1040} \\
        $\ndres$, $\nrres$
		& Domain and range anti-restriction 	\prf{1050} \\
	$\_^\sim$
		& Relational inversion 			\prf{1060} \\
        $\_\limg\_\rimg$
		& Relational image 			\prf{1070} \\
	$\oplus$
		& Overriding		 		\prf{1075} \\
        $\_^+$, $\_ \star$
		& Transitive closure 			\prf{1080} \\
	\blank
		& Monotonic operations			\prf{1090}
\end{menu}
In Section~\ref{s:funlib}, functions are introduced as a special kind
of relation; and injections, surjections and bijections are introduced as
special kinds of function.  Because functions are really relations,
the operations on relations may be used on functions too.  Extra laws
about this usage are listed on a separate page.
\begin{menu}
	$\pfun$, $\fun$, $\pinj$, $\inj$,
		& Partial and total functions, injections, \\
	\quad $\psurj$, $\surj$, $\bij$
		& \quad surjections, bijections \prf{2010} \\
	\blank
		& Relational operations on functions 	\prf{2050}
\end{menu}
Natural numbers are introduced in Section~\ref{s:numlib}, together
with the ideas of iteration of a relation and of finite sets and
functions.  Induction is an important proof method for natural
numbers, and it is given its own page.
\begin{menu}
	$\nat$, $\num$, $+$, $\minus$, $*$, $\div$, 
		& Natural numbers, integers 		\prf{3010} \\
	\quad $\mod$, $\lt$, $\leq$, $\geq$, $\gt$\\
        $\nat_1$, $succ$, $\upto$
		& Strictly positive integers, successor \\
		& \quad function, number range 		\prf{3020} \\
	$R^k$, $iter$		
		& Iteration 				\prf{3030} \\
	$\finset$, $\finsetone$, $\#$
		& Finite sets, number of members 	\prf{3050} \\
	$\ffun$, $\finj$
		& Finite partial functions and injections \prf{3060} \\
        $min$, $max$
		& Minimum and maximum numbers 		\prf{3070} \\
	\blank
		& Proof by induction 			\prf{3080}
\end{menu}
Next, sequences are introduced as functions whose domains are certain
segments of the natural numbers (Section~\ref{s:seqlib}). There are
several important operations on sequences, and they inherit the
operations on relations; some extra laws about these are listed on a
separate page. There are specialized induction principles
for sequences, and these too have their own page.
\begin{menu}
        $\seq$, $\seq_1$, $\iseq$ 
		& Finite and injective sequences 	\prf{4010} \\
        $\cat$, $rev$
		& Concatenation, reverse		\prf{4020} \\
        $head$, $last$, $tail$, $front$
		& Sequence decomposition 		\prf{4040} \\
        $\extract$, $\filter$, $squash$
		& Extraction, filtering, compaction 	\prf{4060} \\
	$\prefix$, $\suffix$, $\inseq$
		& Subsequences				\prf{4065} \\
	\blank
		& Relational operations on sequences 	\prf{4090} \\
        $\cat/$
		& Distributed concatenation 		\prf{4070} \\
        $\disjoint$, $\partition$
		& Disjointness, partitions 		\prf{4080} \\
	\blank
		& Induction for sequences 		\prf{4100}
\end{menu}
Bags are like sets, except that it matters how many times a bag
contains each of its elements.  Bags and operations
on bags are defined in Section~\ref{s:baglib}.
\begin{menu}
        $\bag$, $count$, $\bcount$, $\otimes$
		& Bags, counting, scaling	 	\prf{5010} \\
	$\inbag$, $\subbageq$
		& Bag membership, sub-bags		\prf{5015} \\
        $\uplus$, $\uminus$
		& Bag union and difference	 	\prf{5020} \\
        $items$
		& Bag of elements of a sequence 	\prf{5030}%
							\label{p:symlist-}
\end{menu}

The `definition' parts of this chapter are a formal
specification of the tool-kit. The principle of definition
before use\index{definition before use} has been observed in all but
two cases, the symbols $\rel$ and $\fun$.
For completeness, their definitions are given here:\label{p:forward}
\[
        X \rel Y  ==  \power (X \cross Y)
\also
        X \fun Y  ==  \{~f: X \rel Y | %
                \forall x: X @ \exists_1 y: Y @ (x, y) \in f~\}.
\]
%-----------------------------------------------------
\newpage
\section{Sets}\label{s:setlib}
\begin{manpage}\label{p:10}
\item[Name]
\begin{name}
        \neq   & Inequality\index{inequality ($\neq$)}\symdex{$\neq$} \\
        \notin & Non-membership%
			\index{non-membership ($\notin$)}\symdex{$\notin$}
\end{name}

\item[Definition]
\begin{gendef}[X]
        \_ \neq \_: X \rel X \\
        \_ \notin \_: X \rel \power X
\where
        \forall x, y: X @ x \neq y \iff \lnot (x = y)
\also
        \forall x: X; S: \power X @ x \notin S \iff \lnot (x \in S)
\end{gendef}

\item[Description]
The relations $\neq$ and $\notin$ are the complements of the 
equality\index{equality ($=$)} and membership\index{membership ($\in$)}
relations expressed by $=$ and $\in$ respectively.

\item[Laws]
\begin{laws}
        x \neq y \implies y \neq x
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:20}
\item[Name]
\begin{name}
        \empty & Empty set\index{empty set ($\empty$, $\{\}$)}%
			\symdex{$\empty$} \\
        \subseteq & Subset relation%
			\index{subset ($\subseteq$)}\symdex{$\subseteq$} \\
        \subset   & Proper subset relation%
			\index{subset: proper ($\subset$)}\symdex{$\subset$} \\
        \powerone & Non-empty subsets\index{subset: non-empty ($\powerone$)}%
			\symdex{$\powerone$}
\end{name}

\item[Definition]
\[ \empty[X] == \{~x: X | false~\} \]
\begin{gendef}[X]
	\_ \subseteq \_~, \_ \subset \_: \power X \rel \power X
\where
        \forall S, T: \power X @ \\
\t1         (S \subseteq T \iff %
                (\forall x: X @ x \in S \implies x \in T)) \land \\
\t1         (S \subset T \iff S \subseteq T \land S \neq T)
\end{gendef}
\[ \powerone X == \{~S: \power X | S \neq \empty~\} \]

\item[Description]
$\empty$ is the empty set. It has no members.

A set $S$ is a {\em subset\/} of a set $T$ ($S \subseteq T$) if every
member of $S$ is also a member of $T$. We say $S$ is a {\em proper
subset\/} of $T$ ($S \subset T$) if in addition $S$ is different from
$T$.

For any set $X$, $\powerone X$ is the set of all subsets of $X$
which are not empty.

\item[Laws]
\begin{laws}
        x \notin \empty \-
\also
        S \subseteq T \iff S \in \power T \-
\also
        S \subseteq S &
	        \lnot (S \subset S) \\
        S \subseteq T \land T \subseteq S \iff S = T &
	        \lnot (S \subset T \land T \subset S) \\
	S \subseteq T \land T \subseteq V \implies S \subseteq V &
                S \subset T \land T \subset V \implies S \subset V
\also
	\empty \subseteq S &
		\empty \subset S \iff S \neq \empty
\also
        \powerone X = \empty \iff X = \empty \-
	X \neq \empty \iff X \in \powerone X \-
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:40}
\item[Name]
\begin{name}
        \cup      & Set union\index{union ($\cup$)}\symdex{$\cup$} \\
        \cap      & Set intersection%
			\index{intersection ($\cap$)}\symdex{$\cap$} \\
        \setminus & Set difference\index{difference ($\setminus$)}%
                \index{set: subtraction ($\setminus$)}\symdex{$\setminus$}
\end{name}

\item[Definition]
\begin{gendef}[X]
        \_  \cup \_~, \_ \cap \_~, \_ \setminus \_:
                \power X \cross \power X \fun \power X
\where
        \forall S, T: \power X @ \\
\t1         S \cup T = \{~x: X | x \in S \lor x \in T~\} \land \\
\t1         S \cap T = \{~x: X | x \in S \land x \in T~\} \land \\
\t1         S \setminus T = \{~x: X | x \in S \land x \notin T~\}
\end{gendef}

\item[Description]
These are the ordinary operations of set algebra.
The members of the set $S \cup T$ are those objects which are members of
$S$ or $T$ or both. The members of $S \cap T$ are those objects
which are members of both $S$ and $T$. The members of
$S \setminus T$ are those objects which are members of $S$
but not of $T$.

\item[Laws]
\let\less=\setminus
\begin{laws}
        S \cup S = S \cup \empty = S \cap S = S \less \empty = S \-
        S \cap \empty = S \less S = \empty \less S = \empty \-
\also
        S \cup T = T \cup S
	  & S \cap T = T \cap S \\
        S \cup (T \cup V) = (S \cup T) \cup V
	  & S \cap (T \cap V) = (S \cap T) \cap V
\also
        S \cup (T \cap V) = (S \cup T) \cap (S \cup V) 
	  & S \cap (T \cup V) = (S \cap T) \cup (S \cap V)
\also
        (S \cap T) \cup (S \less T) = S
	  & S \cup (T \less V) = (S \cup T) \less (V \less S) \\

        (S \less T) \cap T = \empty
          & S \cap (T \less V) = (S \cap T) \less V \\

        S \less (T \less V) = (S \less T) \cup (S \cap V)
          & (S \cup T) \less V = (S \less V) \cup (T \less V) \\

        (S \less T) \less V = S \less (T \cup V)
          & S \less (T \cap V) = (S \less T) \cup (S \less V)
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:60}
\item[Name]
\begin{name}
        \bigcup & Generalized union%
			\index{generalized union ($\bigcup$)}\symdex{$\bigcup$}
\also
        \bigcap & Generalized intersection%
                \index{generalized intersection ($\bigcap$)}\symdex{$\bigcap$}
\end{name}

\item[Definition]
\begin{gendef}[X]
        \bigcup, \bigcap: \power (\power X) \fun \power X
\where
        \forall A: \power (\power X) @
\also
\t1         \bigcup A = \{~x: X | (\exists S: A @ x \in S)~\} \land
\also
\t1         \bigcap A = \{~x: X | (\forall S: A @ x \in S)~\}
\end{gendef}

\item[Description]
If $A$ is a set of sets, $\bigcup A$ is its {\em generalized union}:
it contains all objects which are members of some member of $A$.
The set $\bigcap A$ is the {\em generalized intersection\/} of $A$:
it contains those objects which are members of all members of $A$.

\item[Laws]
\begin{laws}
        \bigcup (A \cup B) = (\bigcup A) \cup (\bigcup B) \\
        \bigcap (A \cup B) = (\bigcap A) \cap (\bigcap B)
\also
        \bigcup[X]~\empty = \empty \\
        \bigcap[X]~\empty = X
\also
        S \cap (\bigcup A) = \bigcup \{~T: A @ S \cap T~\} \\
        S \cup (\bigcap A) = \bigcap \{~T: A @ S \cup T~\} \\
        (\bigcup A) \setminus S = \bigcup \{~T: A @ T \setminus S~\} \\
        S \setminus (\bigcap A) = \bigcup \{~T: A @ S \setminus T~\} \\
        A \neq \empty \implies S \setminus (\bigcup A) = %
                \bigcap \{~T: A @ S \setminus T~\} \\
        A \neq \empty \implies (\bigcap A) \setminus S = %
                \bigcap \{~T: A @ T \setminus S~\}
\also
       A \subseteq B \implies \bigcup A \subseteq \bigcup B \\
       A \subseteq B \implies \bigcap B \subseteq \bigcap A
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:70}
\item[Name]
\begin{name}
     first\index{$first$}, second\index{$second$} %
        & Projection functions\index{projection function} for ordered pairs
\end{name}

\item[Definition]

\begin{gendef}[X,Y]
        first: X \cross Y \fun X \\
        second: X \cross Y \fun Y
\where
        \forall x: X; y: Y @ \\
\t1         first(x,y) = x \land \\
\t1         second(x,y) = y
\end{gendef}

\item[Description]
These projection functions split ordered pairs into their first and second
co-ordinates.

\item[Laws]
\begin{laws}
        (first~p, second~p) = p
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{pagestyle}{manpage}
\paragraph{Order properties of set operations}\label{p:80}

The subset relation $\subseteq$ on sets is a partial
order\index{partial order}: this is the content of three of the laws
shown on its page in the manual.  The operations of union and
intersection are least upper bound\index{least upper bound} and
greatest lower bound\index{greatest lower bound} operations for this
partial order, as is expressed in the laws which follow.  If $S$ and
$T$ are sets, then $S \cup T$ is the smallest set which contains both
$S$ and $T$ as subsets:
\begin{laws}
       S \subseteq S \cup T \\
       T \subseteq S \cup T \\
       S \subseteq W \land T \subseteq W
		\implies S \cup T \subseteq W.
\end{laws}
For a set of sets $A$, the generalized union $\bigcup A$ is the smallest
set which contains each member of $A$ as a subset:
\begin{laws}
       \forall S: A @ S \subseteq \bigcup A \\
       (\forall S: A @ S \subseteq W) 
		\implies \bigcup A \subseteq W.
\end{laws}
Similarly, $S \cap T$ is the largest set which is a subset of
both $S$ and $T$:
\begin{laws}
       S \cap T \subseteq S \\
       S \cap T \subseteq T \\
       W \subseteq S \land W \subseteq T 
		\implies W \subseteq S \cap T.
\end{laws}
The set $\bigcap A$ is the largest set which is a subset of each
member of $A$:
\begin{laws}
       \forall S: A @ \bigcap A \subseteq S \\
       (\forall S: A @ W \subseteq S) \implies W \subseteq \bigcap A.
\end{laws}
Finally, $S \setminus T$ is the largest subset of $S$ which is
disjoint from $T$:
\begin{laws}
       S \setminus T \subseteq S \\
       (S \setminus T) \cap T = \empty \\
       W \subseteq S \land W \cap T = \empty \implies %
                W \subseteq S \setminus T.
\end{laws}
\newpage
\end{pagestyle}
%-----------------------------------------------------
\section{Relations}\label{s:rellib}
\begin{manpage}\label{p:1010}
\item[Name]
\begin{name}
       \rel & Binary relations\index{relation ($\rel$)}\symdex{$\rel$} \\
       \mapsto & Maplet\index{maplet ($\mapsto$)}\symdex{$\mapsto$}
\end{name}

\item[Definition]
\[ X \rel Y  ==  \power (X \cross Y) \]
\begin{gendef}[X,Y]
       \_ \mapsto \_: X \cross Y \fun X \cross Y
\where
       \forall x: X; y: Y @ \\
\t1           x \mapsto y = (x, y)
\end{gendef}

\item[Description]
If $X$ and $Y$ are sets, then $X \rel Y$ is the set of {\em binary
relations\/} between $X$ and $Y$. Each such relation is a subset
of $X \cross Y$. The `maplet' notation $x \mapsto y$ is a graphic way
of expressing the ordered pair $(x, y)$.

The definition of $X \rel Y$ given here repeats the one given on
page~\pageref{p:forward}.
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:1020}
\item[Name]
\begin{name}
       \dom, \ran & Domain\index{domain ($\dom$)} and %
                range\index{range ($\ran$)} of a relation
\end{name}

\item[Definition]
\begin{gendef}[X,Y]
        \dom: (X \rel Y) \fun \power X \\
        \ran: (X \rel Y) \fun \power Y
\where
        \forall R: X \rel Y @ \\
\t1         \dom R = \{~x:X; y:Y | \reln xRy @ x~\} \land \\
\t1         \ran R = \{~x:X; y:Y | \reln xRy @ y~\}
\end{gendef}

\item[Description]
If $R$ is a binary relation between $X$ and $Y$, then the
{\em domain\/} of $R$ ($\dom R$) is the set of all members of $X$ which
are related to at least one member of $Y$ by $R$. The {\em range\/}
of $R$ ($\ran R$) is the set of all members of $Y$ to which at
least one member of $X$ is related by $R$.

\item[Laws]
\begin{laws}
     x \in \dom R \iff (\exists y: Y @ \reln xRy) \\
     y \in \ran R \iff (\exists x: X @ \reln xRy)
\also
     \dom~\{x_1 \mapsto y_1,\ldots,x_n \mapsto y_n\} = \{x_1,\ldots,x_n\} \\
     \ran~\{x_1 \mapsto y_1,\ldots,x_n \mapsto y_n\} = \{y_1,\ldots,y_n\}
\also
     \dom (Q \cup R) = (\dom Q) \cup (\dom R) \\
     \ran (Q \cup R) = (\ran Q) \cup (\ran R) \\
     \dom (Q \cap R) \subseteq (\dom Q) \cap (\dom R) \\
     \ran (Q \cap R) \subseteq (\ran Q) \cap (\ran R)
\also
     \dom \empty = \empty \\
     \ran \empty = \empty
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:1030}
\item[Name]
\begin{name}
        \id & Identity relation\index{identity relation ($\id$)} \\
        \comp   & Relational composition%
		\index{composition ($\comp$)}\symdex{$\semi$} \\
	\circ	& Backward relational composition%
		\index{backward composition ($\circ$)}\symdex{$\circ$}%
		\index{composition>backward composition}
\end{name}

\item[Definition]
\[ \id X  ==  \{~x: X @ x \mapsto x~\} \]
\begin{gendef}[X,Y,Z]
        \_ \comp \_: (X \rel Y) \cross (Y \rel Z) \fun (X \rel Z) \\
	\_ \circ \_: (Y \rel Z) \cross (X \rel Y) \fun (X \rel Z)
\where
        \forall Q: X \rel Y; R: Y \rel Z @ \\
\t1         Q \comp R = R \circ Q = \{~x: X; y: Y; z: Z | \\
\t5            		\reln xQy \land \reln yRz @ x \mapsto z~\}
\end{gendef}

\item[Description]

The {\em identity relation\/} $\id X$ on a set $X$ relates each member of
$X$ to itself. The {\em composition\/} $Q \comp R$ of two relations $Q: X
\rel Y$ and $R: Y \rel Z$ relates a member $x$ of $X$ to a member $z$
of $Z$ if and only if there is at least one element $y$ of $Y$ to which
$x$ is related by $Q$ and which is itself related to $z$ by $R$. The
notation $R \circ Q$ is an alternative to $Q \comp R$.

\item[Laws]
% If $P: X \rel Y$; $Q: Y \rel Z$; $R: Z \rel W$,
\begin{laws}
        (x \mapsto x') \in \id X \iff x = x' \in X \\
        (x \mapsto z) \in P \comp Q \iff
                (\exists y: Y @ \reln xPy \land	\reln yQz) \\
\also
        P \comp (Q \comp R) = (P \comp Q) \comp R \\
        \id X \comp P = P \\
        P \comp \id Y = P
\also
        \id V \comp \id W = \id (V \cap W)
\also
	(f \circ g)(x) = f(g(x))
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:1040}
\item[Name]
\begin{name}
       \dres & Domain restriction\index{restriction}%
		\index{domain: restriction ($\dres$)}\symdex{$\dres$} \\
       \rres & Range restriction%
		\index{range: restriction ($\rres$)}\symdex{$\rres$}
\end{name}

\item[Definition]
\begin{gendef}[X,Y]
        \_ \dres \_: \power X \cross (X \rel Y) \fun (X \rel Y) \\
        \_ \rres \_: (X \rel Y) \cross \power Y \fun (X \rel Y)
\where
        \forall S: \power X; R: X \rel Y @ \\
\t1         S \dres R = \{~x: X; y: Y |
		x \in S \land \reln xRy @ x \mapsto y~\}
\also
        \forall R: X \rel Y; T: \power Y @ \\
\t1         R \rres T = \{~x: X; y: Y |
		\reln xRy \land y \in T @ x \mapsto y~\}
\end{gendef}

\item[Description]
The {\em domain restriction\/} $S \dres R$ of a relation $R$ to
a set $S$ relates $x$ to $y$ if and only if $R$ relates $x$ to
$y$ and $x$ is a member of $S$. The {\em range restriction\/}
$R \rres T$ of $R$ to a set $T$ relates $x$ to $y$ if and only
if $R$ relates $x$ to $y$ and $y$ is a member of $T$.

\item[Laws]
\begin{laws}
     S \dres R = \id S \comp R = (S \cross Y) \cap R \\
     R \rres T = R \comp \id T = R \cap (X \cross T) \\
\also
     \dom (S \dres R) = S \cap (\dom R) \\
     \ran (R \rres T) = (\ran R) \cap T \\
\also
     S \dres R \subseteq R \\
     R \rres T \subseteq R \\
\also
     (S \dres R) \rres T = S \dres (R \rres T) \\
     S \dres (V \dres R) = (S \cap V) \dres R \\
     (R \rres T) \rres W = R \rres (T \cap W)
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:1050}
\item[Name]
\begin{name}
       \ndres & Domain anti-restriction%
		\index{domain: anti-restriction ($\ndres$)}\symdex{$\ndres$} \\
       \nrres & Range anti-restriction%
		\index{range: anti-restriction ($\nrres$)}\symdex{$\nrres$}%
		\index{anti-restriction ($\ndres$, $\nrres$)}
\end{name}

\item[Definition]
\begin{gendef}[X,Y]
        \_ \ndres \_: \power X \cross (X \rel Y) \fun (X \rel Y) \\
        \_ \nrres \_: (X \rel Y) \cross \power Y \fun (X \rel Y)
\where
        \forall S: \power X; R: X \rel Y @ \\
\t1         S \ndres R = \{~x: X; y: Y |
		x \notin S \land \reln xRy @ x \mapsto y~\}
\also
        \forall R: X \rel Y; T: \power Y @ \\
\t1         R \nrres T = \{~x: X; y: Y |
		\reln xRy \land y \notin T @ x \mapsto y~\}
\end{gendef}

\item[Description]
These two operations are the complemented counterparts of the restriction
operations $\_ \dres \_$ and $\_ \rres \_~$. An object $x$
is related to an object $y$ by the relation $S \ndres R$ if and only
if $x$ is related to $y$ by $R$ and $x$ is not a member of $S$.
Similarly, $x$ is related to $y$ by $R \nrres T$ if and only if $x$
is related to $y$ by $R$ and $y$ is not a member of $T$.

\item[Laws]
\begin{laws}
        S \ndres R = (X \setminus S) \dres R \\
        R \nrres T = R \rres (Y \setminus T) \\
\also
        (S \dres R) \cup (S \ndres R) = R \\
        (R \rres T) \cup (R \nrres T) = R
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:1060}
\item[Name]
\begin{name}
     \_^\sim & Relational inversion%
		\index{inversion ($R^\sim$)}\symdex{$R^\sim$}
\end{name}

\item[Definition]
\begin{gendef}[X,Y]
        \_^\sim: (X \rel Y) \fun (Y \rel X)
\where
        \forall R: X \rel Y @ \\
\t1         R^\sim = \{~x: X; y: Y | %
                        \reln xRy @ y \mapsto x~\}
\end{gendef}

\item[Notation] 
The notation $R^{\minus 1}$ is often used for the inverse of a
homogeneous relation $R$ -- one that is in $X \rel X$; it is a
special case of the notation for iteration (see
page~\pageref{p:iter}).

\item[Description]
An object $y$ is related to an object $x$ by the {\em relational
inverse\/} $R^\sim$ of $R$ if and only if $x$ is related to $y$ by $R$.

\item[Laws]
\begin{laws}
        (y \mapsto x) \in R^\sim \iff (x \mapsto y) \in R \\
        (R^\sim)^\sim = R \\
        (Q \comp R)^\sim = R^\sim \comp Q^\sim \\
        (\id V)^\sim = \id V \\
        \dom (R^\sim) = \ran R \\
        \ran (R^\sim) = \dom R \\
        \id (\dom R) \subseteq R \comp R^\sim \\
        \id (\ran R) \subseteq R^\sim \comp R
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:1070}
\item[Name]
\begin{name}
        \_\limg\_\rimg & Relational image%
                \index{relational image ($\_\limg\_\rimg$)}%
		\symdex{$\_\limg\_\rimg$}
\end{name}

\item[Definition]
\begin{gendef}[X,Y]
        \_\limg\_\rimg: (X \rel Y) \cross \power X \fun \power Y
\where
        \forall R: X \rel Y; S: \power X @ \\
\t1         R\limg S\rimg = \{~x: X; y: Y | x \in S \land %
                                        \reln xRy @ y~\}
\end{gendef}

\item[Description]
The {\em relational image\/} $R\limg S\rimg$ of a set $S$ through
a relation $R$ is the set of all objects $y$ to which $R$ relates
some member $x$ of $S$.

\item[Laws]
\begin{laws}
        y \in R\limg S\rimg \iff (\exists x: X @ %
                                        x \in S \land \reln xRy) \\
        R\limg S\rimg = \ran (S \dres R)
\also
        \dom (Q \comp R) = Q^\sim\limg \dom R\rimg \\
        \ran (Q \comp R) = R\limg \ran Q\rimg \\
        R\limg S \cup T\rimg = R\limg S\rimg \cup R\limg T\rimg \\
        R\limg S \cap T\rimg \subseteq R\limg S\rimg \cap R\limg T\rimg \\
        R\limg \dom R\rimg = \ran R
\also
	\dom R = first\limg R\rimg \\
	\ran R = second\limg R\rimg
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:1075}
\item[Name]
\begin{name}
     \oplus & Overriding\index{overriding ($\oplus$)}\symdex{$\oplus$}
\end{name}

\item[Definition]
\begin{gendef}[X,Y]
        \_ \oplus \_: (X \rel Y) \cross (X \rel Y) \fun (X \rel Y)
\where
        \forall Q, R: X \rel Y @ \\
\t1         Q \oplus R = ((\dom R) \ndres Q) \cup R
\end{gendef}

\item[Description]
The relation $Q \oplus R$ relates everything in the domain of $R$ to
the same objects as $R$ does, and everything else in the domain of
$Q$ to the same objects as $Q$ does.

\item[Laws]
\begin{laws}
        R \oplus R = R \\
        P \oplus (Q \oplus R) = (P \oplus Q) \oplus R \\
        \empty \oplus R = R \oplus \empty = R \\
\also
        \dom (Q \oplus R) = (\dom Q) \cup (\dom R) \\
        \dom Q \cap \dom R = \empty \implies Q \oplus R = Q \cup R \\
\also
        V \dres (Q \oplus R) = (V \dres Q) \oplus (V \dres R) \\
        (Q \oplus R) \rres W \subseteq (Q \rres W) \oplus (R \rres W)
\end{laws}
If $f$ and $g$ are functions, then
\begin{laws}
        x \in (\dom f) \setminus (\dom g) \implies (f \oplus g)~x = f~x \\
        x \in \dom g \implies (f \oplus g)~x = g~x.
\end{laws}
\end{manpage}
%-----------------------------------------------------
\def\reflexivetransitiveclosure{reflexive--transitive closure}
\begin{manpage}\label{p:1080}
\item[Name]
\begin{name}
        \_ \plus & Transitive closure%
		\index{transitive closure ($R \plus$)}\symdex{$R \plus$} \\
        \_ \star & Reflexive--transitive closure%
                \index{\reflexivetransitiveclosure ($R \star$)}%
		\symdex{$R \star$}
\end{name}

\item[Definition]
\begin{gendef}[X]
        \_ \plus, \_ \star: (X \rel X) \fun (X \rel X)
\where
        \forall R: X \rel X @
\also
\t1        R \plus = \bigcap \{~Q: X \rel X |
		R \subseteq Q \land Q \comp Q \subseteq Q~\} \land
\also
\t1        R \star = \bigcap \{~Q: X \rel X |
		\id X \subseteq Q \land R \subseteq Q \land 
						Q \comp Q \subseteq Q~\}
\end{gendef}

\item[Description]
If $R$ is a relation from a set $X$ to itself, $R \plus$ is the strongest
or smallest relation containing $R$ which is transitive, and $R \star$ is the
strongest relation containing $R$ which is both reflexive and
transitive.

For an alternative definition of $R \plus$ and $R \star$ in terms of
iteration, see the laws on page~\pageref{p:iter}.

\item[Laws]
\begin{laws}
     R \subseteq R \plus \\
     R \plus \comp R \plus \subseteq R \plus \\
     R \subseteq Q \land Q \comp Q \subseteq Q \implies R \plus \subseteq Q
\also
     \id X \subseteq R \star \\
     R \subseteq R \star \\
     R \star \comp R \star = R \star \\
     \id X \subseteq Q \land R \subseteq Q \land Q \comp Q \subseteq Q %
                                                \implies R \star \subseteq Q
\also
     R \star = R \plus \cup \id X = (R \cup \id X) \plus \\
     R \plus = R \comp R \star = R \star \comp R \\
     (R \plus) \plus = R \plus \\
     (R \star) \star = R \star
\also
     S \subseteq R \star\limg S\rimg \\
     R\limg R \star\limg S\rimg\rimg \subseteq R \star\limg S\rimg \\
     S \subseteq T \land R\limg T\rimg \subseteq T \implies
		R \star\limg S\rimg \subseteq T
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{pagestyle}{manpage}
\paragraph{Monotonic operations}\label{p:1090}\label{p:mono}

A function $f: \power X \fun \power Y$ is {\em monotonic\/}%
\index{monotonic function}%
	\glossary{[monotonic function] A function
	$f: \power X \fun \power Y$ with the property that
	$f(S) \subseteq f(T)$ if $S \subseteq T$.}
if
\begin{laws}
     S \subseteq T \implies f(S) \subseteq f(T).
\end{laws}
A function $g: \power X \cross \power Y \fun \power Z$ is {\em
monotonic in both arguments\/} if
\begin{laws}
    S \subseteq U \land T \subseteq V
	\implies g(S, T) \subseteq g(U, V).
\end{laws}
Many operations on sets and relations are monotonic, including the
closure operators $R \plus$ and $R \star$; others satisfy the
stronger property of being disjunctive (see below).
It is a theorem that a function $f: \power X \fun \power Y$ is
monotonic if and only if for all $S, T: \power X$,
\begin{laws}
	f(S \cap T) \subseteq f(S) \cap f(T).
\end{laws}
Also, a function $f$ is monotonic if and only if the following
inequality holds for all $S$ and $T$:
\begin{laws}
        f(S) \cup f(T) \subseteq f(S \cup T).
\end{laws}
If the stronger property
$f(S \cup T) = f(S) \cup f(T)$
holds, we say that $f$ is {\em disjunctive}. Disjunctive functions
include the domain and range operations $\dom$ and $\ran$ and
inversion $R \inv$.
A function $g$ of two arguments is {\em disjunctive in both
arguments\/} if
\begin{laws}
	g(S \cup T, U) = g(S, U) \cup g(T, U) \\
	g(S, U \cup V) = g(S, U) \cup g(S, V).
\end{laws} 
Many binary operations on sets and relations are disjunctive in both
arguments, including $\cup$, $\cap$, $\comp$, $\dres$, $\rres$ and
$\_\limg\_\rimg$. These disjunctive operations are also monotonic,
so they share all the properties of monotonic functions.

A few other operations, such as $\setminus$, $\ndres$ and $\nrres$
are disjunctive in one argument and `anti-monotonic' in the other,
in the sense that, for example,
\begin{laws}
        S \ndres (Q \cup R) = (S \ndres Q) \cup (S \ndres R) \\
        S \subseteq T \implies T \ndres R \subseteq S \ndres R.
\end{laws}

If $f: \power X \fun \power X$ is monotonic, then Tarski's
theorem\index{Tarski's theorem} says that it has a least fixed
point\index{fixed point} $S$ given by
\[ S = \bigcap \{~T: \power X | f(T) \subseteq T~\}. \]
This set $S$ has the following two properties:
\[
	f(S) = S
\also
	\forall T: \power X | f(T) \subseteq T @ S \subseteq T.
\]
The first property is that $S$ is a fixed point of $f$, and the second
is that $S$ is included in all `pre-fixed points' of $f$: in
particular, it is a subset of every other fixed point of $f$.
\newpage
\end{pagestyle}
%-----------------------------------------------------
\section{Functions}\label{s:funlib}
\begin{manpage}\label{p:2010}
\item[Name]
\begin{name}
     \pfun & Partial functions\index{function}%
		\index{partial function ($\pfun$)}\symdex{$\pfun$} \\
     \fun & Total functions%
		\index{total function ($\fun$)}\symdex{$\fun$}\\
     \pinj & Partial injections\index{injection ($\pinj$, $\inj$)}%
		\index{partial injection ($\pinj$)}\symdex{$\pinj$} \\
     \inj & Total injections%
		\index{total injection ($\inj$)}\symdex{$\inj$}\\
     \psurj & Partial surjections\index{surjection ($\psurj$, $\surj$)}%
		\index{partial surjection ($\psurj$)}\symdex{$\psurj$} \\
     \surj & Total surjections%
		\index{total surjection ($\surj$)}\symdex{$\surj$} \\
     \bij & Bijections\index{bijection ($\bij$)}\symdex{$\bij$}
\end{name}

\item[Definition]

\[
     X \pfun Y  ==
        \{~f: X \rel Y | (\forall x: X; y_1, y_2: Y @ \\
\t5		(x \mapsto y_1) \in f \land (x \mapsto y_2) \in f %
                                                  \implies y_1 = y_2)~\}
\also
     X \fun Y  ==  \{~f: X \pfun Y | \dom f = X~\}
\also
     X \pinj Y  ==
	\{~f: X \pfun Y | (\forall x_1, x_2: \dom f @ 
		f(x_1) = f(x_2) \implies x_1 = x_2)~\}
\also
     X \inj Y  ==  (X \pinj Y) \cap (X \fun Y)
\also
     X \psurj Y  ==  \{~f: X \pfun Y | \ran f = Y~\}
\also
     X \surj Y  ==  (X \psurj Y) \cap (X \fun Y)
\also
     X \bij Y  ==  (X \surj Y) \cap (X \inj Y)
\]


\item[Description]
If $X$ and $Y$ are sets, $X \pfun Y$ is the set of {\em partial
functions\/} from $X$ to $Y$. These are relations which relate each
member $x$ of $X$ to at most one member of $Y$. This member of
$Y$, if it exists, is written $f(x)$. The set $X \fun Y$ is the set of %FIXED
{\em total functions\/} from $X$ to $Y$. These are partial functions
whose domain is the whole of $X$; they relate each member of $X$ to
exactly one member of $Y$. An alternative definition of $X \fun Y$
was given on page~\pageref{p:forward}. It is equivalent to the one
given~here.

The arrows $\pinj$, $\inj$, and $\bij$ with barbed tails make sets of
functions that are {\em injective}.  $X \pinj Y$ is the set of {\em
partial injections\/} from $X$ to $Y$.  These are partial functions
from $X$ to $Y$ which map different elements of their domain to
different elements of their range. $X \inj Y$ is the set of {\em
total injections\/} from $X$ to $Y$, the partial injections that
are also total functions.

The arrows $\psurj$, $\surj$, and $\bij$ with double heads make sets
of functions that are {\em surjective}.  $X \psurj Y$ is the set of
{\em partial surjections\/} from $X$ to $Y$.  These are partial
functions from $X$ to $Y$ which have the whole of $Y$ as their
range. $X \surj Y$ is the set of {\em total surjections\/} from $X$
to $Y$, the functions which have the whole of $X$ as their domain
and the whole of $Y$ as their range. 

The set $X \bij Y$ is the set of {\em bijections\/} from $X$ to $Y$.
These map the elements of $X$ onto the elements of $Y$ in a
one-to-one correspondence. As suggested by its shape, $X \bij Y$
contains exactly those total functions that are both injective and
surjective.

\item[Laws]
\begin{laws}
        f \in X \pfun Y \iff f \circ f^\sim = \id (\ran f)
\also
        f \in X \pinj Y \iff f \in X \pfun Y \land f^\sim \in Y \pfun X \\
        f \in X \inj Y \iff f \in X \fun Y \land f^\sim \in Y \pfun X
\also
        f \in X \pinj Y \implies %
                f\limg S \cap T\rimg = f\limg S\rimg \cap f\limg T\rimg
\also
        f \in X \bij Y \iff f \in X \fun Y \land f^\sim \in Y \fun X \\
        f \in X \psurj Y \implies f \circ f^\sim = \id Y
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{pagestyle}{manpage}\label{p:2050}
\paragraph{Relational operations on functions}

Functions\index{function: as relation} are just a special kind of
relation, so the relational operations, such as $\circ$, $\dres$ and
$\oplus$, may be used on functions.  Many of these operations yield
functions when applied to functions, and some preserve other
properties such as injectivity.

The identity relation\index{identity relation: as function}
is a function -- in fact, an injection -- and
composition, restriction and overriding map functions to functions:
\begin{laws}
        S \subseteq X \implies \id S \in X \pinj X \\
        \id X \in X \bij X
\also
        f \in X \pfun Y \land g \in Y \pfun Z \implies %
                g \circ f \in X \pfun Z \\
        f \in X \fun Y \land g \in Y \pfun Z \land %
		\ran f \subseteq \dom g \implies g \circ f \in X \fun Z
\also
        f \in X \pfun Y \implies S \dres f \in X \pfun Y \\
        f \in X \pfun Y \implies f \rres T \in X \pfun Y
\also
	f \in X \pfun Y \land g \in X \pfun Y \implies %
		f \oplus g \in X \pfun Y.
\end{laws}
The composition\index{backward composition: of functions} of two 
injections and the restriction of an injection
are again injections, and inversion maps injections to injections:
\begin{laws}
        f \in X \pinj Y \land g \in Y \pinj Z \implies %
                g \circ f \in X \pinj Z \\
        f \in X \pinj Y \implies S \dres f \in X \pinj Y \\
        f \in X \pinj Y \implies f \rres T \in X \pinj Y \\
        f \in X \pinj Y \implies f^\sim \in Y \pinj X.
\end{laws}
Finally, set-theoretic operations may be used to combine functions.
Note especially that the union of two functions is a function
only if they agree on the intersection of their domains:
\begin{laws}
        f \in X \pfun Y \land g \in X \pfun Y \land \\
\t1        (\dom f) \dres g = (\dom g) \dres f \implies f \cup g \in X \pfun Y
\also
        f \in X \pfun Y \land g \in X \pfun Y 
		\implies f \cap g \in X \pfun Y \\
        f \in X \pinj Y \land g \in X \pinj Y 
		\implies f \cap g \in X \pinj Y.
\end{laws}
The last two laws are special cases of the laws that any subset of a
function is a function, and any subset of an injection is an
injection:
\begin{laws}
	f \in X \pfun Y \land g \subseteq f \implies g \in X \pfun Y \\
	f \in X \pinj Y \land g \subseteq f 
		\implies g \in X \pinj Y.
\end{laws}
\newpage
\end{pagestyle}
%-----------------------------------------------------
\section{Numbers and finiteness}\label{s:numlib}
\begin{manpage}\label{p:3010}
\item[Name]
\begin{name}
     \nat & Natural numbers\index{natural number ($\nat$)}\symdex{$\nat$} \\
     \num & Integers\index{integer ($\num$)}\symdex{$\num$} \\
     +, \minus, *, \div\index{division ($\div$)}, \mod\index{modulo ($\mod$)} %
                & Arithmetic operations\index{arithmetic operation}%
		\symdex{$+$}\symdex{$\minus$}\symdex{$*$} \\
     \lt, \leq, \geq, \gt & Numerical comparison\index{comparison: numerical}%
		\symdex{$\lt$}\symdex{$\leq$}\symdex{$\geq$}\symdex{$\gt$}%
		\index{natural number>integer}
\end{name}

\item[Definition]
%% It's a syntax error to have unary minus other than first in its
%% declaration part.  Why?
\[ [\num] \]
\begin{axdef}
        \_ + \_~, \_ \minus \_~, \_ * \_: \num \cross \num \fun \num \\
        \_ \div \_~, \_ \mod \_: 
		\num \cross (\num \setminus \{0\}) \fun \num \\
	\minus: \num \fun \num
\also
        \_ \lt \_~, \_ \leq \_~, \_ \geq \_~, \_ \gt \_: \num \rel \num
\where
        \hbox{\dots\ definitions omitted \dots}
\end{axdef}
\[ \nat == \{~n: \num | n \geq 0~\} \]

\item[Notation]
Decimal notation may be used for elements of $\nat$.  Negative numbers
may be written down using the unary minus function $(\minus)$.

\item[Description]
$\nat$ is the set of {\em natural numbers\/} $\{ 0, 1, 2, \ldots \}$,
and $\num$ is the set of integers
$\{ \ldots, \minus 2, \minus 1, 0, 1, 2, \ldots \}$.
The usual {\em arithmetic operations\/} of addition, subtraction,
multiplication, integer division and modulo are provided. Integer
division and the modulo operation use truncation towards minus
infinity, so that they together obey the three laws listed below.
Numbers may be compared with the usual ordering relations.

\item[Laws]
\begin{laws}
        b \gt 0 \implies 0 \leq a \mod b \lt b
\also
        b \neq 0 \implies a = (a \div b) * b + a \mod b
\also
        b \neq 0 \land c \neq 0 \implies (a * c) \div (b * c) = a \div b
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:3020}
\item[Name]
\begin{name}
        \nat_1 & Strictly positive integers%
                	\index{integer: strictly positive ($\nat_1$)}%
			\symdex{$\nat_1$} \\
        succ & Successor function\index{successor function ($succ$)} \\
        \upto & Number range\index{number range ($\upto$)}\symdex{$\upto$}
\end{name}

\item[Definition]
\[ \nat_1  ==  \nat \setminus \{0\} \]
\begin{axdef}
        succ: \nat \fun \nat \\
	\_ \upto \_: \num \cross \num \fun \power \num
\where
        \forall n: \nat @ succ(n) = n + 1
\also
	\forall a, b: \num @ \\
\t1         a \upto b = \{~k: \num | a \leq k \leq b~\}
\end{axdef}

\item[Description]
$\nat_1$ is the set of {\em strictly positive integers}; it contains
every natural number except $0$. If $n$ is a natural number, $succ(n)$
is the next one, namely $n+1$. If we take $succ$ as primitive,
it is possible to describe all the operations on numbers in terms
of it.

If $a$ and $b$ are integers and $a \leq b$, then $a \upto b$ is the %FIXED
set of integers between $a$ and $b$ inclusive. If $a \gt b$ then $a
\upto b$ is empty.

\item[Laws]
\begin{laws}
     succ \in \nat \bij \nat_1
\also
     a \gt b \implies a \upto b = \empty \\
     a \upto a = \{a\} \\
     a \leq b \land c \leq d \implies b \upto c \subseteq a \upto d
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:iter}\label{p:3030}
\item[Name]
\begin{name}
     R^k & Iteration\index{iteration ($R^k$, $iter$)}\symdex{$R^k$}
\end{name}

\item[Definition]
\begin{gendef}[X]
        iter: \num \fun (X \rel X) \fun (X \rel X)
\where
        \forall R: X \rel X @ \\
\t1        iter~0~R = \id X \land \\
\t1        (\forall k: \nat @ iter~(k+1)~R = %
                                        R \comp (iter~k~R)) \land \\
\t1        (\forall k: \nat @ iter~(\minus k)~R = iter~k~(R^\sim))
\end{gendef}

\item[Notation]
$iter~k~R$ is usually written $R^k$.

\item[Description]
Two objects $x$ and $y$ are related by $R^k$, where $k \geq 0$,
if there are $k+1$ objects $x_0$, $x_1$, \ldots,~$x_k$ with $x = x_0$,
$\reln{x_i}R{x_{i+1}}$ for each $i$ such that $0 \leq i \lt k$,
and $x_k = y$. $R^{\minus k}$ is defined to be $(R^\sim)^k$.

\item[Laws]
\begin{laws}
        R^0 = \id X \\
        R^1 = R \\
        R^2 = R \comp R \\
        R^{\minus 1} = R^\sim \\
        k \geq 0 \implies R^{k+1} = R \comp R^k = R^k \comp R
\also
        (R^\sim)^a = (R^a)^\sim \\
        a \geq 0 \land b \geq 0 \implies R^{a+b} = R^a \comp R^b \\ %FIXED
        R^{a*b} = (R^a)^b
\also
        R \plus = \bigcup \{~k: \nat_1 @ R^k~\} \\
        R \star = \bigcup \{~k: \nat @ R^k~\}
\also
        R \comp S = S \comp R \implies %
                (R \comp S)^a = R^a \comp S^a
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:3050}
\item[Name]
\begin{name}
     \finset   & Finite sets\index{finite set ($\finset$)}\symdex{$\finset$} \\
     \finsetone & Non-empty finite sets%
                \index{finite set: non-empty ($\finsetone$)}%
		\symdex{$\finsetone$} \\
     \#        & Number of members of a set%
                \index{cardinality ($\#$)}\index{size of a set ($\#$)}%
		\symdex{$\#$}
\end{name}

\item[Definition]
\[
        \finset X  ==  \{~S: \power X | \exists n: \nat @ %
			\exists f: 1 \upto n \fun S @ \ran f = S~\}
\also
        \finsetone X  ==  \finset X \setminus \{\empty\}
\]
\begin{gendef}[X]
        \#: \finset X \fun \nat
\where
        \forall S: \finset X @ \\
\t1         \# S = (\mu n: \nat | 
			(\exists f: 1 \upto n \inj S @ \ran f = S))
\end{gendef}

\item[Description]
A subset $S$ of $X$ is {\em finite\/} ($S \in \finset X$) if and
only if the members of $S$ can be counted with some natural number.
In this case, there is a unique natural number which counts the
members of $S$ without repetition, and this is the {\em size\/}
$\# S$ of $S$. The sets in $\finsetone X$ are the non-empty members
of $\finset X$: those finite sets $S$ with $\# S \gt 0$.

\item[Laws]
\begin{laws}
        S \in \finset X \iff (\forall f: S \inj S @ \ran f = S)
\also
        \empty \in \finset X \\
        \forall S: \finset X; x: X @ S \cup \{x\} \in \finset X
\also
        \# (S \cup T) = \# S + \# T \minus \# (S \cap T)
\also
	\finsetone X = \{~S: \finset X | \#S \gt 0~\}
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:3060}
\item[Name]
\begin{name}
     \ffun & Finite partial functions%
		\index{finite function ($\ffun$)}\symdex{$\ffun$} \\
     \finj & Finite partial injections%
		\index{finite injection ($\finj$)}\symdex{$\finj$}
\end{name}

\item[Definition]
\[
     X \ffun Y  ==  \{~f: X \pfun Y | \dom f \in \finset X~\}
\also
     X \finj Y  ==  (X \ffun Y) \cap (X \pinj Y)
\]

\item[Description]
If $X$ and $Y$ are sets, $X \ffun Y$ is the set of {\em finite
partial functions\/} from $X$ to $Y$. These are partial functions
from $X$ to $Y$ whose domain is a finite subset of $X$. The set
of {\em finite partial injections\/} $X \finj Y$ contains those
finite partial functions which are also injections.

\item[Laws]
\begin{laws}
        X \ffun Y = (X \pfun Y) \cap \finset (X \cross Y)
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:3070}
\item[Name]
\begin{name}
        min, max & Minimum\index{minimum ($min$)} and %
                maximum\index{maximum ($max$)} of a set of numbers
\end{name}

\item[Definition]

\begin{axdef}
        min: \powerone \num \pfun \num \\
        max: \powerone \num \pfun \num
\where
        min = \{~S: \powerone \num; m: \num | \\
\t2         m \in S \land (\forall n: S @ m \leq n) @ S \mapsto m~\}
\also
        max = \{~S: \powerone \num; m: \num | \\
\t2         m \in S \land (\forall n: S @ m \geq n) @ S \mapsto m~\}
\end{axdef}

\item[Description]
The minimum\index{greatest lower bound} 
of a set $S$ of integers is that element of $S$ which
is smaller than any other, if any. 
The maximum\index{least upper bound} of $S$ is that
element which is larger than any other, if any.

\item[Laws]
\begin{laws}
        \finsetone \num \subseteq \dom min \\
        \finsetone \num \subseteq \dom max \\
        (\power \nat) \cap (\dom min) = \powerone \nat \\
        (\power \nat) \cap (\dom max) = \finsetone \nat \\
\also
        min (S \cup T) = min \{ min~S, min~T \} \\
        max (S \cup T) = max \{ max~S, max~T \}
\also
        min (S \cap T) \geq min~S \\
        max (S \cap T) \leq max~S
\also
        a \leq b \implies min (a \upto b) = a \land max (a \upto b) = b \\
        (a \upto b) \cap (c \upto d) = max\{a,c\} \upto min\{b,d\}
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{pagestyle}{manpage}
\paragraph{Proof by induction}\label{p:3080}

Mathematical induction\index{induction: proof by}
provides a method of proving universal
properties of natural numbers. To show that a property $P(n)$
holds of all natural numbers $n$, it is enough to show that
\begin{oblig}{a}
\item $P(0)$ holds.

\item If $P(n)$ holds for some $n: \nat$, so does $P(n+1)$:
        \[ \forall n: \nat @ P(n) \implies P(n+1). \]
\end{oblig}
A similar proof method may be used to prove that $P(S)$ holds
for all finite sets $S: \finset X$. It is enough to show that
\begin{oblig}{b}
\item $P(\empty)$ holds.

\item If $P(S)$ holds then $P(S \cup \{x\})$ holds also:
        \[ \forall S: \finset X; x: X @ P(S) 
		\implies P(S \cup \{x\}). \]
\end{oblig}
A more powerful proof method for the natural numbers is to assume
as hypothesis not just that the immediately preceding number has
the property $P$, but that all smaller numbers do. To establish
the theorem $\forall n: \nat @ P(n)$ by
this method, it is enough to show the single fact
\begin{oblig}{c}
\item If for all $k \lt n, P(k)$ holds, so does $P(n)$:
        \[ \forall n: \nat @ %
                (\forall k: \nat | k \lt n @ P(k)) \implies P(n). \]
\end{oblig}
There is no need for a separate case for $n = 0$, because proving
(c1) entails proving $P(0)$ under no assumptions, for there is
no natural number $k$ satisfying $k \lt 0$.

Analogously, a more powerful proof method for sets requires that
a property $P$ be proved to hold of a finite set under the hypothesis
that it holds of all {\em proper\/} subsets. To establish
$\forall S: \finset X @ P(S)$, it is enough to show:
\begin{oblig}{d}
\item $\forall S: \finset X @ (\forall T: \finset X | T
                \subset S @ P(T)) \implies P(S).$
\end{oblig}
Again, since the empty set has no 
proper subsets\index{subset: proper}, there is no
need for a separate case for $S = \empty$.
\newpage
\end{pagestyle}
%-----------------------------------------------------
\section{Sequences}\label{s:seqlib}
\begin{manpage}\label{p:seq}\label{p:4010}
\item[Name]
\begin{name}
        \seq  & Finite sequences\index{sequence ($\seq$)} \\
        \seq_1 & Non-empty finite sequences%
                \index{sequence: non-empty ($\seq_1$)} \\
	\iseq & Injective sequences
\end{name}

\item[Definition]
\[
     \seq X  ==  \{~f: \nat \ffun X | \dom f = 1 \upto \#f~\}
\also
     \seq_1 X  ==  \{~f: \seq X | \#f \gt 0~\}
\also
     \iseq X  ==  \seq X \cap (\nat \pinj X)
\]

\item[Notation]
We write $<a_1, \ldots, a_n>$%
\index{sequence: display ($<\ldots>$)} as a shorthand for the set
\[ \{ 1 \mapsto a_1, \ldots, n \mapsto a_n \}. \]
The empty sequence\index{sequence: empty ($<>$)}
$<>$ is an alternative notation
for the empty function $\empty$ from $\nat$ to $X$.

\item[Description]
$\seq X$ is the set of finite sequences over $X$. These are
finite functions from $\nat$ to $X$ whose domain is a segment
$1 \upto n$ for some natural number $n$. $\seq_1 X$ is the set of
all finite sequences over $X$ except the empty sequence $<>$.

$\iseq X$ is the set of injective finite sequences over $X$: these are
precisely the finite sequences over $X$ which contain no repetitions.

\item[Laws]
\begin{laws}
	\seq_1 X = \seq X \setminus \{<>\}
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:4020}
\item[Name]
\begin{name}
        \cat & Concatenation\index{concatenation ($\cat$)}\symdex{$\cat$} \\
        rev & Reverse\index{reverse ($rev$)}
\end{name}

\item[Definition]
\begin{gendef}[X]
        \_ \cat \_ :  \seq X \cross \seq X \fun \seq X \\
        rev: \seq X \fun \seq X
\where
        \forall s, t: \seq X @ \\
\t1	    s \cat t = s \cup \{~n: \dom t @ n + \#s \mapsto t(n)~\}
\also
        \forall s: \seq X @ \\
\t1         rev~s = (\lambda n: \dom s @ s(\#s \minus n + 1))
\end{gendef}

\item[Description]
For sequences $s$ and $t$, $s \cat t$ is the {\em concatenation\/}
of $s$ and $t$. It contains the elements of $s$ followed by the
elements of $t$.

If $s$ is a sequence, $rev~s$ is the sequence containing the same
elements as $s$, but in reverse order.

\item[Laws]
\begin{laws}
        (s \cat t) \cat u = s \cat (t \cat u) \\
        <> \cat s = s \\
        s \cat <> = s \\
        \# (s \cat t) = \#s + \#t
\also
        rev~<> = <> \\
        rev~<x> = <x> \\
        rev (s \cat t) = (rev~t) \cat (rev~s)
\also
        rev (rev~s) = s
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:4040}
\item[Name]
\begin{name}
        head\index{$head$}, last\index{$last$}, %
        tail\index{$tail$}, front\index{$front$} & Sequence decomposition
\end{name}

\item[Definition]

\begin{gendef}[X]
        head, last: \seq_1 X \fun X \\
        tail, front: \seq_1 X \fun \seq X
\where
        \forall s: \seq_1 X @ \\
\t1         head~s = s(1) \land \\
\t1         last~s = s(\#s) \land \\
\t1         tail~s = (\lambda n: 1 \upto \#s \minus 1 @ s(n+1)) \land \\
\t1         front~s = (1 \upto \#s \minus 1) \dres s
\end{gendef}

\item[Description]
For a non-empty sequence $s$, $head~s$ and $last~s$ are the first
and last elements of $s$ respectively. The sequences $tail~s$
and $front~s$ contain all the elements of $s$ except for the first
and except for the last respectively.

\item[Laws]

\begin{laws}
	head~<x> = last~<x> = x \\
	tail~<x> = front~<x> = <>
\also
        s \neq <> \implies \\
\t1		head(s \cat t) = head~s \land \\
\t1        	tail(s \cat t) = (tail~s) \cat t \\
        t \neq <> \implies \\
\t1		last(s \cat t) = last~t \land \\
\t1		front(s \cat t) = s \cat (front~t) \\
\also
        s \neq <> \implies %
                <head~s> \cat (tail~s) = s \\
        s \neq <> \implies %
                (front~s) \cat <last~s> = s
\also
        s \neq <> \implies head (rev~s) = last~s 
			\land tail(rev~s) = rev(front~s) \\ %FIXED
        s \neq <> \implies last (rev~s) = head~s
			\land front(rev~s) = rev(tail~s) %FIXED
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:4060}
\item[Name]
\begin{name}
	\extract & Extraction%
			\index{extraction ($\extract$)}\symdex{$\extract$} \\
        \filter & Filtering\index{filtering ($\filter$)}\symdex{$\project$} \\
	squash & Compaction\index{$squash$}
\end{name}

\item[Definition]
\begin{gendef}[X]
	\_ \extract \_: \power \nat_1 \cross \seq X \fun \seq X \\
	\_ \filter \_: \seq X \cross \power X \fun \seq X \\
	squash: (\nat_1 \ffun X) \fun \seq X
\where
	\forall U: \power \nat_1; s: \seq X @ \\
\t1		U \extract s = squash~(U \dres s)
\also
	\forall s: \seq X; V: \power X @ \\
\t1		s \filter V = squash~(s \rres V)
\also
	\forall f: \nat_1 \ffun X @ \\
\t1	    squash~f = f \circ (\mu p: 1 \upto \#~f \bij \dom f |
			p \circ succ \circ p \inv \subseteq (\_ \lt \_))
\end{gendef}

\item[Description]
If $U$ is a set of indices and $s$ is a sequence, then $U \extract
s$ is a sequence that contains exactly those elements of $s$ that
appear at an index in $U$, in the same order as in $s$.
If $s$ is a sequence over $X$ and $V$ is a subset of $X$, then
$s \filter V$ is a sequence which contains just those elements of $s$
which are members of $V$, in the same order as in $s$.
Both are defined using a function $squash$ that takes a finite
function defined on the strictly positive integers and compacts it
into a sequence.

\item[Laws]
\begin{laws}
	<> \filter V = U \extract <> = <> \\
        (s \cat t) \filter V = (s \filter V) \cat (t \filter V)
\also
        \ran s \subseteq V \iff s \filter V = s \\
        s \filter \empty = \empty \extract s = <> \\
        \# (s \filter V) \leq \# s \\
        (s \filter V) \filter W = s \filter (V \cap W)
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:4065}
\item[Name]
\begin{name}
	\prefix	& Prefix relation\index{$\prefix$: for sequences} \\
	\suffix	& Suffix relation\index{$\suffix$: for sequences} \\
	\inseq	& Segment relation\index{$\IN$: for sequences}%
				\index{segment ($\inseq$)}
\end{name}

\item[Definition]
\begin{gendef}[X]
	\_\prefix\_~, \_\suffix\_~, \_\inseq\_:
		\seq X \rel \seq X
\where
	\forall s, t: \seq X @ \\
\t1		s \prefix t \iff (\exists v: \seq X @ s \cat v = t) \land \\
\t1		s \suffix t \iff (\exists u: \seq X @ u \cat s = t) \land \\
\t1		s \inseq t \iff (\exists u, v: \seq X @ u \cat s \cat v = t)
\end{gendef}

\item[Description]
These relations hold when one sequence is a contiguous part of
another, taken either from the front ($\prefix$), from the back
($\suffix$) or from anywhere ($\inseq$).  They are all partial
orders on sequences: cf.\ page~\pageref{p:80}\index{partial order}.

\item[Laws]
\begin{laws}
	s \prefix t \iff s = (1 \upto \#s) \extract t \\
	s \suffix t \iff s = (\#t \minus \#s+1 \upto \#t) \extract t \\
	s \inseq t \iff
            (\exists n: 1 \upto \#t @ 
		s = (n \upto n+\#s-1) \extract t) %% FIXED
\also
	s \inseq t \iff
	    (\exists u: \seq X @ s \suffix u \land u \prefix t) \\
	s \inseq t \iff
	    (\exists v: \seq X @ s \prefix v \land v \suffix t)
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{pagestyle}{manpage}\label{p:4090}
\paragraph{Relational operations on sequences}

Sequences\index{sequence: as function} are a special kind of
function -- the ones with domain $1 \upto k$ for some $k$~-- and
functions\index{function: as relation} are a special kind of
relation, so operations defined on relations may be used on
sequences.

If $s: \seq X$ and $f: X \fun Y$, then\index{backward composition: on
sequences} $f \circ s \in \seq Y$ -- it is the sequence with the same
length as $s$ whose elements are the images of corresponding elements
of $s$ under $f$:
\begin{laws}
        \# (f \circ s) = \# s \\
        \forall i: 1 \upto \#s @ (f \circ s)(i) = f(s(i))
\also
        f \circ <> = <> \\
        f \circ <x> = <f(x)> \\
        f \circ (s \cat t) = (f \circ s)\cat(f \circ t).
\end{laws}
Another useful relational operation for sequences is 
`$\ran$'\index{range: on sequences}. The
range $\ran s$ of a sequence $s$ is just the set of objects which
are elements of the sequence:
\begin{laws}
        \ran s = \{~i: 1 \upto \#s @ s(i)~\}
\also
        \ran~<> = \empty \\
        \ran~<x> = \{x\} \\
        \ran (s \cat t) = (\ran s) \cup (\ran t).
\end{laws}
These operations interact with sequence operations such as 
$rev$ and $\filter$ in the expected way:
\begin{laws}
        rev (f \circ s) = f \circ (rev~s) \\
        \ran (rev~s) = \ran s
\also
        (f \circ s) \filter V = f \circ (s \filter f^\sim\limg V\rimg) \\
        \ran (s \filter V) = (\ran s) \cap V.
\end{laws}
\newpage
\end{pagestyle}
%-----------------------------------------------------
\begin{manpage}\label{p:4070}
\item[Name]
\begin{name}
        \dcat & Distributed concatenation%
                \index{distributed concatenation ($\dcat$)}\symdex{$\dcat$}
\end{name}

\item[Definition]
\begin{gendef}[X]
        \dcat: \seq (\seq X) \fun \seq X
\where
        \dcat <> = <> \\
        \forall s: \seq X @ \dcat <s> = s \\
        \forall q, r: \seq (\seq X) @ \\
\t1         \dcat (q \cat r) = (\dcat q) \cat (\dcat r)
\end{gendef}

\item[Description]
If $q$ is a sequence of sequences, $\dcat q$ is the result of
concatenating all the elements of $q$, one after another.

\item[Laws]
\begin{laws}
        \dcat <s, t> = s \cat t. \\
\end{laws}
The following four laws show the interaction of $\dcat$ with $rev$,
$\filter$, $\circ$ and~`$\ran$':
\begin{laws}
        rev (\dcat q) = \dcat (rev (rev \circ q)).
\end{laws}
The expression on the right of this law can be evaluated by reversing
each sequence in $q$, reversing the resulting sequence of sequences,
then concatenating the result with $\dcat$.
\begin{laws}
        (\dcat q) \filter V =
                \dcat ((\lambda s: \seq X @ s \filter V) \circ q).
\end{laws}
On the right-hand side, each sequence in $q$ is filtered by $V$, and
the results are concatenated.
\begin{laws}
        f \circ (\dcat q) = 
                \dcat ((\lambda s: \seq X @ f \circ s) \circ q).
\end{laws}
On the right-hand side of this law, $f$ is composed with each sequence
in $q$, and the results are concatenated.
\begin{laws}
        \ran (\dcat q) = \bigcup \{~i: 1 \upto \#q @ \ran (q(i))~\}
			= \bigcup(\ran(\ran \circ q)).
\end{laws}
The middle expression is the union of the ranges of the
individual elements of $q$. The right-hand expression is a shorter
way of saying the same thing.
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:4080}
\item[Name]
\begin{name}
        \disjoint  & Disjointness\index{$\disjoint\unskip$} \\
        \partition & Partitions\index{$\partition$}
\end{name}

\item[Definition]
\begin{gendef}[I,X]
        \disjoint \_: \power (I \pfun \power X) \\
        \_ \partition \_: (I \pfun \power X) \rel \power X
\where
        \forall S: I \pfun \power X; T: \power X @ \\
\t1         (\disjoint S \iff \\
\t2             (\forall i,j: \dom S | i \neq j @ %
                                        S(i) \cap S(j) = \empty)) \land \\
\t1         (S \partition T \iff \\
\t2             \disjoint S \land \bigcup \{~i: \dom S @ S(i)~\} = T)
\end{gendef}

\item[Description]
An indexed family of sets $S$\index{indexed family of sets}
is {\em disjoint\/} if and only if
each pair of sets $S(i)$ and $S(j)$ for $i \neq j$ have empty
intersection. The family $S$ {\em partitions\/} a set $T$ if, in
addition, the union of all the sets $S(i)$ is $T$. A particularly
common example of an indexed family of sets is a 
sequence\index{sequence: as function} of sets,
which is at base only a function defined on a subset of $\nat$.

\item[Laws]
\begin{laws}
        \disjoint \empty \\
        \disjoint < A > \\
        \disjoint < A, B > \iff A \cap B = \empty
\also
        < A, B > \partition C \iff %
                        A \cap B = \empty \land A \cup B = C
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{pagestyle}{manpage}\label{p:4100}
\paragraph{Induction for sequences}

Proof by induction\index{induction: proof by}
is valid for natural numbers and for finite
sets because every natural number can be reached from zero by
repeatedly adding one, and every finite set can be reached from
the empty set by repeatedly inserting new members. There are two
`generation principles'
like this for sequences, and they correspond
to two slightly different styles of proof by induction.
First, any sequence can be reached from the empty sequence by
repeatedly extending it with new elements. So to prove that a
property $P(s)$ holds of all finite sequences $s: \seq X$, it is
enough to show that
\begin{oblig}{a}
\item $P(<>)$ holds.

\item If $P(s)$ holds for some sequence $s$, then
        $P(s\cat<x>)$ holds also:
        \[ \forall s: \seq X; x: X @ %
                P(s) \implies P(s\cat<x>). \]
\end{oblig}
A variant of this style of induction builds up sequences from
the back instead of from the front: (a2) may be replaced by
\begin{oblig}{a}
\item[($\rm a2'$)] If $P(s)$ holds for some sequence $s$,
        then $P(<x>\cat s)$ holds also:
        \[ \forall x: X; s: \seq X @ %
                P(s) \implies P(<x>\cat s). \]
\end{oblig}

A third way of building up sequences is to start with the empty
sequence $<>$ and singleton sequences $<x>$,
and to obtain longer sequences by concatenating shorter ones.
So to prove $\forall s: \seq X @ P(s)$, it is enough to prove that
\begin{oblig}{b}
\item $P(<>)$ holds.
\item $P(<x>)$ holds for all $x: X$.
\item If $P(s)$ and $P(t)$ hold, so does $P(s\cat t)$:
        \[ \forall s, t: \seq X @ P(s) \land P(t) 
		\implies P(s \cat t). \]
\end{oblig}

Although, on the face of it, proofs in this style are more
long-winded because there are three cases instead of two, in
practice it often leads to more elegant proofs than the first one.
The sequence operations $rev$, $\filter$, and $\dcat$ obey laws that
relate their value on $s \cat t$ to their values on $s$ and $t$, as
do the actions of $\ran$, and $\circ$ on sequences, so proofs about
them fit naturally into this style, and there is no need to break
the symmetry in favour of the first element or the last, as the
other style would require.
\newpage
\end{pagestyle}
%-----------------------------------------------------
\section{Bags}\label{s:baglib}
\begin{manpage}\label{p:bag}\label{p:5010}
\item[Name]
\begin{name}
        \bag   		& Bags\index{bag} \\
        count, \bcount & Multiplicity\index{$count$}\symdex{$\bcount$} \\
	\otimes	& Bag scaling%
		\index{bag: scaling ($\otimes$)}\symdex{$\otimes$}
\end{name}

\item[Definition]
\[ \bag X == X \pfun \nat_1 \]
\begin{gendef}[X]
        count: \bag X \bij (X \fun \nat) \\
	\_ \bcount \_: \bag X \cross X \fun \nat \\
	\_ \otimes \_: \nat \cross \bag X \fun \bag X
\where
	\forall B: \bag X @ \\
\t1         count~B = (\lambda x: X @ 0) \oplus B
\also
        \forall x: X; B: \bag X @ \\
\t1	    B \bcount x = count~B~x
\also
	\forall n: \nat; B: \bag X; x: X @ \\
\t1	    (n \otimes B) \bcount x = n * (B \bcount x)
\end{gendef}

\item[Notation]
We write $\lbag a_1, \ldots, a_n \rbag$%
\index{bag: display ($\lbag\ldots\rbag$)} for the bag
$\{ a_1 \mapsto k_1, \ldots, a_n \mapsto k_n \}$,
where for each $i$, the element $a_i$ appears $k_i$ times in the
list $a_1$, \dots,~$a_n$. The empty bag\index{bag: empty
($\lbag\rbag$)} $\lbag \rbag$ is a notation for the empty function
$\empty$ from $X$ to $\nat$.

\item[Description]
$\bag X$ is the set of {\em bags\/} or multisets %FIXED
of elements of $X$. These are
collections of elements of $X$ in which the number of times an
element occurs is significant. The number of times $x$ appears in
the bag $B$ is $count~B~x$ or $B \bcount x$.
If $n$ is a natural number, $n \otimes B$ is the bag
$B$ {\em scaled} by a factor of $n$: any element appears in it $n$
times as often as it appears in $B$.

\item[Laws]
\begin{laws}
        \dom~\lbag a_1, \ldots, a_n\rbag = \{a_1, \ldots, a_n\}
\also
	n \otimes \lbag~\rbag = 0 \otimes B = \lbag~\rbag \\
	1 \otimes B = B \\
	(n * m) \otimes B = n \otimes (m \otimes B) \\
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:inbag}\label{p:5015}
\item[Name]
\begin{name}
        \inbag 		& Bag membership%
		  \index{bag: membership ($\inbag$)}\symdex{$\inbag$} \\
	\subbageq	& Sub-bag relation%
		  \index{sub-bag ($\subbageq$)}\symdex{$\subbageq$}
\end{name}

\item[Definition]
\begin{gendef}[X]
        \_ \inbag \_: X \rel \bag X \\
	\_ \subbageq \_: \bag X \rel \bag X
\where
        \forall x: X; B: \bag X @ \\
\t1         (x \inbag B \iff x \in \dom B)
\also
	\forall B, C: \bag X @ \\
\t1	    B \subbageq C \iff
		(\forall x: X @ B \bcount x \leq C \bcount x)
\end{gendef}

\item[Description]
The relationship $x \inbag B$ holds exactly if $x$ appears in $B$ a
non-zero number of times.  A bag $B$ is a sub-bag of another bag $C$
($B \subbageq C$) if each element occurs in $B$ no more often than it
occurs in $C$.

\item[Laws]
\begin{laws}
        x \inbag B \iff B \bcount x \gt 0
\also
	B \subbageq C \implies \dom B \subseteq \dom C
\also
	\lbag~\rbag \subbageq B \\
	B \subbageq B \\
	B \subbageq C \land C \subbageq B \implies B = C \\
	B \subbageq C \land C \subbageq D \implies B \subbageq D
\end{laws}
\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:5020}
\item[Name]
\begin{name}
        \uplus & Bag union\index{bag: union ($\uplus$)}\symdex{$\uplus$} \\
        \uminus & Bag difference%
		\index{bag: difference ($\uminus$)}\symdex{$\uminus$}
\end{name}

\item[Definition]
\begin{gendef}[X]
        \_ \uplus \_~, \_ \uminus \_: \bag X \cross \bag X \fun \bag X
\where
        \forall B, C: \bag X; x: X @ \\
\t1 	    (B \uplus C) \bcount x = B \bcount x + C \bcount x \land \\
\t1	    (B \uminus C) \bcount x
		= max~\{B \bcount x \minus C \bcount x, 0\}
\end{gendef}

\item[Description] 
$B \uplus C$ is the {\em bag union\/} of $B$ and
$C$: the number of times any object appears in $B \uplus C$ is the
sum of the number of times it appears in $B$ and in $C$. $B \uminus
C$ is the {\em bag difference\/} of $B$ and $C$: the number of times
any object appears in it is the number of times it appears in $B$
minus the number of times it appears in $C$, or zero if that would
be negative.

\item[Laws]
\begin{laws}
        \dom (B \uplus C) = \dom B \cup \dom C
\also
        \lbag~\rbag \uplus B = B \uplus \lbag~\rbag = B \\
        B \uplus C = C \uplus B \\
        (B \uplus C) \uplus D = B \uplus (C \uplus D)
\also
	B \uminus \lbag~\rbag = B \\
	\lbag~\rbag \uminus B = \lbag~\rbag \\
	(B \uplus C) \uminus C = B
\also
	(n + m) \otimes B = n \otimes B \uplus m \otimes B \\
	n \geq m \implies 
	    (n \minus m) \otimes B = n \otimes B \uminus m \otimes B \\
	n \otimes (B \uplus C) = n \otimes B \uplus n \otimes C \\
	n \otimes (B \uminus C) = n \otimes B \uminus n \otimes C \\
\end{laws}
\end{manpage}
%-----------------------------------------------------
%\begin{manpage}\label{p:5025}
%\item[Name]
%\begin{name}
%	\biguplus & Generalized bag union%
%		\index{generalized union: for bags ($\biguplus$)}%
%		\symdex{$\biguplus$} \\
%	\sum & Sum of a bag of numbers\index{sum ($\sum$)}\symdex{$\sum$}
%\end{name}
%
%\item[Definition]
%\begin{gendef}[X]
%	\biguplus: \bag (\bag X) \fun \bag X
%\where
%	\biguplus \lbag~\rbag = \lbag~\rbag
%\also
%	\forall B: \bag X @ \biguplus \lbag B \rbag = B
%\also
%	\forall BB, CC \bag (\bag X) @ \\
%\t1		\biguplus (BB \uplus CC) = \biguplus BB \uplus \biguplus CC
%\end{gendef}
%
%\begin{axdef}
%	\sum: \bag \num \fun \num
%\where
%	\sum \lbag~\rbag = 0
%\also
%	\forall n: \num @ \sum \lbag n \rbag = n
%\also
%	\forall B, C: \bag \num @ \\
%\t1		\sum (B \uplus C) = \sum B + \sum C
%\end{axdef}
%
%\item[Description]
%\dots
%\end{manpage}
%-----------------------------------------------------
\begin{manpage}\label{p:5030}
\item[Name]
\begin{name}
        items\index{$items$} & Bag of elements of a sequence
\end{name}

\item[Definition]
\begin{gendef}[X]
        items: \seq X \fun \bag X
\where
        \forall s: \seq X; x: X @ \\
\t1         (items~s) \bcount x = \# \{~i: \dom s | s(i) = x~\}
\end{gendef}

\item[Description]
If $s$ is a sequence, $items~s$ is the bag in which each element
$x$ appears exactly as often as $x$ appears in $s$.

\item[Laws]
\begin{laws}
        \dom (items~s) = \ran s
\also
        items~<a_1, \ldots, a_n> = \lbag a_1, \ldots, a_n\rbag
\also
        items (s \cat t) = items~s \uplus items~t
\also
	items~s = items~t \iff \\
\t1     	( \exists f: \dom s \bij \dom t @ s = t \circ f )
\end{laws}
\end{manpage}

\chapter{Sequential Systems}\label{c:seqprog}

The Z language described in Chapter~\ref{c:language} is a system of
notation for building structured mathematical theories, and the
library of definitions in Chapter~\ref{c:library} provides a
vocabulary for that language; but neither has any necessary connection
with computer programming.  Even the most complex Z specification is,
from one point of view, nothing more than a mathematical theory with a
certain structure.  This chapter explains the conventions which allow
us to use these structured mathematical theories to describe computer
programs.  It concentrates on sequential, imperative programming,
explaining how schemas describe the state space and operations of
abstract data types.  It also explains rules for proving that one
abstract data type is implemented by another.

\section{States and operations}

An abstract data type\index{abstract data type+}%
	\glossary{[abstract data type] A {\em state space}, together
	with an initial state and a number of operations.  In Z,
	these are all described using schemas.}
consists of a set of states, called the
{\em state space\/}\index{state space},%
	\glossary{[state space] The set of states which an
	abstract data type can have.  In Z, the state space is
	specified by a schema with the same name as the abstract data
	type. None of the components of this schema should have a
	decoration.}
a non-empty set of {\em initial states}, and a number of {\em operations}.
Each operation has certain input and output variables, and is specified by a
relationship between the input and output variables and a pair of states, one
representing the state before execution of the operation, and the other
representing the state afterwards.

In Z, the set of states of an abstract data type is specified by a
schema, usually with the same name as the data type
itself.  By convention, none of the components of the state space
schema has any decoration. As an example, the following schema
defines the state space of a simple counter with a current value and a
limit:
\begin{schema}{Counter}
	value, limit: \nat
\where
	value \leq limit
\end{schema}
Here the state space is the set $\{\,Counter \spot \theta Counter\,\}$
of bindings having two components $value$ and $limit$ with $0 \leq
value \leq limit$. All states of the system obey this invariant
relationship documented by the declaration of $value$ and $limit$ and
by the predicate part of the schema.

The set of initial states of an abstract data type is specified by
another schema with the same signature as the state space schema.
The abstract data type may start in any one of the initial states;
often there is only one of them. Here is a schema describing an
initial state for the counter:
\begin{schema}{InitCounter}
	Counter
\where
	value = 0 \\
	limit = 100
\end{schema}
For a specification to describe a genuine abstract data type, there
must be at least one possible initial state. In the example, this is
expressed by the theorem
\[ \exists Counter \spot InitCounter. \]

The operations of an abstract data type are specified by schemas which
have all the components of both $State: Exp $ and
$State'$, where $State: Exp $ is the schema describing the state space. The
state of the abstract data type before the operation is modelled by
the undashed components of its schema, and the state afterwards is
modelled by the components decorated with a dash. As an example, here
is an operation which increments the value of the counter by one:
\begin{schema}{Inc}
	Counter \\
	Counter'
\where
	value' = value + 1 \\
	limit' = limit
\end{schema}
Because of the meaning of schema inclusion (see Section~\ref{s:decl}),
the properties of $Counter$ and $Counter'$ are implicitly part of the
property of this schema: it is implicitly part of the specification of
the operation that the invariant relationship holds before and after it.

The property of this schema is a relationship between the state before
the operation and the state after it: this relationship holds when the
invariant is satisfied by both these states, and they are related by
the two predicates in the body of $Inc$. Here is how this relationship
can be understood as specifying a program. Think of a state before the
operation is executed; if the state is related to at least one
possible state after the operation, then the operation must terminate
successfully, and the state after the operation must be one of those
related to the state before it. If the predicate relates the state
before the operation to no possible state afterwards, then nothing is
guaranteed: the operation may fail to terminate, may terminate
abnormally, or may terminate successfully in any state at all.

\new The {\em pre-condition\/}\index{pre-condition}%
	\glossary{[pre-condition] The predicate that is true of
	those inputs and states before an operation that are 
	related by its post-condition to at least one output and state 
	after it.}
of an operation holds of exactly those states before the operation that are
related to at least one possible state after it.  If $Op$ is a
schema describing an operation on a state space $State: Exp $, then $\pre
Op$ is a schema describing its pre-condition: if $Op$ has no inputs
or outputs, $\pre Op$ is equivalent to the schema
\[ \exists State' \spot Op. \]
This has the same signature as $State: Exp $, but its property is the
pre-condition of the operation $Op$.
The pre-condition schema $\pre Inc$ of the operation $Inc$ is
\begin{schema*}
	Counter
\where
	\exists Counter' \spot \\
\t1		value' = value + 1 \land \\
\t1		limit' = limit
\end{schema*}
The state after the operation is implicitly required to
satisfy the invariant, and the predicate in this schema is logically
equivalent to
\[
	\exists value', limit': \nat \mid value' \leq limit' \spot \\
\t1		value' = value + 1 \land limit' = limit,
\]
or to $value + 1 \leq limit$. This means that $value$
must be strictly less than $limit$ for the success of $Inc$ to be
guaranteed.  It is a useful check on the accuracy of a specification
to make such {\em implicit pre-conditions\/}\index{pre-condition: implicit}%
	\glossary{[implicit pre-condition] A pre-condition of an
	operation which is not explicitly stated in its specification,
	but is implicitly part of the post-condition or of the
	invariant on the final state.}
explicit and check them against the expected pre-condition.
Also, the state before the operation is required to satisfy the
invariant: the specification of $Inc$ implicitly includes the fact
that $Inc$ need not behave properly if started in an invalid state.

As well as states before and after execution, operations can have
inputs and outputs.  The inputs are modelled by components of the
schema decorated with~$?$, and the outputs by components decorated
with~$!$.  Here is an operation which adds its input to the
value of the counter, and outputs the new value:
\begin{schema}{Add}
	Counter \\
	Counter' \\
	jump?: \nat \\
	new\_value!: \nat
\where
	value' = value + jump? \\
	limit' = limit \\
	new\_value! = value'
\end{schema}
This operation is guaranteed to terminate successfully, provided the
state before execution and the input satisfy the implicit pre-condition that
$value + jump? \leq limit$. If this pre-condition is satisfied, then
the state after execution and the output will satisfy the
relationship specified in the body of the schema.

The schema operator `$\rm pre$' is also defined for operations with
inputs and outputs. If $Op$ has the input $x?: X$ and the output
$y!: Y$, then $\pre Op$ is the schema
\[ \exists State'; y!: Y \spot Op \]
whose components are the state variables from $State: Exp $, together with
the input $x?$. The pre-condition $\pre Add$ schema for $Add$ is
the schema
\begin{schema*}
	Counter \\
	jump?: \nat
\where
	\exists Counter'; new\_value!: \nat \spot \\
\t1		value' = value + jump? \\
\t1		limit' = limit \\
\t1		new\_value! = value'
\end{schema*}
The predicate part of this schema is logically equivalent to
$value + jump? \leq limit$.

Both the operations $Inc$ and $Add$ have the property that the state after the
operation and the output are completely determined by the state before
the operation and the input, but this need not be the case.  It is
possible to specify {\em non-deterministic\/}\index{non-determinism}%
	\glossary{[non-deterministic] An operation in an {\em abstract
	data type\/} is non-deterministic if there may be more than
	one possible state after execution of the operation for a
	single state before it.}
operations, in which the state before the operation and the input
determine a range of possible outputs and states after the operation.
Non-deterministic operations are important because they sometimes
allow specifications to be made simpler and more
abstract.\index{abstract data type-}

% \section{The $\Delta$ and $\Xi$ conventions}\label{s:deltaxi}
\section{The \char1\ and \char4\ conventions}\label{s:deltaxi}

Operations\index{Delta convention ($\Delta$)+} on data types are
specified by schemas which have two copies of the state variables
among their components: an undecorated set corresponding to the
state of the data type before the operation, and a dashed set
corresponding to the state after the operation.  To make it more
convenient to declare these variables, there is a convention that
whenever a schema $State: Exp $ is introduced as the state space of an
abstract data type, the schema $\Delta State$\symdex{$\Delta$} is
implicitly defined as the combination of $State: Exp $ and $State'$,
unless a different definition is made explicitly:
%%unchecked
\begin{schema}{\Delta State}
	State \\
	State'
\end{schema}
With this definition, each operation on the data type can be specified by
extending $\Delta State$ with declarations of the inputs and outputs of the
operation and predicates giving the pre-condition and post-condition.

The character $\Delta$ is just a letter in the name of this schema, and
the implicit definition of $\Delta State$ is no more than a convention.
In many specifications, a different definition is given to $\Delta State$:
for example, the state of the data type may contain a count of the number
of operations performed so far, and the fact that it is incremented at
each operation could be made part of $\Delta State$, rather than
repeating it for each operation specified.

\new Generic schemas may be used with $\Delta$ too; if the schema $State: Exp $
has, say, two generic parameters, then so does $\Delta State$, and it
is implicitly defined as follows:
%%unchecked
\begin{schema}{\Delta State[X, Y]}
	State[X, Y] \\
	State'[X, Y]
\end{schema} 
As before, the specifier is free to define $\Delta State$ in any
other way, and even with a different number of generic parameters.
In the default definition shown here, the formal parameters $X$ and
$Y$ have been used, but they may clash with other names used in the
specification; if this happens, the definition of $\Delta State$
uses other identifiers that do not appear elsewhere.\index{Delta
convention-}

Many\index{Xi convention ($\Xi$)+} data types have operations which
access information in the state without changing the state at all.
This fact can be recorded by including the equation $\theta State =
\theta State'$ in the post-condition of the operation, but it is
convenient to have a special schema $\Xi State$\symdex{$\Xi$} on
which these access operations can be built. Like $\Delta State$, the
schema $\Xi State$ is implicitly defined whenever a schema $State: Exp $
is introduced as the state space of a data type:  
%%unchecked
\begin{schema}{\Xi State}
	State \\
	State'
\where
	\theta State = \theta State'
\end{schema}
Again, this definition may be overridden by an explicit definition of
$\Xi State$: if, for example, a record were being kept of the number
of operations performed on the system, $\Xi State$ might say that no
part of the state changed except the count.

Like $\Delta$, the $\Xi$ symbol may also be used with generic
schemas; if $State: Exp $ has two parameters, then the implicit definition
of $\Xi State$ is as follows:
%%unchecked
\begin{schema}{\Xi State[X, Y]}
	State[X, Y] \\
	State'[X, Y]
\where
	\theta State = \theta State'\index{Xi convention-}
\end{schema}

\section{Loose specifications}\label{s:loose}

The\index{loose specification+} schemas which define the state space
and operations of an abstract data type may refer to global variables
of the specification, and (as discussed in Section~\ref{ss:glovar})
there may be more than one binding of these variables that satisfies
the global property of the specification. In other words, the
predicates which constrain the global variables may not completely
fix their values.  We call specifications in which this happens {\em
loose} specifications.%
	\glossary{[loose specification] A specification in which the
	values of global variables are not completely determined
	by the predicates which constrain them.}
The same kind of thing occurs with specifications which introduce new
basic types by the mechanism described in Section~\ref{ss:basictype},
because the specification does not fix what objects are members of the
basic types.

There are several circumstances where loose specifications and new basic
types are useful:
\begin{itemize}
\item	The specification may describe in detail only some aspects of
	a system, but need to mention other things not specified in
	detail. For example, a text editor needs to deal with
	characters, and it might treat blanks specially; but the
	specification need not say precisely what characters there
	are, except that one of them is the blank character.

\item	There may be constants of a system which must be chosen by the
	implementor. For example, a filing system may encode its
	directory information in data blocks, and this encoding must
	be constant, but it can be chosen by the implementor of the
	filing system.

\item	There may be parameters of a system chosen when the system is
	configured. For example, an operating system may run on
	machine configurations with any number of disk drives, and the
	implementor must allow the number to be chosen when the
	operating system is configured.
\end{itemize}
Whatever use is made of loose specifications, they provide a way to
describe a {\em family} of abstract data types.  Each binding of
global variables that
satisfies the global property of the specification identifies one
member of the family.  In some cases it is up to the implementor to
choose a member of the family and implement it; in other cases, the
choice is forced by information outside the formal specification; and
sometimes all the members of the family must be implemented, so that
one of them can be chosen later. In all these cases, the formal
specification describes the range of members in the family, but the
way the choice is made is outside its scope.%
\index{loose specification-}

\section{Sequential composition and piping}

If $Op1$ and $Op2$ are schemas describing two operations, then 
$Op1 \semi Op2$ is a schema which describes their {\em sequential
composition}.\index{sequential composition}\symdex{$\semi$}%
	\glossary{[sequential composition] The sequential composition
	$Op1 \semi Op2$ of two operation schemas $Op1$ and $Op2$
	describes a composite operation in which first $Op1$ then
	$Op2$ occurs.}
For it to be defined, each dashed component of $Op1$ must match in
type any undashed component of $Op2$ that matches it in name, and any
other components, including inputs, outputs, and unmatched state
variables, shared by $Op1$ and $Op2$ must have the same types in both
of them.  The components of $Op1 \semi Op2$ are the merged
components of $Op1$ and $Op2$, with the matching state variables
hidden.  The formal definition of $Op1 \semi Op2$ is given on
page~\pageref{p:opcomp}.

Some care is needed in the case of
non-deterministic\index{non-determinism} operations, for the meaning
of $Op1 \semi Op2$ then differs from the meaning that would be
natural in a programming language, in that its pre-condition is more
liberal.  In $Op1 \semi Op2$, the state in which $Op1$ finishes is
chosen, if possible, to satisfy the pre-condition of $Op2$, so the
pre-condition of $Op1 \semi Op2$ requires only the existence of a
possible intermediate state. In programming, the pre-condition would
require that {\em every} possible state after $Op1$ should satisfy
the pre-condition of $Op2$. The specification $Op1 \semi Op2$ is correctly
implemented by the program `$Op1; Op2$' if the following sufficient condition
holds:
\[
	\forall State'' \spot \\
\t1	    (\exists Op1 \spot \theta State' = \theta State'') \\
\t1	    \implies (\exists Op2 \spot \theta State = \theta State'').
\]
This condition says that any state in which $Op1$ may finish
satisfies the pre-condition of $Op2$.

If $Op1$ and $Op2$ share any outputs, $Op1 \semi Op2$ specifies that
the same values should be produced as output by both operations; there
is no direct way of achieving this in a program.

Returning to the % counter example, 
example of the counter, $Inc \semi Inc$ describes an operation which
adds 2 to the value of the counter. The operation $Inc \semi Add$
adds to the counter one more than its input, producing the new value
as output. It is the schema
\begin{schema*}
	\Delta Counter \\
	jump?: \nat \\
	new\_value!: \nat
\where
	value' = value + jump? + 1 \\
	limit' = limit \\
	new\_value! = value'
\end{schema*}
In contrast, $Add \semi Inc$ has the same effect, but its output is
one less than the final value of the counter:
\begin{schema*}
	\Delta Counter \\
	jump?: \nat \\
	new\_value!: \nat
\where
	value' = value + jump? + 1 \\
	limit' = limit \\
	new\_value! = value + jump?
\end{schema*}
This is because the output now comes from the first of the two
operations, and is produced before the final increment.

The piping operator $\pipe$ is useful for describing operations that
have an almost independent effect on two disjoint sets of state
variables.  In $Op1 \pipe Op2$, the outputs of $Op1$ (i.e.\ the
components decorated with $!$) are matched with the inputs of $Op2$
(decorated with $?$) and hidden, but the other components are merged
as they would be in $Op1 \land Op2$.

An example is the operation $AddSquare$ which inputs a number and
adds its square to the value of the counter, producing the new value
as output.  Here is an operation with no state variables that squares
its input:
\begin{schema}{Square}
	x?, y!: \nat
\where
	y! = x? * x?
\end{schema}
The whole operation $AddSquare$ is defined by
\begin{zed}
	AddSquare \defs Square \pipe Add[y?/jump?].
\end{zed}
Renaming has been used to make the output of $Square$ match the
input of $Add$.  The schema $AddSquare$ is equivalent to
\begin{schema*}
	\Delta Counter \\
	x?: \nat \\
	new\_value!: \nat
\where
	value' = value + x? * x? \\
	limit' = limit \\
	new\_value! = value'
\end{schema*}

\section{Operation refinement}

When a program is developed from a specification, two sorts of design
decision usually need to be taken: the operations described by
predicates in the specification must be implemented by algorithms
expressed in a programming language, and the data described by
mathematical data types in the specification must be implemented by
data structures of the programming language.

This section contains the rules for simple {\em operation
refinement}.\index{operation refinement+}%
	\glossary{[operation refinement] The process of showing that
	one operation is implemented by another with the same
	state space. In its general form, this allows constructs from
	a programming language to be introduced into a design.}
This allows us to show that one operation is a
correct implementation of another operation with the same state space,
when both operations are specified by schemas.  This is the simplest
kind of refinement of one operation by another, and it needs to be
extended in two directions to make it generally useful in program
development. One of these directions, the introduction of programming
language constructs, is outside the scope of this book. The other
direction, {\em data refinement},%
	   \glossary{[data refinement] The process of showing that one
	   set of operations is implemented by another set operating on a
	   different state space.  Data refinement allows the
	   mathematical data types of a specification to be replaced
	   in a design by more computer-oriented data types.}
by which computer-oriented data structures can be introduced, is the
subject of Section~\ref{s:dataref}. 

If a concrete operation $Cop$ is an operation refinement of an
abstract operation $Aop$, there are two ways they can differ. The
pre-condition of $Cop$ may be more liberal than the pre-condition of
$Aop$, so that $Cop$ is guaranteed to terminate for more states than
is $Aop$. Also, $Cop$ may be more deterministic than $Aop$, in that
for some states before the operation, the range of possible states
afterwards may be smaller.  But $Cop$ must be guaranteed to terminate
whenever $Aop$ is, and if $Aop$ is guaranteed to terminate, then every
state which $Cop$ might produce must be one of those which $Aop$ might
produce.

Here is the first of these conditions expressed as a predicate. The
schema $State: Exp $ is the state space of the abstract data type, and $Aop$
and $Cop$ are operations with an input $x?: X$ and an output $y!: Y$:
\[ \forall State; x?: X \spot \pre Aop \implies \pre Cop. \]
This predicate uses the pre-condition operator `$\rm pre$', but it can also
be expressed directly in terms of the existential quantifier
$\exists$:
\[
	\forall State; x?: X \spot \\
\t1		(\exists State'; y!: Y \spot Aop) \implies
				(\exists State'; y!: Y \spot Cop).
\]
If the pre-condition of $Aop$ is satisfied, then every result which
$Cop$ might produce must be a possible result of $Aop$. This is
expressed by the following predicate:
\[
	\forall State; State'; x?: X; y!: Y \spot \\
\t1		\pre Aop \land Cop \implies Aop.
\]
Again this can be expressed without using `$\rm pre$':
\[
	\forall State; x?: X \spot \\
\t1	    (\exists State'; y!: Y \spot Aop) \\
\t1	    \implies (\forall State'; y!: Y \spot Cop \implies Aop).
\]
If these two conditions are satisfied, then the concrete operation is
suitable for all purposes for which the abstract operation was
suitable. If the abstract operation could be relied upon to terminate,
then so can the concrete operation.  This is the content of the first
condition. Also, if the abstract operation could be relied on to
produce a state after execution which had a certain property, then so
can the concrete operation, because the second condition guarantees
that all the states which might be reached by the concrete operation
can also be reached by the abstract operation.%
\index{operation refinement-}

\section{Data refinement}\label{s:dataref}

Data refinement\index{data refinement+} extends operation refinement
by allowing the state space of the concrete operations to be different
from the state space of the abstract operations.  It allows the
mathematical data types of a specification to be replaced by more
computer-oriented data types in a design.

A step of data refinement relates an {\em abstract\/} data type, the
specification, to a {\em concrete\/} data type, the design.  In fact,
the concrete data type is another abstract data type, in the sense
that it consists of a state space and some operations described by
schemas. In this section, we shall call the state space of the
abstract data type $Astate$, and the state space of the concrete data
type $Cstate$.  These state space schemas must not have any components
in common. We shall use the names $Aop$ and $Cop$ to refer to an
operation on the abstract state space, and the corresponding operation
which implements it on the concrete data space. These operations have
input $x?: X$ and output $y!: Y$.

In order to prove that the concrete data type correctly implements the
abstract data type, we must explain which concrete states
represent which abstract states.  This is done with an
{\em abstraction schema\/}\index{abstraction schema},%
	\glossary{[abstraction schema] In data refinement, a schema
	which documents the relationship between the abstract and
	concrete state spaces.}
which we shall call $Abs$. This schema relates abstract and concrete
states: it has the same signature as $Astate \land Cstate$, and its
property holds if the concrete state is one of those which represent
the abstract state.  It is quite usual for one abstract state to be
represented by many concrete states. As an example, finite sets can be
represented by sequences in which the order of elements does not
matter; in this representation, a set of size $n$ can be represented
by any one of $n$ factorial different sequences with the elements in
different orders.

It is also possible for several abstract states to be represented by
the same concrete state; this can happen if the abstract state
contains information which cannot be extracted by any of the
operations on the abstract data type. However, a simpler set of
rules applies to the case where each concrete state represents a
unique abstract state: this simpler set is listed in the last part of
this section.  It is not necessary for every abstract state to be
represented, but only enough of them that one possible result of
each execution of an operation on the type is represented.  This
means that abstract states which can never be reached using the
operations need not be represented.

\new For each operation of an abstract data type, there are two
conditions which must be satisfied for a data refinement to be
correct, and they are analogues of the two conditions of operation
refinement.  The first condition ensures that the concrete operation
terminates whenever the abstract operation is guaranteed to
terminate. If an abstract state and a concrete state are related by
the abstraction schema $Abs$, and the abstract state satisfies the
pre-condition of the abstract operation, then the concrete state must
satisfy the pre-condition of the concrete operation.  In symbols:
\[
	\forall Astate; Cstate; x?: X \spot \\
\t1		\pre Aop \land Abs \implies \pre Cop.
\]
The second condition ensures that the state after the concrete
operation represents one of those abstract states in which the
abstract operation could terminate.  If an abstract state and a
concrete state are related by $Abs$, and both the abstract and
concrete operations are guaranteed to terminate, then every possible
state after the concrete operation must be related by $Abs'$ to a
possible state after the abstract operation. In symbols:
\[
	\forall Astate; Cstate; Cstate'; x?: X; y!: Y \spot \\
\t1		\pre Aop \land Abs \land Cop \implies
			(\exists Astate' \spot Abs' \land Aop).
\]
These two conditions should be proved for each operation on the data
types.

Another condition relates the initial states of the abstract and
concrete types. Each possible initial state of the concrete type must
represent a possible initial state of the abstract type. In symbols:
\[
	\forall Cstate \spot \\
\t1	    Cinit \implies (\exists Astate \spot Ainit \land Abs).
\]

\subsection*{Example}

\new Chapter~\ref{c:tutorial} contains an example of data refinement
in which the birthday-book specification is implemented using an
array of names and an array of dates.  The abstraction schema
relates the abstract state space $BirthdayBook$ to the concrete
state space $BirthdayBook1$, and is defined like this:
\begin{schema}{Abs}
	BirthdayBook \\
	BirthdayBook1
\where
	known = \{\,i: 1 \upto hwm \spot names(i)\,\} \\
\also
	\forall i: 1 \upto hwm \spot \\
\t1		birthday(names(i)) = dates(i)
\end{schema}

To show that $AddBirthday1$ correctly implements the $AddBirthday$
operation, we need to prove that its pre-condition is liberal
enough, and that it produces the right answer.  For the
pre-condition, we need to show
\[
	\forall BirthdayBook; BirthdayBook1;
			name?: NAME; date?: DATE \spot \!\!\\
\t1		\pre AddBirthday \land Abs \implies \pre AddBirthday1.
\]
This formula can be simplified by substituting the actual
pre-conditions
\[ name? \notin known \]
for $\pre AddBirthday$ and
\[ \forall i: 1 \upto hwm @ name? \neq names(i) \]
for $\pre AddBirthday1$, and replacing $Abs$ by the weaker condition
\[ known = \{\,i: 1 \upto hwm \spot names(i)\,\}. \]
This gives
\[
	\forall BirthdayBook; BirthdayBook1;
			name?: NAME; date?: DATE \spot \!\! \\
\t1		name? \notin known \land \\
\t1		known = \{\,i: 1 \upto hwm \spot names(i)\,\} \\
\t1		\implies (\forall i: 1 \upto hwm @ name? \neq names(i)),
\]
exactly the fact that is proved by the calculation in
Chapter~\ref{c:tutorial}.
To show that the result of $AddBirthday1$ is right, we must prove
\[
	\forall BirthdayBook; BirthdayBook1; \\
\t2		BirthdayBook1'; name?: NAME; date?: DATE \spot \\
\t1	    \pre AddBirthday \land Abs \land AddBirthday1 \\
\t1	    \implies (\exists BirthdayBook' \spot Abs' \land AddBirthday).
\]
This formula contains an inconvenient existential quantifier, but it
can be eliminated using the `one-point rule'\index{one-point rule},
that the predicate
\[ (\exists x: X @ x = E \land \ldots x \ldots) \]
is logically equivalent to the predicate obtained by deleting the
quantifier and the defining equation for $x$, and substituting $E$
for $x$ in what remains.  This rule applies to both the
variables $known'$ and $birthday'$ of $BirthdayBook'$, because
$AddBirthday$ contains the equation
\[ birthday' = birthday \cup \{name? \mapsto date?\}, \]
and $BirthdayBook'$ itself contains the equation
\[ known' = \dom birthday'. \]
After expanding $Abs$ and $AddBirthday$, applying the one-point rule
and making the substitutions for $birthday'$ and $known'$, the last
line of the correctness formula becomes
\[
	\dom (birthday \cup \{name? \mapsto date?\})
		= \{~i: 1 \upto hwm' @ names'(i)~\} \land \\
	(\forall i: 1 \upto hwm' @
		birthday'(names'(i)) = dates'(i)) \land \\
	name? \notin known.
\]
The three conjuncts in this formula can be proved using
facts from the hypotheses $\pre AddBirthday$, $Abs$, and
$AddBirthday1$; but it is easier to exploit the fact that $Abs$
defines a total function from concrete to abstract states, and use
the rules in the next section.

\section{Functional data refinement}

A simpler set of conditions can be used if the abstraction schema,
when viewed as a relation between concrete states and abstract states,
is a total function. This property of $Abs$ is expressed by the predicate
\[ \forall Cstate \spot \exists_1 Astate \spot Abs. \]
The first condition on each operation is the same as before:
\[
	\forall Astate; Cstate; x?: X \spot \\
\t1		\pre Aop \land Abs \implies \pre Cop.
\]
The existential quantifier in the second condition can be avoided; the
condition simplifies to
\[
	\forall Astate; Astate'; Cstate; Cstate'; x?: X; y!: Y \spot \\
\t1		\pre Aop \land Abs \land Cop \land Abs' \implies Aop.
\] 
The condition on initial states can also be simplified to avoid the
existential quantifier:
\[ \forall Astate; Cstate \spot Cinit \land Abs \implies Ainit. \]
These simplified conditions are equivalent to the general ones if the
abstraction schema is a total function.  Their advantage is that the
proof that $Abs$ is functional need be done only once for the whole
data type, and this work does not have to be repeated for each
operation.

\subsection*{Example}

\new In the birthday book example, the abstraction schema is in fact
functional, because it directly defines $known$, the domain of the
$birthday$ function, and also gives the value of $birthday$ at each
point of its domain.  So instead of proving the second condition for
the correctness of $AddBirthday$ given in Section~\ref{s:dataref},
it is enough to prove the simpler condition
\[
	\forall BirthdayBook; BirthdayBook'; BirthdayBook1; \\
\t2			BirthdayBook1'; name?: NAME; date?: DATE @ \\
\t1	    	\pre AddBirthday \land Abs \land AddBirthday1 \land Abs' \\
\t1	    	\implies AddBirthday.
\]
\filbreak
Expanding the schemas, substituting pre-conditions and simplifying
slightly gives the formula
\[
	\forall BirthdayBook; BirthdayBook'; BirthdayBook1; \\
\t2			BirthdayBook1'; name?: NAME; date?: DATE @ \\
\t1	    name? \notin known \land \\
\t1	    known = \{~i: 1 \upto hwm @ names(i)~\} \land \\
\t1	    (\forall i: 1 \upto hwm @ birthday(names(i)) = dates(i)) \land \\
\t1	    hwm' = hwm + 1 \land \\
\t1	    names' = names \oplus \{hwm' \mapsto name?\} \land \\
\t1	    dates' = dates \oplus \{hwm' \mapsto date?\} \land \\
\t1	    known' = \{~i: 1 \upto hwm' @ names'(i)~\} \land \\
\t1	    (\forall i: 1 \upto hwm @ birthday'(names'(i)) = dates'(i)) \\
\t1	    \implies birthday' = birthday \cup \{name? \mapsto date?\}.
\]
This is, in all its detail, exactly the result proved by the
calculation in Chapter~\ref{c:tutorial}.\index{data refinement-}

\chapter{Syntax Summary}\label{c:syntax}

This syntax summary supplements the syntax rules in
Chapter~\ref{c:language} by making precise the binding powers of
various constructs and collecting all the rules in one place.

\mathchardef\semicolon="003B
The same conventions about repeated and optional phrases are used here
as in Chapter~\ref{c:language}; \(S, \ldots, S\) stands for a list of
one or more instances of the class \(S\) separated by commas, and
\(S\semicolon\,\ldots\semicolon\,S\) stands for one or more instances of
\(S\) separated by semicolons.  The notation \(S\;\ldots\;S\) stands
for one or more adjacent instances of \(S\) with no separators.
Phrases enclosed in slanted square brackets are optional.

The possibility of eliding semicolons which separate items both above and
below the line in the three kinds of boxes has been made explicit here;
these items are separated by instances of the class \(Sep\), which may be
either semicolons or newlines (\(NL\)). The rule given in
Section~\ref{ss:layout} which allows extra newlines to be inserted
before or after certain symbols is not made explicit in the grammar,
however.

Certain collections of symbols have a range of binding powers: they
are the logical connectives, used in predicates and schema
expressions, the special-purpose schema operators, and infix function
symbols, used in expressions. The relative binding powers of the
logical connectives are indicated by listing them in decreasing order
of binding power; the binding powers of infix function symbols are
given in Section~\ref{ss:infix}. Each production for which a binding
power is relevant has been marked with an upper case letter at the
right margin; `L' marks a symbol which associates to the left -- so $A
\land B \land C$ means $(A \land B) \land C$ -- and `R' marks a symbol
which associates to the right. Unary symbols are marked with~`U'.

\newpage
\begin{raggedbottom}
\zedindent=0pt
\begin{syntax}
Specification\index{\(Specification\)}
	& ::= & Paragraph\;NL\;\ldots\;NL\;Paragraph
\also
Paragraph\index{\(Paragraph\)}
	& ::= & [ Ident, \ldots, Ident ] \\
	&  |  & Axiomatic-Box \\
	&  |  & Schema-Box \\
	&  |  & Generic-Box \\
	&  |  & Schema-Name\;\lopt Gen-Formals\ropt \defs Schema-Exp \\
	&  |  & Def-Lhs == Expression \\
	&  |  & Ident ::= Branch | \ldots | Branch \\
	&  |  & Predicate
\also\also
Axiomatic-Box\index{\(Axiomatic-Box\)} & ::= &
        \qquad \vcenter{\boxpream\boxcontents{Decl-Part}{Axiom-Part}\egroup}
\also\also
Schema-Box\index{\(Schema-Box\)} & ::= &
	\qquad \vcenter{\boxpream
		\boxtop{$\sf Schema-Name\;\lopt Gen-Formals \ropt$}
                \boxcontents{Decl-Part}{Axiom-Part}\boxend\egroup}
\also\also
Generic-Box\index{\(Generic-Box\)} & ::= &
        \qquad \vcenter{
            \setbox0=\hbox{$\sf \lopt Gen-Formals \ropt$}
            \setbox1=\null \wd1=\wd0
            \boxpream\boxtop{\box0}
            \noalign{\kern-\normalbaselineskip\kern-\doublerulesep}
            \boxtop{\box1} \noalign{\kern\doublerulesep}
	    \boxcontents{Decl-Part}{Axiom-Part}\boxend\egroup}
\also\also
Decl-Part\index{\(Decl-Part\)}
	& ::= & Basic-Decl\;Sep\;\ldots\;Sep\;Basic-Decl
\also
Axiom-Part\index{\(Axiom-Part\)}
	& ::= & Predicate\;Sep\;\ldots\;Sep\;Predicate
\also
Sep\index{\(Sep\)}
	& ::= & \semicolon | NL
\also
Def-Lhs\index{\(Def-Lhs\)}
	& ::= & Var-Name\;\lopt Gen-Formals\ropt \\
	&  |  & Pre-Gen\;Decoration\;Ident \\
	&  |  & Ident\;In-Gen\;Decoration\;Ident
\also
Branch\index{\(Branch\)}
	& ::= & Ident \\
	&  |  & Var-Name\,\ldata Expression \rdata
\also
Schema-Exp\index{\(Schema-Exp\)}
	& ::= & \forall Schema-Text @ Schema-Exp \\
	&  |  & \exists Schema-Text @ Schema-Exp \\
	&  |  & \exists_{\rm 1} Schema-Text @ Schema-Exp \\ %*
	&  |  & Schema-Exp-1
\also
Schema-Exp-1\index{\(Schema-Exp-1\)}
	& ::= & [ Schema-Text ] \\
	&  |  & Schema-Ref \\
	&  |  & \lnot Schema-Exp-1			& U \\
	&  |  & \pre Schema-Exp-1			& U \\
	&  |  & Schema-Exp-1 \land Schema-Exp-1	& L \\
	&  |  & Schema-Exp-1 \lor Schema-Exp-1		& L \\
	&  |  & Schema-Exp-1 \implies Schema-Exp-1	& R \\
	&  |  & Schema-Exp-1 \iff Schema-Exp-1		& L \\
	&  |  & Schema-Exp-1 \project Schema-Exp-1	& L \\
	&  |  & Schema-Exp-1 \hide (Decl-Name,\ldots,Decl-Name) & L \\
	&  |  & Schema-Exp-1 \semi Schema-Exp-1		& L \\
	&  |  & Schema-Exp-1 \pipe Schema-Exp-1		& L \\
	&  |  & ( Schema-Exp )
\also
Schema-Text\index{\(Schema-Text\)}
	& ::= & Declaration\;\lopt | Predicate \ropt
\also
Schema-Ref\index{\(Schema-Ref\)}
	& ::= & Schema-Name\;Decoration\;
			\lopt Gen-Actuals \ropt\;\lopt Renaming \ropt
\also
Renaming\index{\(Renaming\)}
	& ::= & [Decl-Name/Decl-Name, \ldots, Decl-Name/Decl-Name]
\also
Declaration\index{\(Declaration\)}
	& ::= & Basic-Decl; \ldots; Basic-Decl
\also
Basic-Decl\index{\(Basic-Decl\)}
	& ::= & Decl-Name, \ldots, Decl-Name : Expression \\
	&  |  & Schema-Ref
\also
Predicate\index{\(Predicate\)}
	& ::= & \forall Schema-Text @ Predicate \\
	&  |  & \exists Schema-Text @ Predicate \\
	&  |  & \exists_{\rm 1} Schema-Text @ Predicate \\
	&  |  & \LET Let-Def; \ldots; Let-Def @ Predicate \\
	&  |  & Predicate-1
\also
Predicate-1\index{\(Predicate-1\)}
	& ::= & Expression\;Rel\;Expression\;Rel\;\ldots\;Rel\;Expression \\
	&  |  & Pre-Rel\;Decoration\;Expression \\
	&  |  & Schema-Ref \\
	&  |  & \pre Schema-Ref \\
	&  |  & {\it true} \\
	&  |  & {\it false} \\
	&  |  & \lnot Predicate-1			& U \\
	&  |  & Predicate-1 \land Predicate-1		& L \\
	&  |  & Predicate-1 \lor Predicate-1		& L \\
	&  |  & Predicate-1 \implies Predicate-1	& R \\
	&  |  & Predicate-1 \iff Predicate-1		& L \\
	&  |  & ( Predicate )
\also
Rel\index{\(Rel\)}
	& ::= & {=} | {\in} | In-Rel\;Decoration
\also
Let-Def\index{\(Let-Def\)}
	& ::= & Var-Name == Expression
\also
Expression-0\index{\(Expression-0\)}
	& ::= & \lambda Schema-Text @ Expression \\
	&  |  & \mu Schema-Text\;\lopt @ Expression \ropt \\
	&  |  & \LET Let-Def; \ldots; Let-Def @ Expression \\
	&  |  & Expression
\also
Expression\index{\(Expression\)}
	& ::= & \IF Predicate \THEN Expression \ELSE Expression \\
	&  |  & Expression-1
\also
Expression-1\index{\(Expression-1\)}
	& ::= & Expression-1\;In-Gen\;Decoration\;Expression-1	& R \\
	&  |  & Expression-2 \cross Expression-2
			\cross \ldots \cross Expression-2 \\
	&  |  & Expression-2
\also
Expression-2\index{\(Expression-2\)}
	& ::= & Expression-2\;In-Fun\;Decoration\;Expression-2	& L \\
	&  |  & \power Expression-4 \\
	&  |  & Pre-Gen\;Decoration\;Expression-4 \\
	&  |  & \minus\;Decoration\;Expression-4 \\
	&  |  & Expression-4\;\limg\;Expression-0\;\rimg\;Decoration \\
	&  |  & Expression-3
\also
Expression-3\index{\(Expression-3\)}
	& ::= & Expression-3\;Expression-4 \\
	&  |  & Expression-4
\also
Expression-4\index{\(Expression-4\)}
	& ::= & Var-Name\;\lopt Gen-Actuals \ropt \\
	&  |  & Number \\
	&  |  & Schema-Ref \\
	&  |  & Set-Exp \\ %*
	&  |  & \langle\;\lopt Expression,\ldots,Expression \ropt\;\rangle \\
	&  |  & \lbag\;\lopt Expression, \ldots,Expression \ropt\;\rbag \\ %*
	&  |  & ( Expression, Expression, \ldots, Expression ) \\ %FIXED
	&  |  & \theta\;Schema-Name\;Decoration\;\lopt Renaming \ropt \\
	&  |  & Expression-4\;.\;Var-Name \\
	&  |  & Expression-4\;Post-Fun\;Decoration \\ 	% was Expr-1
	&  |  & Expression-4^{Expression} \\		% was Expr-1
	&  |  & ( Expression-0 )
\also
\noalign{\noindent {\bf Note}: The syntax of set expressions
(\(Set-Exp\)) is ambiguous: if $S$ is a schema, the expression
$\{\,S\,\}$ may be either a (singleton) set display or a set
comprehension, equivalent to $\{\,S @ \theta S\,\}$. The
expression should be interpreted as a set comprehension; the set
display can be written~$\{(S)\}$.}
\also
Set-Exp\index{\(Set-Exp\)}
	& ::= & \{\;\lopt Expression , \ldots, Expression \ropt\;\} \\
	&  |  & \{\;Schema-Text\;\lopt @ Expression \ropt\;\}
\also
Ident\index{\(Ident\)}
	& ::= & Word\;Decoration
\also
Decl-Name\index{\(Decl-Name\)}
	& ::= & Ident | Op-Name
\also
Var-Name\index{\(Var-Name\)}
	& ::= & Ident | ( Op-Name )
\also
Op-Name\index{\(Op-Name\)}
	& ::= & \_\;In-Sym\;Decoration\;\_ \\
	&  |  & Pre-Sym\;Decoration\;\_ \\
	&  |  & \_\;Post-Sym\;Decoration \\
	&  |  &	\_\;\limg\;\_\;\rimg\;Decoration \\
	&  |  & {\minus}\;Decoration
\also
In-Sym\index{\(In-Sym\)}
	& ::= & In-Fun | In-Gen | In-Rel
\also
Pre-Sym\index{\(Pre-Sym\)}
	& ::= & Pre-Gen | Pre-Rel
\also
Post-Sym\index{\(Post-Sym\)}
	& ::= & Post-Fun
\also
Decoration\index{\(Decoration\)}
	& ::= & \lopt Stroke\;\ldots\;Stroke \ropt
\also
Gen-Formals\index{\(Gen-Formals\)}
	& ::= & [ Ident, \ldots, Ident ]
\also
Gen-Actuals\index{\(Gen-Actuals\)}
	& ::= & [ Expression, \ldots, Expression ]
\end{syntax}
\end{raggedbottom}%
\new Here is a list of the classes of terminal symbols used in the
grammar:
\begin{name}
	\sf Word	& Undecorated name or special symbol \\
	\sf Stroke	& Single decoration: ${}'$, $?$, $!$
				or a subscript digit \\
	\sf Schema-Name	& Same as \(Word\), but used to name a schema \\
	\sf In-Fun	& Infix function symbol \\
	\sf In-Rel	& Infix relation symbol
				(or underlined identifier)\\
	\sf In-Gen	& Infix generic symbol \\
	\sf Pre-Rel	& Prefix relation symbol \\
	\sf Pre-Gen	& Prefix generic symbol \\
	\sf Post-Fun	& Postfix function symbol \\
	\sf Number	& Unsigned decimal integer
\end{name}
\par\nobreak\vfill\nobreak

% From the TeXbook
\def\frac#1/#2{\leavevmode\kern.1em
  \raise.5ex\hbox{\the\scriptfont0 #1}\kern-.1em
  /\kern-.15em\lower.25ex\hbox{\the\scriptfont0 #2}}

\begin{quotation}
	\new The brilliant, articulate, white-eyelashed
	Mr.~Zed\index{pronunciation} turns his
	eyes to his wife and sees nothing but
	$\mit T x \frac1/4 \, p \frac3/4 
		= \frac1/2 \minus p r x \frac1/4$ (inverted).
	%Then he turns them on the willowy man with the cracked
	%voice, and realizes all in an instant that his last three
	%years of constructive thought have been wasted. His premises
	%have failed him. He had been assuming that Space was
	%intrinsically modelled.
	\source{Mervyn Peake, {\em Titus Alone}} % p. 123 of the Penguin edn.
\end{quotation}
\unskip\newpage
	
\chapter*{Changes from the first edition}

The following are the substantive differences between the Z notation and
mathematical tool-kit described in the first and second editions:
\begin{enumerate}
\item Renaming of schema components has been added
	(Section~\ref{ss:decor}).

\item An ordinary identifier may now be used as an infix relation
	symbol by underlining it (Section~\ref{ss:infix}).

\item Decorations can be empty, so the decoration after $\theta$
	is no longer optional (Section~\ref{s:exp}).

\item The `quantifier' $\bf let$ for both expressions
	(Section~\ref{s:exp}) and predicates (Section~\ref{s:pred})
	has been added.

\item Conditional expressions $\IF P \THEN E_1 \ELSE E_2$ have been
	added (Section~\ref{s:exp}).

\item The piping operator $\pipe$ on schemas has been added.  The
	definition of sequential composition now allows state
	variables that do not match (Section~\ref{s:schemaexp}).

\item The overriding operator $\oplus$ has been extended to apply to
	relations, not just functions (Section~\ref{s:rellib}).

\item The extraction operator $\extract$ and the $squash$ function
	on sequences have been added.  Also the subsequence
	relations $\prefix$, $\suffix$ and $\inseq$
	(Section~\ref{s:seqlib}).

\item A new infix operator $\bcount$ is a synonym for $count$ on
	bags. The membership relation for bags is now $\inbag$.
	Other new bag operators are $\subbageq$, $\otimes$ and
	$\uminus$ (Section~\ref{s:baglib}).

\item $\Delta$ and $\Xi$ may now be used with generic schemas, and
	the definition of $\Xi State$ no longer uses $\Delta State$,
	in case $\Delta State$ is redefined but $\Xi State$ isn't
	(Section~\ref{s:deltaxi}).

\item The syntax has been extended to allow decorations after infix
	operators, etc.\ (Chapter~\ref{c:syntax}).

\item The forms \(Expression-4\;Post-Fun\) and
	\(Expression-4^{Expression}\) are now alternatives for
	\(Expression-4\) rather than \(Expression-1\)
	(Chapter~\ref{c:syntax}).
\end{enumerate}
The following major changes in exposition will affect the use of the
manual as a text:
\begin{enumerate}
\item `Situations' have been eliminated in favour of `bindings' as
	the structures with respect to which the values of
	expressions and the truth of predicates are defined
	(Chapter~\ref{c:background}).

\item The term `finitary construction' in the description of free
	types now has something more like its usual meaning
	(Section~\ref{s:freetype}).

\item Chapter~\ref{c:seqprog} now explains the rules for data
	refinement in terms of the examples from
	Chapter~\ref{c:tutorial}.
\end{enumerate}

\input{glossary}

\input{symdex}

\indexnote{Entries set in {\it italic\/} type are the names of
	constants in the mathematical tool-kit. Special
	symbols are indexed here under a descriptive name; the
	symbol itself is shown in parentheses. The one-page
	`Index of symbols' lists all these symbols for ease of
	reference.  Entries set in {\sf sans-serif\/} type are
	syntactic categories; they refer to the pages where
	the syntax rules for the categories may be found.}

\input{index}

\end{document}
